

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>2.5. 分解成分中的信号（矩阵分解问题） &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.6. 协方差估计" href="covariance.html" />
    <link rel="prev" title="2.4. 双聚类" href="biclustering.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/decomposition.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="biclustering.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      2.4. 双聚类
      </span>
          <span class="hiddenrellink">
          2.4. 双聚类
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="covariance.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      2.6. 协方差估计
      </span>
          <span class="hiddenrellink">
          2.6. 协方差估计
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../unsupervised_learning.html">
      Up
      <br/>
      <span class="smallrellink">
      2. 无监督学习
      </span>
          <span class="hiddenrellink">
          2. 无监督学习
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">2.5. 分解成分中的信号（矩阵分解问题）</a><ul>
<li><a class="reference internal" href="#pca">2.5.1. 主成分分析（PCA）</a><ul>
<li><a class="reference internal" href="#pca-exact-pca-and-probabilistic-interpretation">2.5.1.1. 准确的PCA和概率解释（Exact PCA and probabilistic interpretation）</a></li>
<li><a class="reference internal" href="#pca-incremental-pca">2.5.1.2. 增量PCA (Incremental PCA)</a></li>
<li><a class="reference internal" href="#pca-svd">2.5.1.3. PCA 使用随机SVD</a></li>
<li><a class="reference internal" href="#kernel-pca">2.5.1.4. 核 PCA</a></li>
<li><a class="reference internal" href="#sparsepca-minibatchsparsepca">2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lsa">2.5.2. 截断奇异值分解和隐语义分析</a></li>
<li><a class="reference internal" href="#dictionarylearning">2.5.3. 词典学习</a><ul>
<li><a class="reference internal" href="#sparsecoder">2.5.3.1. 带有预计算词典的稀疏编码</a></li>
<li><a class="reference internal" href="#id9">2.5.3.2. 通用词典学习</a></li>
<li><a class="reference internal" href="#minibatchdictionarylearning">2.5.3.3. 小批量字典学习</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fa">2.5.4. 因子分析</a></li>
<li><a class="reference internal" href="#ica">2.5.5. 独立成分分析（ICA）</a></li>
<li><a class="reference internal" href="#nmf-nnmf">2.5.6. 非负矩阵分解(NMF 或 NNMF)</a><ul>
<li><a class="reference internal" href="#nmf-frobenius">2.5.6.1. NMF 与 Frobenius 范数</a></li>
<li><a class="reference internal" href="#beta-divergence-nmf">2.5.6.2. 具有 beta-divergence 的 NMF</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dirichlet-lda">2.5.7. 隐 Dirichlet 分配（LDA）</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="decompositions">
<span id="id1"></span><h1>2.5. 分解成分中的信号（矩阵分解问题）<a class="headerlink" href="#decompositions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pca">
<span id="id2"></span><h2>2.5.1. 主成分分析（PCA）<a class="headerlink" href="#pca" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pca-exact-pca-and-probabilistic-interpretation">
<h3>2.5.1.1. 准确的PCA和概率解释（Exact PCA and probabilistic interpretation）<a class="headerlink" href="#pca-exact-pca-and-probabilistic-interpretation" title="Permalink to this headline">¶</a></h3>
<p>PCA 用于对一组连续正交分量中的多变量数据集进行方差最大方向的分解。
在 scikit-learn 中， <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 被实现为一个变换对象， 通过 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法可以降维成 <cite>n</cite> 个成分，
并且可以将新的数据投影(project, 亦可理解为分解)到这些成分中。</p>
<p>可选参数 <code class="docutils literal notranslate"><span class="pre">whiten=True</span></code> 使得可以将数据投影到奇异（singular）空间上，同时将每个成分缩放到单位方差。
如果下游模型对信号的各向同性作出强烈的假设，这通常是有用的，例如，使用RBF内核的 SVM 算法和 K-Means 聚类算法。</p>
<p>以下是iris数据集的一个示例，该数据集包含4个特征， 通过PCA降维后投影到方差最大的二维空间上：</p>
<div class="figure" id="id27">
<img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_lda_001.png:target:../auto_examples/decomposition/plot_pca_vs_lda.html:align:center:scale:75%" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_lda_001.png:target:../auto_examples/decomposition/plot_pca_vs_lda.html:align:center:scale:75%" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 对象还提供了 PCA 的概率解释， 其可以基于其解释的方差量给出数据的可能性。</span></p>
</div>
<p>可以通过在交叉验证（cross-validation）中使用 <cite>score</cite> 方法来实现：</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_001.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">例子:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="section" id="pca-incremental-pca">
<span id="incrementalpca"></span><h3>2.5.1.2. 增量PCA (Incremental PCA)<a class="headerlink" href="#pca-incremental-pca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 对象非常有用, 但对大型数据集有一定的限制。
最大的限制是 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 仅支持批处理，这意味着所有要处理的数据必须适合主内存。
<a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 对象使用不同的处理形式使之允许部分计算，
这一形式几乎和 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 以小型批处理方式处理数据的方法完全匹配。
<a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 可以通过以下方式实现核外（out-of-core）主成分分析：</p>
<blockquote>
<div><ul class="simple">
<li>使用 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法从本地硬盘或网络数据库中以此获取数据块。</li>
<li>通过 <code class="docutils literal notranslate"><span class="pre">numpy.memmap</span></code> 在一个 memory mapped file 上使用 fit 方法。</li>
</ul>
<p><a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 仅存储成分和噪声方差的估计值，并按顺序递增地更新解释方差比（<a href="#id28"><span class="problematic" id="id29">explained_variance_ratio_</span></a>）。</p>
</div></blockquote>
<p>这就是为什么内存使用取决于每个批次的样本数，而不是数据集中要处理的样本数。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_001.png" /></a>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_002.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_002.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="std std-ref">Incremental PCA</span></a></li>
</ul>
</div>
</div>
<div class="section" id="pca-svd">
<span id="randomizedpca"></span><h3>2.5.1.3. PCA 使用随机SVD<a class="headerlink" href="#pca-svd" title="Permalink to this headline">¶</a></h3>
<p>通过丢弃具有较低奇异值的奇异向量成分，将数据降维到低维空间并保留大部分方差是非常有意义的。</p>
<p>例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096，
在这样大的数据上训练含RBF内核的支持向量机是很慢的。
此外我们知道数据本质上的维度远低于4096，因为人脸的所有照片都看起来有点相似。
样本位于许多的很低维度（例如约200维）。PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分方差。</p>
<p>在这种情况下，使用可选参数 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> 的 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 是非常有用的。
因为我们将要丢弃大部分奇异值，所以对我们将保留并实际执行变换的奇异向量进行近似估计的有限的计算更有效。</p>
<p>例如：以下显示了来自 Olivetti 数据集的 16 个样本肖像（以 0.0 为中心）。
右侧是前 16 个奇异向量重画为肖像。因为我们只需要使用大小为 <span class="math">n_{samples} = 400</span>
和 <span class="math">n_{features} = 64 \times 64 = 4096</span> 的数据集的前 16 个奇异向量, 使得计算时间小于 1 秒。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="orig_img" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_001.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a></strong></p><p>注意：使用可选参数 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> ，在 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 中我们还需要给出输入低维空间大小 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 。</p>
<p>如果我们注意到： <span class="math">n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})</span> 且
<span class="math">n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})</span>,
对于PCA中实施的确切方式，随机 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的时间复杂度是：<span class="math">O(n_{\max}^2 \cdot n_{\mathrm{components}})</span> ，
而不是 <span class="math">O(n_{\max}^2 \cdot n_{\min})</span> 。</p>
<p>对于确切的方式，随机 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的内存占用量正比于 <span class="math">2 \cdot n_{\max} \cdot n_{\mathrm{components}}</span> ，
而不是 <span class="math">n_{\max}\cdot n_{\min}</span></p>
<p>注意：选择参数 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> 的 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>，在执行 <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> 时，
并不是 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 的确切的逆变换操作（即使 参数设置为默认的 <code class="docutils literal notranslate"><span class="pre">whiten=False</span></code>）</p>
<div class="topic">
<p class="topic-title first">例子:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://arxiv.org/abs/0909.4061">“Finding structure with randomness: Stochastic algorithms for
constructing approximate matrix decompositions”</a>
Halko, et al., 2009</li>
</ul>
</div>
</div>
<div class="section" id="kernel-pca">
<span id="id3"></span><h3>2.5.1.4. 核 PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> 是 PCA 的扩展，通过使用核方法实现非线性降维（dimensionality reduction） (参阅 <a class="reference internal" href="metrics.html#metrics"><span class="std std-ref">成对的矩阵, 类别和核函数</span></a>)。
它具有许多应用，包括去噪, 压缩和结构化预测（ structured prediction ） (kernel dependency estimation（内核依赖估计）)。</p>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> 支持 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 和 <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> 。</div></blockquote>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_kernel_pca.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_kernel_pca_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_kernel_pca_001.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="std std-ref">Kernel PCA</span></a></li>
</ul>
</div>
</div>
<div class="section" id="sparsepca-minibatchsparsepca">
<span id="sparsepca"></span><h3>2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )<a class="headerlink" href="#sparsepca-minibatchsparsepca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> 是 PCA 的一个变体，目的是提取能最好地重建数据的稀疏组分集合。</p>
<p>小批量稀疏 PCA ( <a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a> ) 是一个 <a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> 的变种，它速度更快但准确度有所降低。对于给定的迭代次数，通过迭代该组特征的小块来达到速度的增加。</p>
<p>Principal component analysis（主成分分析） (<a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>) 的缺点在于，通过该方法提取的成分具有唯一的密度表达式，即当表示为原始变量的线性组合时，它们具有非零系数，使之难以解释。在许多情况下，真正的基础组件可以更自然地想象为稀疏向量; 例如在面部识别中，每个组件可能自然地映射到面部的某个部分。</p>
<p>稀疏的主成分产生更简洁、可解释的表达式，明确强调了样本之间的差异性来自哪些原始特征。</p>
<p>以下示例说明了使用稀疏 PCA 提取 Olivetti 人脸数据集中的 16 个组分。可以看出正则化项产生了许多零。此外，数据的自然结构导致了非零系数垂直相邻 （vertically adjacent）。该模型不会在数学上强制执行: 每个组分都是一个向量  <span class="math">h \in \mathbf{R}^{4096}</span>,除非人性化地的可视化为 64x64 像素的图像，否则没有垂直相邻性的概念。
下面显示的组分看起来局部化（appear local)是数据的内在结构的影响，这种局部模式使重建误差最小化。有一种考虑到邻接性和不同结构类型的导致稀疏的规范（sparsity-inducing norms）,参见 <a class="reference internal" href="#jen09" id="id4">[Jen09]</a> 对这种方法进行了解。
有关如何使用稀疏 PCA 的更多详细信息，请参阅下面的示例部分。
更多关于 Sparse PCA 使用的内容，参见示例部分，如下：</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="spca_img" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_005.png" /></a></strong></p><p>请注意，有多种不同的计算稀疏PCA 问题的公式。 这里使用的方法基于 <a class="reference internal" href="#mrl09" id="id5">[Mrl09]</a> 。优化问题的解决是一个带有惩罚项（L1范数的） <span class="math">\ell_1</span> 的一个 PCA 问题（dictionary learning（字典学习））:</p>
<div class="math">
<p><span class="math">(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||V||_1 \\
             \text{subject to\,} &amp; ||U_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{components}</span></p>
</div><p>导致稀疏（sparsity-inducing）的 <span class="math">\ell_1</span> 规范也可以避免当训练样本很少时从噪声中学习成分。可以通过超参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 来调整惩罚程度（从而减少稀疏度）。值较小会导致温和的正则化因式分解，而较大的值将许多系数缩小到零。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">虽然本着在线算法的精神， <a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a> 类不实现 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> , 因为在线算法沿特征方向，而不是样本方向。</p>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils citation" frame="void" id="mrl09" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[Mrl09]</a></td><td><a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online Dictionary Learning for Sparse Coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jen09" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[Jen09]</a></td><td><a class="reference external" href="www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf">“Structured Sparse Principal Component Analysis”</a>
R. Jenatton, G. Obozinski, F. Bach, 2009</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="lsa">
<span id="id6"></span><h2>2.5.2. 截断奇异值分解和隐语义分析<a class="headerlink" href="#lsa" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 实现了一个奇异值分解（SVD）的变体，它只计算 <span class="math">k</span> 个最大的奇异值，其中 <span class="math">k</span> 是用户指定的参数。</p>
<p>当截断的 SVD被应用于 term-document矩阵（由 <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> 或 <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code> 返回）时，这种转换被称为 <a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">latent semantic analysis</a> (LSA), 因为它将这样的矩阵转换为低纬度的 “semantic（语义）” 空间。
特别地是 LSA 能够抵抗同义词和多义词的影响（两者大致意味着每个单词有多重含义），这导致 term-document 矩阵过度稀疏，并且在诸如余弦相似性的度量下表现出差的相似性。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">LSA 也被称为隐语义索引 LSI，尽管严格地说它是指在持久索引（persistent indexes）中用于信息检索的目的。</p>
</div>
<p>数学表示中， 训练样本 <span class="math">X</span> 用截断的SVD产生一个低秩的（ low-rank）近似值 <span class="math">X</span> :</p>
<div class="math">
<p><span class="math">X \approx X_k = U_k \Sigma_k V_k^\top</span></p>
</div><p>在这个操作之后，<span class="math">U_k \Sigma_k^\top</span> 是转换后的训练集，其中包括 <span class="math">k</span> 个特征（在 API 中被称为 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> ）。</p>
<p>还需要转换一个测试集 <span class="math">X</span>, 我们乘以 <span class="math">V_k</span>:</p>
<div class="math">
<p><span class="math">X' = X V_k</span></p>
</div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">自然语言处理(NLP) 和信息检索(IR) 文献中的 LSA 的大多数处理方式是交换矩阵 <span class="math">X</span> 的坐标轴,使其具有 <code class="docutils literal notranslate"><span class="pre">n_features</span></code> × <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> 的形状。
我们以 scikit-learn API 相匹配的不同方式呈现 LSA, 但是找到的奇异值是相同的。</p>
</div>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 非常类似于 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, 但不同之处在于它工作在样本矩阵 <span class="math">X</span> 而不是它们的协方差矩阵。
当从特征值中减去 <span class="math">X</span> 的每列（每个特征per-feature）的均值时，在得到的矩阵上应用 truncated SVD 相当于 PCA 。
实际上，这意味着 <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 转换器（transformer）接受 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 矩阵，而不需要对它们进行密集（density），因为即使对于中型大小文档的集合，密集化 （densifying）也可能填满内存。</p>
<p>虽然 <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 转换器（transformer）可以在任何（稀疏的）特征矩阵上工作，但还是建议在 LSA/document 处理设置中，在 tf–idf 矩阵上的原始频率计数使用它。
特别地，应该打开子线性缩放（sublinear scaling）和逆文档频率（inverse document frequency） (<code class="docutils literal notranslate"><span class="pre">sublinear_tf=True,</span> <span class="pre">use_idf=True</span></code>) 以使特征值更接近于高斯分布，补偿 LSA 对文本数据的错误假设。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),
<em>Introduction to Information Retrieval</em>, Cambridge University Press,
chapter 18: <a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">Matrix decompositions &amp; latent semantic indexing</a></li>
</ul>
</div>
</div>
<div class="section" id="dictionarylearning">
<span id="id7"></span><h2>2.5.3. 词典学习<a class="headerlink" href="#dictionarylearning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sparsecoder">
<span id="id8"></span><h3>2.5.3.1. 带有预计算词典的稀疏编码<a class="headerlink" href="#sparsecoder" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder" title="sklearn.decomposition.SparseCoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseCoder</span></code></a> 对象是一个估计器 （estimator），可以用来将信号转换成一个固定的预计算的词典内原子（atoms）的稀疏线性组合（sparse linear combination），如离散小波基（ discrete wavelet basis ） 。
因此，该对象不实现 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法。该转换相当于一个稀疏编码问题: 将数据的表示为尽可能少的词典原子的线性组合。
词典学习的所有变体实现以下变换方法，可以通过 <code class="docutils literal notranslate"><span class="pre">transform_method</span></code> 初始化参数进行控制:</p>
<ul class="simple">
<li>Orthogonal matching pursuit(追求正交匹配) (<a class="reference internal" href="linear_model.html#omp"><span class="std std-ref">正交匹配追踪法（OMP）</span></a>)</li>
<li>Least-angle regression (最小角度回归)(<a class="reference internal" href="linear_model.html#least-angle-regression"><span class="std std-ref">最小角回归</span></a>)</li>
<li>Lasso computed by least-angle regression(最小角度回归的Lasso 计算)</li>
<li>Lasso using coordinate descent ( 使用坐标下降的Lasso)(<a class="reference internal" href="linear_model.html#lasso"><span class="std std-ref">Lasso</span></a>)</li>
<li>Thresholding(阈值)</li>
</ul>
<p>阈值方法速度非常快，但是不能产生精确的重建。
它们在分类任务的文献中已被证明是有用的。对于图像重建任务，追求正交匹配可以产生最精确、无偏的重建。</p>
<p>词典学习对象通过 <code class="docutils literal notranslate"><span class="pre">split_code</span></code> 参数提供稀疏编码结果中的正值和负值分离的可能性。当使用词典学习来提取将用于监督学习的特征时，这是有用的，因为它允许学习算法将不同的权重从正加载（loading）分配给相应的负加载的特定原子。</p>
<p>单个样本的分割编码具有长度 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">n_components</span></code> ，并使用以下规则构造: 首先，计算长度为 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 的常规编码。然后， <code class="docutils literal notranslate"><span class="pre">split_code</span></code> 的第一个 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 条目将用正常编码向量的正部分填充。分割编码的第二部分用编码向量的负部分填充，只有一个正号。因此， split_code 是非负的。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py"><span class="std std-ref">Sparse coding with a precomputed dictionary</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id9">
<h3>2.5.3.2. 通用词典学习<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>词典学习( <a class="reference internal" href="generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning" title="sklearn.decomposition.DictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictionaryLearning</span></code></a> ) 是一个矩阵因式分解问题，相当于找到一个在拟合数据的稀疏编码中表现良好的（通常是过完备的（overcomplete））词典。</p>
<p>将数据表示为来自过完备词典的原子的稀疏组合被认为是哺乳动物初级视觉皮层的工作方式。
因此，应用于图像补丁的词典学习已被证明在诸如图像完成、修复和去噪，以及有监督的识别图像处理任务中表现良好的结果。</p>
<p>词典学习是通过交替更新稀疏编码来解决的优化问题，作为解决多个 Lasso 问题的一个解决方案，考虑到字典固定，然后更新字典以最好地适合稀疏编码。</p>
<div class="math">
<p><span class="math">(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||U||_1 \\
             \text{subject to\,} &amp; ||V_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{\mathrm{atoms}}</span></p>
</div><p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img2" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="dict_img2" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_006.png" /></a></strong></p><p>在使用这样一个过程来拟合词典之后，变换只是一个稀疏的编码步骤，与所有的词典学习对象共享相同的实现。(参见 <a class="reference internal" href="#sparsecoder"><span class="std std-ref">带有预计算词典的稀疏编码</span></a>)。</p>
<p>以下图像显示了字典学习是如何从浣熊脸部的部分图像中提取的4x4像素图像补丁中进行词典学习的。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_image_denoising_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_image_denoising_001.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py"><span class="std std-ref">Image denoising using dictionary learning</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online dictionary learning for sparse coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</li>
</ul>
</div>
</div>
<div class="section" id="minibatchdictionarylearning">
<span id="id11"></span><h3>2.5.3.3. 小批量字典学习<a class="headerlink" href="#minibatchdictionarylearning" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> 实现了更快、更适合大型数据集的字典学习算法，其运行速度更快，但准确度有所降低。</p>
<p>默认情况下，<a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> 将数据分成小批量，并通过在指定次数的迭代中循环使用小批量，以在线方式进行优化。但是，目前它没有实现停止条件。</p>
<p>估计器还实现了  <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>, 它通过在一个小批处理中仅迭代一次来更新字典。 当在线学习的数据从一开始就不容易获得，或者数据超出内存时，可以使用这种迭代方法。</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_dict_face_patches.html"><img alt="modules/../auto_examples/cluster/images/sphx_glr_plot_dict_face_patches_001.png" class="align-right" src="modules/../auto_examples/cluster/images/sphx_glr_plot_dict_face_patches_001.png" /></a>
<div class="topic">
<p class="topic-title first"><strong>字典学习聚类</strong></p>
<p>注意，当使用字典学习来提取表示（例如，用于稀疏编码）时，聚类可以是学习字典的良好中间方法。
例如，<a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code></a> 估计器能高效计算并使用 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法实现在线学习。</p>
<p>示例: 在线学习面部部分的字典 <a class="reference internal" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="std std-ref">Online learning of a dictionary of parts of faces</span></a></p>
</div>
</div>
</div>
<div class="section" id="fa">
<span id="id12"></span><h2>2.5.4. 因子分析<a class="headerlink" href="#fa" title="Permalink to this headline">¶</a></h2>
<p>在无监督的学习中，我们只有一个数据集 <span class="math">X = \{x_1, x_2, \dots, x_n\}</span>.
这个数据集如何在数学上描述？ <span class="math">X</span> 的一个非常简单的连续隐变量模型</p>
<div class="math">
<p><span class="math">x_i = W h_i + \mu + \epsilon</span></p>
</div><p>矢量 <span class="math">h_i</span> 被称为 “隐性的”，因为它是不可观察的。
<span class="math">\epsilon</span> 被认为是符合高斯分布的噪声项，平均值为 0，协方差为 <span class="math">\Psi</span> （即 <span class="math">\epsilon \sim \mathcal{N}(0, \Psi)</span>），
<span class="math">\mu</span> 是偏移向量。 这样一个模型被称为 “生成的”，因为它描述了如何从 <span class="math">h_i</span> 生成 <span class="math">x_i</span> 。
如果我们使用所有的 <span class="math">x_i</span> 作为列来形成一个矩阵 <span class="math">\mathbf{X}</span> ，并将所有的 <span class="math">h_i</span> 作为矩阵 <span class="math">\mathbf{H}</span> 的列，
那么我们可以写（适当定义的 <span class="math">\mathbf{M}</span> 和 <span class="math">\mathbf{E}</span> ）:</p>
<div class="math">
<p><span class="math">\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}</span></p>
</div><p>换句话说，我们 <em>分解</em> 矩阵 <span class="math">\mathbf{X}</span>.
如果给出 <span class="math">h_i</span>，上述方程自动地表示以下概率解释：</p>
<div class="math">
<p><span class="math">p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)</span></p>
</div><p>对于一个完整的概率模型，我们还需要隐变量 <span class="math">h</span> 的先验分布。
最直接的假设（基于高斯分布的良好性质）是 <span class="math">h \sim \mathcal{N}(0, \mathbf{I})</span>. 这产生一个高斯分布作为 <span class="math">x</span> 的边际分布:</p>
<div class="math">
<p><span class="math">p(x) = \mathcal{N}(\mu, WW^T + \Psi)</span></p>
</div><p>现在，在没有任何进一步假设的前提下，隐变量 <span class="math">h</span> 是多余的 – <span class="math">x</span> 完全可以用均值和协方差来建模。
我们需要对这两个参数之一进行更具体的构造。 一个简单的附加假设是将误差协方差 <span class="math">\Psi</span> 构造成如下:</p>
<ul class="simple">
<li><span class="math">\Psi = \sigma^2 \mathbf{I}</span>: 这个假设能推导出 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的概率模型。</li>
<li><span class="math">\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)</span>: 这个模型称为 <a class="reference internal" href="generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis" title="sklearn.decomposition.FactorAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">FactorAnalysis</span></code></a>, 一个经典的统计模型。 矩阵W有时称为 “因子加载矩阵”。</li>
</ul>
<p>两个模型基都基于高斯分布是低阶协方差矩阵的假设。
因为这两个模型都是概率性的，所以它们可以集成到更复杂的模型中，
例如因子分析器的混合。如果隐变量基于非高斯分布，则得到完全不同的模型（例如， <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">FastICA</span></code></a> ）。</p>
<p>因子分析 <em>可以</em> 产生与 :class:<a href="#id13"><span class="problematic" id="id14">`</span></a>PCA`类似的成分（例如其加载矩阵的列）。
然而，这些成分没有通用的性质（例如它们是否是正交的）:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img3" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="fa_img3" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_009.png" /></a></strong></p><p>因子分析( <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> ) 的主要优点是可以独立地对输入空间的每个方向（异方差噪声）的方差建模:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_008.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_008.png" /></a>
</div>
<p>在异方差噪声存在的情况下，这可以比概率 PCA 作出更好的模型选择:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_002.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_002.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="section" id="ica">
<span id="id15"></span><h2>2.5.5. 独立成分分析（ICA）<a class="headerlink" href="#ica" title="Permalink to this headline">¶</a></h2>
<p>独立分量分析将多变量信号分解为独立性最强的加性子组件。
它通过 <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fast</span> <span class="pre">ICA</span></code></a> 算法在 scikit-learn 中实现。
ICA 通常不用于降低维度，而是用于分离叠加信号。
由于 ICA 模型不包括噪声项，因此要使模型正确，必须使用白化。
这可以在内部调节白化参数或手动使用 PCA 的一种变体。</p>
<p>ICA 通常用于分离混合信号（称为 <em>盲源分离</em> 的问题），如下例所示:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_ica_blind_source_separation_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_ica_blind_source_separation_001.png" /></a>
</div>
<p>ICA 也可以用于具有稀疏子成分的非线性分解:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img4" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="ica_img4" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_004.png" /></a></strong></p><div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="std std-ref">Blind source separation using FastICA</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="std std-ref">FastICA on 2D point clouds</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
</div>
<div class="section" id="nmf-nnmf">
<span id="nmf"></span><h2>2.5.6. 非负矩阵分解(NMF 或 NNMF)<a class="headerlink" href="#nmf-nnmf" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nmf-frobenius">
<h3>2.5.6.1. NMF 与 Frobenius 范数<a class="headerlink" href="#nmf-frobenius" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> <a class="footnote-reference" href="#id22" id="id16">[1]</a> 是在数据和分量是非负情况下的另一种降维方法。
在数据矩阵不包含负值的情况下，可以插入 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 而不是 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 或其变体。
通过优化 <span class="math">X</span> 与矩阵乘积 <span class="math">WH</span> 之间的距离 <span class="math">d</span> ，可以将样本 <span class="math">X</span> 分解为两个非负矩阵 <span class="math">W</span> 和 <span class="math">H</span>。
最广泛使用的距离函数是 Frobenius 平方范数，它是欧几里德范数到矩阵的推广:</p>
<div class="math">
<p><span class="math">d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2</span></p>
</div><p>与 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 不同，通过叠加分量而不减去，以加法方式获得向量的表示。这种加性模型对于表示图像和文本是有效的。</p>
<blockquote>
<div>[Hoyer, 2004] <a class="footnote-reference" href="#id23" id="id17">[2]</a> 研究表明，当处于一定约束时，<a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 可以产生数据集基于某子部分的表示，从而获得可解释的模型。</div></blockquote>
<p>以下示例展示了与 PCA 特征面相比， <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 从 Olivetti 面部数据集中的图像中发现的16个稀疏组件。</p>
<p>Unlike <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, the representation of a vector is obtained in an additive
fashion, by superimposing the components, without subtracting. Such additive
models are efficient for representing images and text.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img5" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="nmf_img5" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_003.png" /></a></strong></p><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">init</span></code> 属性确定了应用的初始化方法，这对方法的性能有很大的影响。
<a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 实现了非负双奇异值分解方法。NNDSVD <a class="footnote-reference" href="#id24" id="id18">[4]</a> 基于两个 SVD 过程，一个近似数据矩阵，
使用单位秩矩阵的代数性质，得到的部分SVD因子的其他近似正部分。
基本的 NNDSVD 算法更适合稀疏分解。其变体 NNDSVDa（全部零值替换为所有元素的平均值）和
NNDSVDar（零值替换为比数据平均值除以100小的随机扰动）在稠密情况时推荐使用。</p>
<p>请注意，乘法更新 (‘mu’) 求解器无法更新初始化中存在的零，因此当与引入大量零的基本 NNDSVD 算法联合使用时，
会导致较差的结果; 在这种情况下，应优先使用 NNDSVDa 或 NNDSVDar。</p>
<p>也可以通过设置 <code class="xref py py-attr docutils literal notranslate"><span class="pre">init=&quot;random&quot;</span></code>，使用正确缩放的随机非负矩阵初始化 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 。
整数种子或 <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> 也可以传递给 <code class="xref py py-attr docutils literal notranslate"><span class="pre">random_state</span></code> 以控制重现性。</p>
<p>在 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 中，L1 和 L2 先验可以被添加到损失函数中以使模型正规化。
L2 先验使用 Frobenius 范数，而L1 先验使用 L1 范数。与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code> 一样，
我们通过 <code class="xref py py-attr docutils literal notranslate"><span class="pre">l1_ratio</span></code> (<span class="math">\rho</span>) 参数和正则化强度参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (<span class="math">\alpha</span>) 来控制 L1 和 L2 的组合。那么先验项是:</p>
<div class="math">
<p><span class="math">\alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2</span></p>
</div><p>正则化目标函数为:</p>
<div class="math">
<p><span class="math">d_{\mathrm{Fro}}(X, WH)
+ \alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2</span></p>
</div><p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 正则化 W 和 H . 公共函数 <code class="xref py py-func docutils literal notranslate"><span class="pre">non_negative_factorization</span></code> 允许通过 <code class="xref py py-attr docutils literal notranslate"><span class="pre">regularization</span></code> 属性进行更精细的控制，将 仅W ，仅H 或两者正规化。</p>
</div>
<div class="section" id="beta-divergence-nmf">
<h3>2.5.6.2. 具有 beta-divergence 的 NMF<a class="headerlink" href="#beta-divergence-nmf" title="Permalink to this headline">¶</a></h3>
<p>如前所述，最广泛使用的距离函数是平方 Frobenius 范数，这是欧几里得范数到矩阵的推广:</p>
<div class="math">
<p><span class="math">d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2</span></p>
</div><p>其他距离函数可用于 NMF，例如（广义） Kullback-Leibler(KL) 散度，也称为 I-divergence:</p>
<div class="math">
<p><span class="math">d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})</span></p>
</div><p>或者， Itakura-Saito(IS) divergence:</p>
<div class="math">
<p><span class="math">d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)</span></p>
</div><p>这三个距离函数是 beta-divergence 函数族的特殊情况，其参数分别为 <span class="math">\beta = 2, 1, 0</span> <a class="footnote-reference" href="#id26" id="id19">[6]</a> 。 beta-divergence 定义如下:</p>
<div class="math">
<p><span class="math">d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})</span></p>
</div><div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_beta_divergence.html"><img alt="modules/../auto_examples/decomposition/images/sphx_glr_plot_beta_divergence_001.png" src="modules/../auto_examples/decomposition/images/sphx_glr_plot_beta_divergence_001.png" /></a>
</div>
<p>请注意，在 <span class="math">\beta \in (0; 1)</span> 上定义无效，仅仅在 <span class="math">d_{KL}</span>
和 <span class="math">d_{IS}</span> 的上可以分别连续扩展。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 使用 Coordinate Descent (‘cd’) <a class="footnote-reference" href="#id25" id="id20">[5]</a> 和乘法更新 (‘mu’) <a class="footnote-reference" href="#id26" id="id21">[6]</a> 来实现两个求解器。
‘mu’ 求解器可以优化每个 beta-divergence，包括 Frobenius 范数 (<span class="math">\beta=2</span>) ，
（广义） Kullback-Leibler divergence (<span class="math">\beta=1</span>) 和Itakura-Saito divergence（beta = 0） ）。
请注意，对于 <span class="math">\beta \in (1; 2)</span>，’mu’ 求解器明显快于 <span class="math">\beta</span> 的其他值。
还要注意，使用负数（或0，即 ‘itakura-saito’ ） <span class="math">\beta</span>，输入矩阵不能包含零值。</p>
<p>‘cd’ 求解器只能优化 Frobenius 范数。由于 NMF 的潜在非凸性，即使优化相同的距离函数，
不同的求解器也可能会收敛到不同的最小值。</p>
<p>NMF最适用于 <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> 方法，该方法返回矩阵W.矩阵 H 被 <code class="docutils literal notranslate"><span class="pre">components_</span></code> 属性中存储到拟合模型中;
方法 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 将基于这些存储的组件分解新的矩阵 X_new:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_beta_divergence.html#sphx-glr-auto-examples-decomposition-plot-beta-divergence-py"><span class="std std-ref">Beta-divergence loss functions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[1]</a></td><td><a class="reference external" href="http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf">“Learning the parts of objects by non-negative matrix factorization”</a>
D. Lee, S. Seung, 1999</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[2]</a></td><td><a class="reference external" href="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">“Non-negative Matrix Factorization with Sparseness Constraints”</a>
P. Hoyer, 2004</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id18">[4]</a></td><td><a class="reference external" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">“SVD based initialization: A head start for nonnegative
matrix factorization”</a>
C. Boutsidis, E. Gallopoulos, 2008</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id20">[5]</a></td><td><a class="reference external" href="http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf">“Fast local algorithms for large scale nonnegative matrix and tensor
factorizations.”</a>
A. Cichocki, P. Anh-Huy, 2009</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><em>(<a class="fn-backref" href="#id19">1</a>, <a class="fn-backref" href="#id21">2</a>)</em> <a class="reference external" href="http://http://arxiv.org/pdf/1010.1763v3.pdf">“Algorithms for nonnegative matrix factorization with the beta-divergence”</a>
C. Fevotte, J. Idier, 2011</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="dirichlet-lda">
<span id="latentdirichletallocation"></span><h2>2.5.7. 隐 Dirichlet 分配（LDA）<a class="headerlink" href="#dirichlet-lda" title="Permalink to this headline">¶</a></h2>
<p>隐 Dirichlet 分配是离散数据集（如文本语料库）的集合的生成概率模型。
它也是一个主题模型，用于从文档集合中发现抽象主题。</p>
<p>LDA 的图形模型是一个三层贝叶斯模型:</p>
<img alt="../_images/lda_model_graph.png" class="align-center" src="../_images/lda_model_graph.png" />
<p>当建模文本语料库时，该模型假设具有 <span class="math">D</span> 文档和 <span class="math">K</span> 主题的语料库的以下生成过程:</p>
<blockquote>
<div><ol class="arabic simple">
<li>对于每个主题 <span class="math">k</span>，绘制 <span class="math">\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K</span></li>
<li>对于每个文档 <span class="math">d</span>，绘制 <span class="math">\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D</span></li>
<li>对于文档 <span class="math">d</span> 中的每个单词 <span class="math">i</span>:</li>
</ol>
<blockquote>
<div><ol class="loweralpha simple">
<li>绘制主题索引 <span class="math">z_{di} \sim \mathrm{Multinomial}(\theta_d)</span></li>
<li>绘制观察词 <span class="math">w_{ij} \sim \mathrm{Multinomial}(beta_{z_{di}}.)</span></li>
</ol>
</div></blockquote>
</div></blockquote>
<p>对于参数估计，后验分布为:</p>
<div class="math">
<p><span class="math">p(z, \theta, \beta |w, \alpha, \eta) =
  \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}</span></p>
</div><p>由于后验分布难以处理，变体贝叶斯方法使用更简单的分布 <span class="math">q(z,\theta,\beta | \lambda, \phi, \gamma)</span> 近似，
并且优化了这些变体参数  <span class="math">\lambda</span>, <span class="math">\phi</span>, <span class="math">\gamma</span> 最大化Evidence Lower Bound (ELBO):</p>
<div class="math">
<p><span class="math">\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]</span></p>
</div><p>最大化 ELBO 相当于最小化 <span class="math">q(z,\theta,\beta)</span> 和后验 <span class="math">p(z, \theta, \beta |w, \alpha, \eta)</span> 之间的 Kullback-Leibler(KL) 散度。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 实现在线变体贝叶斯算法，支持在线和批量更新方法。
批处理方法在每次完全传递数据后更新变分变量，在线方法从小批量数据点中更新变体变量。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">虽然在线方法保证收敛到局部最优点，最优点的质量和收敛速度可能取决于与小批量大小和学习率相关的属性。</p>
</div>
<p>当 <a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 应用于 “document-term” 矩阵时，矩阵将被分解为 “topic-term” 矩阵和 “document-topic” 矩阵。
虽然 “topic-term” 矩阵在模型中被存储为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">components_</span></code> ，但是可以通过 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 方法计算 “document-topic” 矩阵。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 还实现了  <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法。这可用于当数据被顺序提取时.</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">“Latent Dirichlet Allocation”</a>
D. Blei, A. Ng, M. Jordan, 2003</li>
<li><a class="reference external" href="https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf">“Online Learning for Latent Dirichlet Allocation”</a>
M. Hoffman, D. Blei, F. Bach, 2010</li>
<li><a class="reference external" href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">“Stochastic Variational Inference”</a>
M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    
    

    

    

    

    
    <!-- modules/decomposition.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@武器大师一个挑俩</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@png</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@柠檬</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@片刻</a><br/>    
    </div>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/decomposition.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="biclustering.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="covariance.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>