

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.1. 广义线性模型 &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2. 线性和二次判别分析" href="lda_qda.html" />
    <link rel="prev" title="1. 监督学习" href="../supervised_learning.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/linear_model.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="../supervised_learning.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      1. 监督学习
      </span>
          <span class="hiddenrellink">
          1. 监督学习
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="lda_qda.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      1.2. 线性和二次判别分析
      </span>
          <span class="hiddenrellink">
          1.2. 线性和二次判别分析
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../supervised_learning.html">
      Up
      <br/>
      <span class="smallrellink">
      1. 监督学习
      </span>
          <span class="hiddenrellink">
          1. 监督学习
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">1.1. 广义线性模型</a><ul>
<li><a class="reference internal" href="#ordinary-least-squares">1.1.1. 普通最小二乘法</a><ul>
<li><a class="reference internal" href="#id3">1.1.1.1. 普通最小二乘法复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ridge-regression">1.1.2. 岭回归</a><ul>
<li><a class="reference internal" href="#id9">1.1.2.1. 岭回归的复杂度</a></li>
<li><a class="reference internal" href="#id10">1.1.2.2. 设置正则化参数：广义交叉验证</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lasso">1.1.3. Lasso</a><ul>
<li><a class="reference internal" href="#id12">1.1.3.1. 设置正则化参数</a><ul>
<li><a class="reference internal" href="#id13">1.1.3.1.1. 使用交叉验证</a></li>
<li><a class="reference internal" href="#id14">1.1.3.1.2. 基于信息标准的模型选择</a></li>
<li><a class="reference internal" href="#svm">1.1.3.1.3. 与 SVM 的正则化参数的比较</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id17">1.1.4. 多任务 Lasso</a></li>
<li><a class="reference internal" href="#elastic-net">1.1.5. 弹性网络</a></li>
<li><a class="reference internal" href="#multi-task-elastic-net">1.1.6. 多任务弹性网络</a></li>
<li><a class="reference internal" href="#least-angle-regression">1.1.7. 最小角回归</a></li>
<li><a class="reference internal" href="#lars-lasso">1.1.8. LARS Lasso</a><ul>
<li><a class="reference internal" href="#id21">1.1.8.1. 数学表达式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#omp">1.1.9. 正交匹配追踪法（OMP）</a></li>
<li><a class="reference internal" href="#bayesian-regression">1.1.10. 贝叶斯回归</a><ul>
<li><a class="reference internal" href="#bayesian-ridge-regression">1.1.10.1. 贝叶斯岭回归</a></li>
<li><a class="reference internal" href="#ard">1.1.10.2. 主动相关决策理论 - ARD</a></li>
</ul>
</li>
<li><a class="reference internal" href="#logistic">1.1.11. logistic 回归</a></li>
<li><a class="reference internal" href="#sgd">1.1.12. 随机梯度下降， SGD</a></li>
<li><a class="reference internal" href="#perceptron">1.1.13. Perceptron（感知器）</a></li>
<li><a class="reference internal" href="#passive-aggressive-algorithms">1.1.14. Passive Aggressive Algorithms（被动攻击算法）</a></li>
<li><a class="reference internal" href="#robustness-regression-outliers">1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误</a><ul>
<li><a class="reference internal" href="#id43">1.1.15.1. 各种使用场景与相关概念</a></li>
<li><a class="reference internal" href="#ransac-random-sample-consensus">1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）</a><ul>
<li><a class="reference internal" href="#id44">1.1.15.2.1. 算法细节</a></li>
</ul>
</li>
<li><a class="reference internal" href="#theil-sen-generalized-median-based-estimator">1.1.15.3. Theil-Sen 预估器: 广义中值估计器（generalized-median-based estimator）</a><ul>
<li><a class="reference internal" href="#id45">1.1.15.3.1. 算法理论细节</a></li>
</ul>
</li>
<li><a class="reference internal" href="#huber">1.1.15.4. Huber 回归</a></li>
<li><a class="reference internal" href="#id48">1.1.15.5. 注意</a></li>
</ul>
</li>
<li><a class="reference internal" href="#polynomial-regression">1.1.16. 多项式回归：用基函数展开线性模型</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="linear-model">
<span id="id1"></span><h1>1.1. 广义线性模型<a class="headerlink" href="#linear-model" title="Permalink to this headline">¶</a></h1>
<p>下面是一组用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 在数学概念中，如果 <span class="math">\hat{y}</span> 是预测值。</p>
<div class="math">
<p><span class="math">\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p</span></p>
</div><p>在整个模块中，我们定义向量 <span class="math">w = (w_1,..., w_p)</span> 作为 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> ，定义 <span class="math">w_0</span> 作为 <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> 。</p>
<p>如果需要使用广义线性模型进行分类，请参阅 <a class="reference internal" href="#logistic-regression"><span class="std std-ref">logistic 回归</span></a> 。</p>
<div class="section" id="ordinary-least-squares">
<span id="id2"></span><h2>1.1.1. 普通最小二乘法<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> 拟合一个带有系数 <span class="math">w = (w_1, ..., w_p)</span> 的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为:</p>
<div class="math">
<p><span class="math">\underset{w}{min\,} {|| X w - y||_2}^2</span></p>
</div><div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ols.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_ols_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_ols_001.png" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> 会调用 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法来拟合数组 X， y，并且将线性模型的系数 <span class="math">w</span> 存储在其成员变量 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span> <span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([ 0.5,  0.5])</span>
</pre></div>
</div>
<p>然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 <span class="math">X</span> 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"><span class="std std-ref">Linear Regression Example</span></a></li>
</ul>
</div>
<div class="section" id="id3">
<h3>1.1.1.1. 普通最小二乘法复杂度<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个 size 为 (n, p) 的矩阵，设 <span class="math">n \geq p</span> ，则该方法的复杂度为 <span class="math">O(n p^2)</span></p>
</div>
</div>
<div class="section" id="ridge-regression">
<span id="id4"></span><h2>1.1.2. 岭回归<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 回归通过对系数的大小施加惩罚来解决 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 的一些问题。 岭系数最小化的是带罚项的残差平方和，</p>
<div class="math">
<p><span class="math">\underset{w}{min\,} {{|| X w - y||_2}^2 + \alpha {||w||_2}^2}</span></p>
</div><p>其中， <span class="math">\alpha \geq 0</span> 是控制系数收缩量的复杂性参数： <span class="math">\alpha</span> 的值越大，收缩量越大，因此系数对共线性的稳定性也更强。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ridge_path.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_ridge_path_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_ridge_path_001.png" /></a>
</div>
<p>与其他线性模型一样， <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 用 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法将模型系数 <span class="math">w</span> 存储在其 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 成员中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span> <span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 
<span class="go">Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,</span>
<span class="go">      normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([ 0.34545455,  0.34545455])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span> 
<span class="go">0.13636...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>:ref:<a href="#id5"><span class="problematic" id="id6">`</span></a>sphx_glr_auto_examples_linear_model_plot_ridge_path.py`( 作为正则化的函数，绘制岭系数 )</li>
<li>:ref:<a href="#id7"><span class="problematic" id="id8">`</span></a>sphx_glr_auto_examples_text_document_classification_20newsgroups.py`( 使用稀疏特征的文本文档分类 )</li>
</ul>
</div>
<div class="section" id="id9">
<h3>1.1.2.1. 岭回归的复杂度<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>这种方法与 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 的复杂度是相同的.</p>
</div>
<div class="section" id="id10">
<h3>1.1.2.2. 设置正则化参数：广义交叉验证<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code></a> 通过内置的 Alpha 参数的交叉验证来实现岭回归。 该对象与 GridSearchCV 的使用方法相同，只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的留一验证方法（LOO-CV）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>       
<span class="go">RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,</span>
<span class="go">    normalize=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>                                      
<span class="go">0.1</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考</p>
<ul class="simple">
<li>“Notes on Regularized Least Squares”, Rifkin &amp; Lippert (<a class="reference external" href="http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf">technical report</a>,
<a class="reference external" href="http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf">course slides</a>).</li>
</ul>
</div>
</div>
</div>
<div class="section" id="lasso">
<span id="id11"></span><h2>1.1.3. Lasso<a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 是估计稀疏系数的线性模型。 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。 因此，Lasso 及其变体是压缩感知领域的基础。 在一定条件下，它可以恢复一组非零权重的精确集（见 <a class="reference internal" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a> ）。</p>
<p>在数学公式表达上，它由一个带有 <span class="math">\ell_1</span> 先验的正则项的线性模型组成。 其最小化的目标函数是:</p>
<div class="math">
<p><span class="math">\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}</span></p>
</div><p>lasso estimate 解决了加上罚项 <span class="math">\alpha ||w||_1</span> 的最小二乘法的最小化，其中， <span class="math">\alpha</span> 是一个常数， <span class="math">||w||_1</span> 是参数向量的 <span class="math">\ell_1</span>-norm 范数。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数。 查看 <a class="reference internal" href="#least-angle-regression"><span class="std std-ref">最小角回归</span></a> ，这是另一种方法:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">   normalize=False, positive=False, precompute=False, random_state=None,</span>
<span class="go">   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 0.8])</span>
</pre></div>
</div>
<p>对于较低级别的任务，同样有用的是函数 <a class="reference internal" href="generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lasso_path</span></code></a> 。它能够通过搜索所有可能的路径上的值来计算系数。</p>
<div class="topic">
<p class="topic-title first">举例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">Lasso and Elastic Net for Sparse Signals</span></a> （稀疏信号的 lasso 和弹性网）</li>
<li><a class="reference internal" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a> （压缩感知：L1 先验（Lasso）的断层扫描重建）</li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>Feature selection with Lasso（使用 Lasso 进行特征选择）</strong></p>
<p class="last">由于 Lasso 回归产生稀疏模型，因此可以用于执行特征选择，详见
<a class="reference internal" href="feature_selection.html#l1-feature-selection"><span class="std std-ref">基于 L1 的特征选取</span></a> （基于 L1 的特征选择）。</p>
</div>
<div class="section" id="id12">
<h3>1.1.3.1. 设置正则化参数<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">alpha</span></code> 参数控制估计系数的稀疏度。</div></blockquote>
<div class="section" id="id13">
<h4>1.1.3.1.1. 使用交叉验证<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<p>scikit-learn 通过交叉验证来公开设置 Lasso <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 参数的对象: <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>。
<a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 是基于下面解释的 <a class="reference internal" href="#least-angle-regression"><span class="std std-ref">最小角回归</span></a> 算法。</p>
<p>对于具有许多线性回归的高维数据集， <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 最常见。 然而，<a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 在寻找 <cite>alpha</cite> 参数值上更具有优势，而且如果样本数量与特征数量相比非常小时，通常 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 比 <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 要快。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_1" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_002.png" /></a> <a class="reference external" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_2" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_003.png" /></a></strong></p></div>
<div class="section" id="id14">
<h4>1.1.3.1.2. 基于信息标准的模型选择<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>有多种选择时，估计器 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsIC</span></code></a> 建议使用 Akaike information criterion （Akaike 信息准则）（AIC）和 Bayes Information criterion （贝叶斯信息准则）（BIC）。
当使用 k-fold 交叉验证时，正则化路径只计算一次而不是 k + 1 次，所以找到 α 的最优值是一种计算上更便宜的替代方法。 然而，这样的标准需要对解决方案的自由度进行适当的估计，对于大样本（渐近结果）导出，并假设模型是正确的，即数据实际上是由该模型生成的。 当问题严重受限（比样本更多的特征）时，他们也倾向于打破。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_001.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>:ref:<a href="#id15"><span class="problematic" id="id16">`</span></a>sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`(Lasso 型号选择：交叉验证/AIC/BIC)</li>
</ul>
</div>
</div>
<div class="section" id="svm">
<h4>1.1.3.1.3. 与 SVM 的正则化参数的比较<a class="headerlink" href="#svm" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> 和 SVM 的正则化参数``C`` 之间的等式关系是 <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">C</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(n_samples</span> <span class="pre">*</span> <span class="pre">C)</span></code> ，并依赖于估计器和模型优化的确切的目标函数。
.. _multi_task_lasso:</p>
</div>
</div>
</div>
<div class="section" id="id17">
<h2>1.1.4. 多任务 Lasso<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a> 是一个估计多元回归稀疏系数的线性模型： <code class="docutils literal notranslate"><span class="pre">y</span></code> 是一个 <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_tasks)</span></code> 的二维数组，其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。</div></blockquote>
<p>下图比较了通过使用简单的 Lasso 或 MultiTaskLasso 得到的 W 中非零的位置。 Lasso 估计产生分散的非零值，而 MultiTaskLasso 的一整列都是非零的。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_1" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_001.png" /></a> <a class="reference external" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_2" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_002.png" /></a></strong></p><p class="centered">
<strong>拟合 time-series model （时间序列模型），强制任何活动的功能始终处于活动状态。</strong></p><div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py"><span class="std std-ref">Joint feature selection with multi-task Lasso</span></a> （联合功能选择与多任务 Lasso）</li>
</ul>
</div>
<p>在数学上，它由一个线性模型组成，以混合的 <span class="math">\ell_1</span> <span class="math">\ell_2</span> 作为正则化器进行训练。目标函数最小化是：</p>
<div class="math">
<p><span class="math">\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro} ^ 2 + \alpha ||W||_{21}}</span></p>
</div><p>其中 <span class="math">Fro</span> 表示 Frobenius 标准：</p>
<div class="math">
<p><span class="math">||A||_{Fro} = \sqrt{\sum_{ij} a_{ij}^2}</span></p>
</div><p>并且 <span class="math">\ell_1</span> <span class="math">\ell_2</span> 读取为:</p>
<div class="math">
<p><span class="math">||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}</span></p>
</div><p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a> 类的实现使用了坐标下降作为拟合系数的算法。</p>
</div>
<div class="section" id="elastic-net">
<span id="id18"></span><h2>1.1.5. 弹性网络<a class="headerlink" href="#elastic-net" title="Permalink to this headline">¶</a></h2>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">弹性网络</span></code> 是一种使用 L1， L2 范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 <a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 一样，但是它仍然保持
一些像 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 的正则性质。我们可利用 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> 参数控制 L1 和 L2 的凸组合。</p>
<p>弹性网络在很多特征互相联系的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p>在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在循环过程（Under rotate）中继承 Ridge 的稳定性。</p>
<p>在这里，最小化的目标函数是</p>
<div class="math">
<p><span class="math">\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}</span></p>
</div><div class="figure" id="id50">
<img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_coordinate_descent_path_001.png:target:../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html:align:center:scale:50%" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_coordinate_descent_path_001.png:target:../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html:align:center:scale:50%" />
<p class="caption"><span class="caption-text"><a class="reference internal" href="generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a> 类可以通过交叉验证来设置参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> （ <span class="math">\alpha</span> ） 和 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> （ <span class="math">\rho</span> ） 。</span></p>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">Lasso and Elastic Net for Sparse Signals</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py"><span class="std std-ref">Lasso and Elastic Net</span></a></li>
</ul>
</div>
</div>
<div class="section" id="multi-task-elastic-net">
<span id="id19"></span><h2>1.1.6. 多任务弹性网络<a class="headerlink" href="#multi-task-elastic-net" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a> 是一个对多回归问题估算稀疏参数的弹性网络: <code class="docutils literal notranslate"><span class="pre">Y</span></code> 是一个二维数组，形状是 <code class="docutils literal notranslate"><span class="pre">(n_samples,n_tasks)</span></code>。  其限制条件是和其他回归问题一样，是选择的特征，也称为 tasks 。</div></blockquote>
<p>从数学上来说， 它包含一个混合的 <span class="math">\ell_1</span> <span class="math">\ell_2</span> 先验和 <span class="math">\ell_2</span> 先验为正则项训练的线性模型
目标函数就是最小化:</p>
<div class="math">
<p><span class="math">\underset{W}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro}^2 + \alpha \rho ||W||_{2 1} +
\frac{\alpha(1-\rho)}{2} ||W||_{Fro}^2}</span></p>
</div><p>在 <a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a> 类中的实现采用了坐标下降法求解参数。</p>
<p>在 <a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a> 中可以通过交叉验证来设置参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> （ <span class="math">\alpha</span> ） 和 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> （ <span class="math">\rho</span> ） 。</p>
</div>
<div class="section" id="least-angle-regression">
<span id="id20"></span><h2>1.1.7. 最小角回归<a class="headerlink" href="#least-angle-regression" title="Permalink to this headline">¶</a></h2>
<p>最小角回归 （LARS） 是对高维数据的回归算法， 由 Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 开发完成。 LARS 和逐步回归很像。在每一步，它寻找与响应最有关联的
预测。当有很多预测有相同的关联时，它没有继续利用相同的预测，而是在这些预测中找出应该等角的方向。</p>
<p>LARS的优点:</p>
<blockquote>
<div><ul class="simple">
<li>当 p &gt;&gt; n，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)</li>
<li>它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。</li>
<li>它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。</li>
<li>如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉
上的判断一样，而且还更加稳定。</li>
<li>它很容易修改并为其他估算器生成解，比如Lasso。</li>
</ul>
</div></blockquote>
<p>LARS 的缺点:</p>
<blockquote>
<div><ul class="simple">
<li>因为 LARS 是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。这个问题，在 2004 年统计年鉴的文章由 Weisberg 详细讨论。</li>
</ul>
</div></blockquote>
<p>LARS 模型可以在 <a class="reference internal" href="generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lars</span></code></a> ，或者它的底层实现 <a class="reference internal" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lars_path</span></code></a> 中被使用。</p>
</div>
<div class="section" id="lars-lasso">
<h2>1.1.8. LARS Lasso<a class="headerlink" href="#lars-lasso" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLars</span></code></a> 是一个使用 LARS 算法的 lasso 模型，不同于基于坐标下降法的实现，它可以得到一个精确解，也就是一个关于自身参数标准化后的一个分段线性解。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_lasso_lars.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_lars_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_lasso_lars_001.png" /></a>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLars</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  
<span class="go">LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,</span>
<span class="go">     fit_path=True, max_iter=500, normalize=True, positive=False,</span>
<span class="go">     precompute=&#39;auto&#39;, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>    
<span class="go">array([ 0.717157...,  0.        ])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py"><span class="std std-ref">Lasso path using LARS</span></a></li>
</ul>
</div>
<p>Lars 算法提供了一个几乎无代价的沿着正则化参数的系数的完整路径，因此常利用函数 <a class="reference internal" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lars_path</span></code></a> 来取回路径。</p>
<div class="section" id="id21">
<h3>1.1.8.1. 数学表达式<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>该算法和逐步回归非常相似，但是它没有在每一步包含变量，它估计的参数是根据与
其他剩余变量的联系来增加的。</p>
<p>在 LARS 的解中，没有给出一个向量的结果，而是给出一条曲线，显示参数向量的 L1 范式的每个值的解。
完全的参数路径存在 <code class="docutils literal notranslate"><span class="pre">coef_path_</span></code> 下。它的 size 是 (n_features, max_features+1)。 其中第一列通常是全 0 列。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Original Algorithm is detailed in the paper <a class="reference external" href="http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">Least Angle Regression</a>
by Hastie et al.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="omp">
<span id="id23"></span><h2>1.1.9. 正交匹配追踪法（OMP）<a class="headerlink" href="#omp" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code class="xref py py-class docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuit</span></code></a> (正交匹配追踪法)和 <a class="reference internal" href="generated/sklearn.linear_model.orthogonal_mp.html#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code class="xref py py-func docutils literal notranslate"><span class="pre">orthogonal_mp</span></code></a></div></blockquote>
<p>使用了 OMP 算法近似拟合了一个带限制的线性模型，该限制影响于模型的非 0 系数(例：L0 范数)。</p>
<p>就像最小角回归一样，作为一个前向特征选择方法，正交匹配追踪法可以近似一个固定非 0 元素的最优向量解:</p>
<div class="math">
<p><span class="math">\text{arg\,min\,} ||y - X\gamma||_2^2 \text{ subject to } \
||\gamma||_0 \leq n_{nonzero\_coefs}</span></p>
</div><p>正交匹配追踪法也可以针对一个特殊的误差而不是一个特殊的非零系数的个数。可以表示为:</p>
<div class="math">
<p><span class="math">\text{arg\,min\,} ||\gamma||_0 \text{ subject to } ||y-X\gamma||_2^2 \
\leq \text{tol}</span></p>
</div><p>OMP 是基于每一步的贪心算法，其每一步元素都是与当前残差高度相关的。它跟较为简单的匹配追踪（MP）很相似，但是相比 MP 更好，在每一次迭代中，可以利用正交投影到之前选择的字典元素重新计算残差。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py"><span class="std std-ref">Orthogonal Matching Pursuit</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></li>
<li><a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">Matching pursuits with time-frequency dictionaries</a>,
S. G. Mallat, Z. Zhang,</li>
</ul>
</div>
</div>
<div class="section" id="bayesian-regression">
<span id="id24"></span><h2>1.1.10. 贝叶斯回归<a class="headerlink" href="#bayesian-regression" title="Permalink to this headline">¶</a></h2>
<p>贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。</p>
<p>上述过程可以通过引入 <a class="reference external" href="https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors">无信息先验</a> 于模型中的超参数来完成。
在 <cite>岭回归</cite> 中使用的 <span class="math">\ell_{2}</span> 正则项相当于在 <span class="math">w</span> 为高斯先验条件下，且此先验的精确度为 <span class="math">\lambda^{-1}</span> 求最大后验估计。在这里，我们没有手工调参数 lambda ，而是让他作为一个变量，通过数据中估计得到。</p>
<p>为了得到一个全概率模型，输出 <span class="math">y</span> 也被认为是关于 <span class="math">X w</span> 的高斯分布。</p>
<div class="math">
<p><span class="math">p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha)</span></p>
</div><p>Alpha 在这里也是作为一个变量，通过数据中估计得到。</p>
<p>贝叶斯回归有如下几个优点:</p>
<blockquote>
<div><ul class="simple">
<li>它能根据已有的数据进行改变。</li>
<li>它能在估计过程中引入正则项。</li>
</ul>
</div></blockquote>
<p>贝叶斯回归有如下缺点:</p>
<blockquote>
<div><ul class="simple">
<li>它的推断过程是非常耗时的。</li>
</ul>
</div></blockquote>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>一个对于贝叶斯方法的很好的介绍 C. Bishop: Pattern Recognition and Machine learning</li>
<li>详细介绍原创算法的一本书 <cite>Bayesian learning for neural networks</cite> by Radford M. Neal</li>
</ul>
</div>
<div class="section" id="bayesian-ridge-regression">
<span id="id25"></span><h3>1.1.10.1. 贝叶斯岭回归<a class="headerlink" href="#bayesian-ridge-regression" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge" title="sklearn.linear_model.BayesianRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">BayesianRidge</span></code></a> 利用概率模型估算了上述的回归问题，其先验参数 <span class="math">w</span> 是由以下球面高斯公式得出的：</div></blockquote>
<div class="math">
<p><span class="math">p(w|\lambda) =
\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})</span></p>
</div><p>先验参数 <span class="math">\alpha</span> 和 <span class="math">\lambda</span> 一般是服从 <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma 分布</a> ， 这个分布与高斯成共轭先验关系。</p>
<p>得到的模型一般称为 <em>贝叶斯岭回归</em>， 并且这个与传统的 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 非常相似。参数 <span class="math">w</span> ， <span class="math">\alpha</span> 和 <span class="math">\lambda</span> 是在模型拟合的时候一起被估算出来的。 剩下的超参数就是
关于:math:<cite>alpha</cite> 和 <span class="math">\lambda</span>  的 gamma 分布的先验了。 它们通常被选择为 <em>无信息先验</em> 。模型参数的估计一般利用最大 <em>边缘似然对数估计</em> 。</p>
<p>默认 <span class="math">\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}</span>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_bayesian_ridge.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_bayesian_ridge_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_bayesian_ridge_001.png" /></a>
</div>
<p>贝叶斯岭回归用来解决回归问题:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,</span>
<span class="go">       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,</span>
<span class="go">       normalize=False, tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>在模型训练完成后，可以用来预测新值:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span> <span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="go">array([ 0.50000013])</span>
</pre></div>
</div>
<p>权值 <span class="math">w</span> 可以被这样访问:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([ 0.49999993,  0.49999993])</span>
</pre></div>
</div>
<p>由于贝叶斯框架的缘故，权值与 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 产生的不太一样。
但是，贝叶斯岭回归对病态问题（ill-posed）的鲁棒性要更好。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py"><span class="std std-ref">Bayesian Ridge Regression</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>更多细节可以参考 <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;rep=rep1&amp;type=pdf">Bayesian Interpolation</a>
by MacKay, David J. C.</li>
</ul>
</div>
</div>
<div class="section" id="ard">
<h3>1.1.10.2. 主动相关决策理论 - ARD<a class="headerlink" href="#ard" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">ARDRegression</span></code></a> （主动相关决策理论）和 <a href="#id51"><span class="problematic" id="id52">`Bayesian Ridge Regression`_</span></a> 非常相似，</div></blockquote>
<dl class="docutils">
<dt>但是会导致一个更加稀疏的权重 <span class="math">w</span> <a class="footnote-reference" href="#id30" id="id26">[1]</a> <a class="footnote-reference" href="#id31" id="id27">[2]</a> 。</dt>
<dd><a class="reference internal" href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">ARDRegression</span></code></a> 提出了一个不同的 <span class="math">w</span> 的先验假设。具体来说，就是弱化了高斯分布为球形的假设。</dd>
</dl>
<p>它采用 <span class="math">w</span> 分布是与轴平行的椭圆高斯分布。</p>
<p>也就是说，每个权值 <span class="math">w_{i}</span> 从一个中心在 0 点，精度为 <span class="math">\lambda_{i}</span> 的高斯分布中采样得到的。</p>
<div class="math">
<p><span class="math">p(w|\lambda) = \mathcal{N}(w|0,A^{-1})</span></p>
</div><p>并且 <span class="math">diag \; (A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}</span>.</p>
<p>与 <a href="#id53"><span class="problematic" id="id54">`Bayesian Ridge Regression`_</span></a> 不同， 每个 <span class="math">w_{i}</span> 都有一个标准差 <span class="math">\lambda_i</span> 。所有 <span class="math">\lambda_i</span> 的先验分布
由超参数 <span class="math">\lambda_1</span> 、 <span class="math">\lambda_2</span> 确定的相同的 gamma 分布确定。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ard.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_ard_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_ard_001.png" /></a>
</div>
<p>ARD 也被称为 <em>稀疏贝叶斯学习</em> 或 <em>相关向量机</em> <a class="footnote-reference" href="#id32" id="id28">[3]</a> <a class="footnote-reference" href="#id33" id="id29">[4]</a> 。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py"><span class="std std-ref">Automatic Relevance Determination Regression (ARD)</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="id30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id26">[1]</a></td><td>Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id27">[2]</a></td><td>David Wipf and Srikantan Nagarajan: <a class="reference external" href="http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf">A new view of automatic relevance determination</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id28">[3]</a></td><td>Michael E. Tipping: <a class="reference external" href="http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf">Sparse Bayesian Learning and the Relevance Vector Machine</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id29">[4]</a></td><td>Tristan Fletcher: <a class="reference external" href="http://www.tristanfletcher.co.uk/RVM%20Explained.pdf">Relevance Vector Machines explained</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="logistic">
<span id="logistic-regression"></span><h2>1.1.11. logistic 回归<a class="headerlink" href="#logistic" title="Permalink to this headline">¶</a></h2>
<p>logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。在某些文献中，logistic 回归又被称作 logit 回归，maximum-entropy classification（MaxEnt，最大熵分类），或 log-linear classifier（对数线性分类器）。该模型利用函数 <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>
将单次试验（single trial）的可能结果输出为概率。</p>
<p>scikit-learn 中 logistic 回归在 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p>
<p>作为优化问题，带 L2 罚项的二分类 logistic 回归要最小化以下代价函数（cost function）：</p>
<div class="math">
<p><span class="math">\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) .</span></p>
</div><p>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p>
<div class="math">
<p><span class="math">\underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) .</span></p>
</div><p>在 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 类中实现了这些优化算法: “liblinear”， “newton-cg”， “lbfgs”， “sag” 和 “saga”。</p>
<p>“liblinear” 应用了坐标下降算法（Coordinate Descent, CD），并基于 scikit-learn 内附的高性能 C++ 库 <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR library</a> 实现。不过 CD 算法训练的模型不是真正意义上的多分类模型，而是基于 “one-vs-rest” 思想分解了这个优化问题，为每个类别都训练了一个二元分类器。因为实现在底层使用该求解器的 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 实例对象表面上看是一个多元分类器。 <a class="reference internal" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.svm.l1_min_c</span></code></a> 可以计算使用 L1 罚项时 C 的下界，以避免模型为空（即全部特征分量的权重为零）。</p>
<p>“lbfgs”, “sag” 和 “newton-cg” solvers （求解器）只支持 L2 惩罚项，对某些高维数据收敛更快。这些求解器的参数 <a href="#id34"><span class="problematic" id="id35">`</span></a>multi_class`设为 “multinomial” 即可训练一个真正的多项式 logistic 回归 <a class="footnote-reference" href="#id39" id="id36">[5]</a> ，其预测的概率比默认的 “one-vs-rest” 设定更为准确。</p>
<p>“sag” 求解器基于平均随机梯度下降算法（Stochastic Average Gradient descent） <a class="footnote-reference" href="#id40" id="id37">[6]</a>。在大数据集上的表现更快，大数据集指样本量大且特征数多。</p>
<p>“saga” 求解器 <a class="footnote-reference" href="#id41" id="id38">[7]</a> 是 “sag” 的一类变体，它支持非平滑（non-smooth）的 L1 正则选项 <code class="docutils literal notranslate"><span class="pre">penalty=&quot;l1&quot;</span></code> 。因此对于稀疏多项式 logistic 回归 ，往往选用该求解器。</p>
<p>一言以蔽之，选用求解器可遵循如下规则:</p>
<table border="1" class="docutils">
<colgroup>
<col width="45%" />
<col width="55%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Case</th>
<th class="head">Solver</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>L1正则</td>
<td>“liblinear” or “saga”</td>
</tr>
<tr class="row-odd"><td>多项式损失（multinomial loss）</td>
<td>“lbfgs”, “sag”, “saga” or “newton-cg”</td>
</tr>
<tr class="row-even"><td>大数据集（<cite>n_samples</cite>）</td>
<td>“sag” or “saga”</td>
</tr>
</tbody>
</table>
<p>“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear” 。</p>
<p>对于大数据集，还可以用 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> ，并使用对数损失（’log’ loss）</p>
<div class="topic">
<p class="topic-title first">示例：</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py"><span class="std std-ref">L1 Penalty and Sparsity in Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py"><span class="std std-ref">Path with L1- Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py"><span class="std std-ref">Plot multinomial and One-vs-Rest Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py"><span class="std std-ref">Multiclass sparse logisitic regression on newgroups20</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py"><span class="std std-ref">MNIST classfification using multinomial logistic + L1</span></a></li>
</ul>
</div>
<div class="topic" id="liblinear-differences">
<p class="topic-title first">与 liblinear 的区别:</p>
<p>当 <code class="docutils literal notranslate"><span class="pre">fit_intercept=False</span></code> 拟合得到的 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 或者待预测的数据为零时，用 <code class="docutils literal notranslate"><span class="pre">solver=liblinear</span></code> 的 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a>
或 <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code> 与直接使用外部 liblinear 库预测得分会有差异。这是因为，
对于 <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> 为零的样本， <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 和 <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code>
将预测为负类，而 liblinear 预测为正类。
注意，设定了 <code class="docutils literal notranslate"><span class="pre">fit_intercept=False</span></code> ，又有很多样本使得 <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> 为零的模型，很可能会欠拟合，其表现往往比较差。建议您设置 <code class="docutils literal notranslate"><span class="pre">fit_intercept=True</span></code> 并增大 <code class="docutils literal notranslate"><span class="pre">intercept_scaling</span></code> 。</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>利用稀疏 logistic 回归进行特征选择</strong></p>
<blockquote>
<div>带 L1 罚项的 logistic 回归 将得到稀疏模型（sparse model），相当于进行了特征选择（feature selection），详情参见 <a class="reference internal" href="feature_selection.html#l1-feature-selection"><span class="std std-ref">基于 L1 的特征选取</span></a> 。</div></blockquote>
<p class="last"><a class="reference internal" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegressionCV</span></code></a> 对 logistic 回归 的实现内置了交叉验证（cross-validation），可以找出最优的参数 C 。”newton-cg”， “sag”， “saga” 和 “lbfgs” 在高维数据上更快，因为采用了热启动（warm-starting）。
在多分类设定下，若 <cite>multi_class</cite> 设为 “ovr” ，会为每类求一个最佳的 C 值；若 <cite>multi_class</cite> 设为 “multinomial” ，会通过交叉熵损失（cross-entropy loss）求出一个最佳 C 值。</p>
</div>
<div class="topic">
<p class="topic-title first">参考文献：</p>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id36">[5]</a></td><td>Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id40" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id37">[6]</a></td><td>Mark Schmidt, Nicolas Le Roux, and Francis Bach: <a class="reference external" href="https://hal.inria.fr/hal-00860051/document">Minimizing Finite Sums with the Stochastic Average Gradient.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id41" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id38">[7]</a></td><td>Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <a class="reference external" href="https://arxiv.org/abs/1407.0202">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</a></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="sgd">
<h2>1.1.12. 随机梯度下降， SGD<a class="headerlink" href="#sgd" title="Permalink to this headline">¶</a></h2>
<p>随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。
方法 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。
例如，设定 <code class="docutils literal notranslate"><span class="pre">loss=&quot;log&quot;</span></code> ，则 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 拟合一个逻辑斯蒂回归模型，而 <code class="docutils literal notranslate"><span class="pre">loss=&quot;hinge&quot;</span></code> 拟合线性支持向量机（SVM）。</p>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li><a class="reference internal" href="sgd.html#sgd"><span class="std std-ref">随机梯度下降</span></a></li>
</ul>
</div>
</div>
<div class="section" id="perceptron">
<span id="id42"></span><h2>1.1.13. Perceptron（感知器）<a class="headerlink" href="#perceptron" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perceptron</span></code></a> 是适用于大规模学习的一种简单算法。默认情况下：</p>
<blockquote>
<div><ul class="simple">
<li>不需要设置学习率（learning rate）。</li>
<li>不需要正则化处理。</li>
<li>仅使用错误样本更新模型。</li>
</ul>
</div></blockquote>
<p>最后一点表明使用合页损失（hinge loss）的感知机比 SGD 略快，所得模型更稀疏。</p>
</div>
<div class="section" id="passive-aggressive-algorithms">
<span id="passive-aggressive"></span><h2>1.1.14. Passive Aggressive Algorithms（被动攻击算法）<a class="headerlink" href="#passive-aggressive-algorithms" title="Permalink to this headline">¶</a></h2>
<p>被动攻击算法是大规模学习的一类算法。和感知机类似，它也不需要设置学习率，不过比感知机多出一个正则化参数 <code class="docutils literal notranslate"><span class="pre">C</span></code> 。</p>
<p>对于分类问题， <a class="reference internal" href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">PassiveAggressiveClassifier</span></code></a> 可设定
<code class="docutils literal notranslate"><span class="pre">loss='hinge'</span></code> （PA-I）或 <code class="docutils literal notranslate"><span class="pre">loss='squared_hinge'</span></code> （PA-II）。对于回归问题，
<a class="reference internal" href="generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">PassiveAggressiveRegressor</span></code></a> 可设置
<code class="docutils literal notranslate"><span class="pre">loss='epsilon_insensitive'</span></code> （PA-I）或
<code class="docutils literal notranslate"><span class="pre">loss='squared_epsilon_insensitive'</span></code> （PA-II）。</p>
<div class="topic">
<p class="topic-title first">参考文献：</p>
<ul class="simple">
<li><a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">“Online Passive-Aggressive Algorithms”</a>
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</li>
</ul>
</div>
</div>
<div class="section" id="robustness-regression-outliers">
<h2>1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误<a class="headerlink" href="#robustness-regression-outliers" title="Permalink to this headline">¶</a></h2>
<p>稳健回归（robust regression）特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png" /></a>
</div>
<div class="section" id="id43">
<h3>1.1.15.1. 各种使用场景与相关概念<a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h3>
<p>处理包含离群点的数据时牢记以下几点:</p>
<ul>
<li><p class="first"><strong>离群值在 X 上还是在 y 方向上</strong>?</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">离群值在 y 方向上</th>
<th class="head">离群值在 X 方向上</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_003.png" /></a></td>
<td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="X_outliers" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_002.png" /></a></td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><strong>离群点的比例 vs. 错误的量级（amplitude）</strong></p>
<p>离群点的数量很重要，离群程度也同样重要。</p>
</li>
</ul>
<p>稳健拟合（robust fitting）的一个重要概念是崩溃点（breakdown point），即拟合模型（仍准确预测）所能承受的离群值最大比例。</p>
<p>注意，在高维数据条件下（ <cite>n_features</cite> 大），一般而言很难完成稳健拟合，很可能完全不起作用。</p>
<div class="topic">
<p class="topic-title first"><strong>折中： 预测器的选择</strong></p>
<blockquote>
<div><p>Scikit-learn提供了三种稳健回归的预测器（estimator）: <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> ， <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> 和 <a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a></p>
<ul class="simple">
<li><a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a> 一般快于 <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> 和 <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> ，除非样本数很大，即 <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> &gt;&gt; <code class="docutils literal notranslate"><span class="pre">n_features</span></code> 。
这是因为 <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> 和 <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> 都是基于数据的较小子集进行拟合。但使用默认参数时， <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> 和 <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> 可能不如 <a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a> 鲁棒。</li>
<li><a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> 比 <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> 更快，在样本数量上的伸缩性（适应性）更好。</li>
<li><a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> 能更好地处理y方向的大值离群点（通常情况下）。</li>
<li><a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> 能更好地处理x方向中等大小的离群点，但在高维情况下无法保证这一特点。</li>
</ul>
</div></blockquote>
<p>实在决定不了的话，请使用 <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a></p>
</div>
</div>
<div class="section" id="ransac-random-sample-consensus">
<span id="ransac-regression"></span><h3>1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）<a class="headerlink" href="#ransac-random-sample-consensus" title="Permalink to this headline">¶</a></h3>
<p>随机抽样一致性算法（RANdom SAmple Consensus， RANSAC）利用全体数据中局内点（inliers）的一个随机子集拟合模型。</p>
<p>RANSAC 是一种非确定性算法，以一定概率输出一个可能的合理结果，依赖于迭代次数（参数 <cite>max_trials</cite> ）。这种算法主要解决线性或非线性回归问题，在计算机视觉摄影测绘领域尤为流行。</p>
<p>算法从全体样本输入中分出一个局内点集合，全体样本可能由于测量错误或对数据的假设错误而含有噪点、离群点。最终的模型仅从这个局内点集合中得出。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ransac.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_ransac_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_ransac_001.png" /></a>
</div>
<div class="section" id="id44">
<h4>1.1.15.2.1. 算法细节<a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h4>
<p>每轮迭代执行以下步骤:</p>
<ol class="arabic simple">
<li>从原始数据中抽样 <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> 数量的随机样本，检查数据是否合法（见 <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> ）。</li>
<li>用一个随机子集拟合模型（ <code class="docutils literal notranslate"><span class="pre">base_estimator.fit</span></code> ）。检查模型是否合法（见 <code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code> ）。</li>
<li>计算预测模型的残差（residual），将全体数据分成局内点和离群点（ <code class="docutils literal notranslate"><span class="pre">base_estimator.predict(X)</span> <span class="pre">-</span> <span class="pre">y</span></code> ）</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>绝对残差小于 <code class="docutils literal notranslate"><span class="pre">residual_threshold</span></code> 的全体数据认为是局内点。</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li>若局内点样本数最大，保存当前模型为最佳模型。以免当前模型离群点数量恰好相等（而出现未定义情况），规定仅当数值大于当前最值时认为是最佳模型。</li>
</ol>
<p>上述步骤或者迭代到最大次数（ <code class="docutils literal notranslate"><span class="pre">max_trials</span></code> ），或者某些终止条件满足时停下（见 <code class="docutils literal notranslate"><span class="pre">stop_n_inliers</span></code> 和 <code class="docutils literal notranslate"><span class="pre">stop_score</span></code> )。最终模型由之前确定的最佳模型的局内点样本（一致性集合，consensus set）预测。</p>
<p>函数 <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> 和 <code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code> 可以识别出随机样本子集中的退化组合（degenerate combinations）并予以丢弃（reject）。即便不需要考虑退化情况，也会使用 <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> ，因为在拟合模型之前调用它能得到更高的计算性能。</p>
<div class="topic">
<p class="topic-title first">示例：</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py"><span class="std std-ref">Robust linear model estimation using RANSAC</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="std std-ref">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献：</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></li>
<li><a class="reference external" href="http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf">“Random Sample Consensus: A Paradigm for Model Fitting with Applications to
Image Analysis and Automated Cartography”</a>
Martin A. Fischler and Robert C. Bolles - SRI International (1981)</li>
<li><a class="reference external" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">“Performance Evaluation of RANSAC Family”</a>
Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="theil-sen-generalized-median-based-estimator">
<span id="theil-sen-regression"></span><h3>1.1.15.3. Theil-Sen 预估器: 广义中值估计器（generalized-median-based estimator）<a class="headerlink" href="#theil-sen-generalized-median-based-estimator" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> 估计器：使用中位数在多个维度泛化，对多元异常值更具有鲁棒性，但问题是，随着维数的增加，估计器的准确性在迅速下降。准确性的丢失，导致在高维上的估计值比不上普通的最小二乘法。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="std std-ref">Theil-Sen Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="std std-ref">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</a></li>
</ul>
</div>
<div class="section" id="id45">
<h4>1.1.15.3.1. 算法理论细节<a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> 在渐近效率和无偏估计方面足以媲美 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">Ordinary Least Squares (OLS)</span></a> （普通最小二乘法（OLS））。与 OLS 不同的是， Theil-Sen 是一种非参数方法，这意味着它没有对底层数据的分布假设。由于 Theil-Sen 是基于中值的估计，它更适合于损坏的数据即离群值。
在单变量的设置中，Theil-Sen 在简单的线性回归的情况下，其崩溃点大约 29.3% ，这意味着它可以容忍任意损坏的数据高达 29.3% 。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png" /></a>
</div>
<p>scikit-learn 中实现的 <a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> 是多元线性回归模型的推广 <a class="footnote-reference" href="#f1" id="id46">[8]</a> ，利用了空间中值方法，它是多维中值的推广 <a class="footnote-reference" href="#f2" id="id47">[9]</a> 。</p>
<p>关于时间复杂度和空间复杂度，Theil-Sen 的尺度根据</p>
<div class="math">
<p><span class="math">\binom{n_{samples}}{n_{subsamples}}</span></p>
</div><p>这使得它不适用于大量样本和特征的问题。因此，可以选择一个亚群的大小来限制时间和空间复杂度，只考虑所有可能组合的随机子集。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="std std-ref">Theil-Sen Regression</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="f1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id46">[8]</a></td><td>Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <a class="reference external" href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">Theil-Sen Estimators in a Multiple Linear Regression Model.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="f2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id47">[9]</a></td><td><ol class="first last upperalpha simple" start="20">
<li>Kärkkäinen and S. Äyrämö: <a class="reference external" href="http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf">On Computation of Spatial Median for Robust Data Mining.</a></li>
</ol>
</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="huber">
<span id="huber-regression"></span><h3>1.1.15.4. Huber 回归<a class="headerlink" href="#huber" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 与 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 不同，因为它对于被分为异常值的样本应用了一个线性损失。如果这个样品的绝对误差小于某一阈值，样品就被分为内围值。
它不同于 <a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor" title="sklearn.linear_model.RANSACRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RANSACRegressor</span></code></a> ，因为它没有忽略异常值的影响，并分配给它们较小的权重。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_huber_vs_ridge.html"><img alt="auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png" src="auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png" /></a>
</div>
<p>这个 <a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 最小化的损失函数是：</p>
<div class="math">
<p><span class="math">\underset{w, \sigma}{min\,} {\sum_{i=1}^n\left(\sigma + H_m\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}</span></p>
</div><p>其中</p>
<div class="math">
<p><span class="math">H_m(z) = \begin{cases}
       z^2, &amp; \text {if } |z| &lt; \epsilon, \\
       2\epsilon|z| - \epsilon^2, &amp; \text{otherwise}
\end{cases}</span></p>
</div><p>建议设置参数 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> 为 1.35 以实现 95% 统计效率。</p>
</div>
<div class="section" id="id48">
<h3>1.1.15.5. 注意<a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 与将损失设置为 <cite>huber</cite> 的 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 并不相同，体现在以下方面的使用方式上。</p>
<ul class="simple">
<li><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 是标度不变性的. 一旦设置了 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> ， 通过不同的值向上或向下缩放 <code class="docutils literal notranslate"><span class="pre">X</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y</span></code> ，就会跟以前一样对异常值产生同样的鲁棒性。相比 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 其中 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> 在 <code class="docutils literal notranslate"><span class="pre">X</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y</span></code> 被缩放的时候必须再次设置。</li>
<li><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 应该更有效地使用在小样本数据，同时 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 需要一些训练数据的 passes 来产生一致的鲁棒性。</li>
</ul>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="std std-ref">HuberRegressor vs Ridge on dataset with strong outliers</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</li>
</ul>
</div>
<p>另外，这个估计是不同于 R 实现的 Robust Regression (<a class="reference external" href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>) ，因为 R 实现加权最小二乘，权重考虑到每个样本并基于残差大于某一阈值的量。</p>
</div>
</div>
<div class="section" id="polynomial-regression">
<span id="id49"></span><h2>1.1.16. 多项式回归：用基函数展开线性模型<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>机器学习中一种常见的模式，是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>
<p>例如，可以通过构造系数的 <strong>polynomial features</strong> 来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型:</p>
<div class="math">
<p><span class="math">\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2</span></p>
</div><p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>
<div class="math">
<p><span class="math">\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2</span></p>
</div><p>观察到这 <em>还是一个线性模型</em> （这有时候是令人惊讶的）: 看到这个，想象创造一个新的变量</p>
<div class="math">
<p><span class="math">z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]</span></p>
</div><p>有了这些重新标记的数据，我们可以将问题写成</p>
<div class="math">
<p><span class="math">\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5</span></p>
</div><p>我们看到，所得的 <em>polynomial regression</em> 与我们上文所述线性模型是同一类（即关于 <span class="math">w</span> 是线性的），因此可以用同样的方法解决。通过用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>
<p>这里是一个例子，使用不同程度的多项式特征将这个想法应用于一维数据:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_polynomial_interpolation.html"><img alt="modules/../auto_examples/linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png" src="modules/../auto_examples/linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png" /></a>
</div>
<p>这个图是使用&nbsp;<a class="reference internal" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code></a>&nbsp;预创建。该预处理器将输入数据矩阵转换为给定度的新数据矩阵。使用方法如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span>
<span class="go">       [  1.,   2.,   3.,   4.,   6.,   9.],</span>
<span class="go">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">X</span></code> 的特征已经从 <span class="math">[x_1, x_2]</span> 转换到 <span class="math">[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]</span>, 并且现在可以用在任何线性模型。</p>
<p>这种预处理可以通过 <a class="reference internal" href="pipeline.html#pipeline"><span class="std std-ref">Pipeline</span></a> 工具进行简化。可以创建一个表示简单多项式回归的单个对象，使用方法如下所示:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="k">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)),</span>
<span class="gp">... </span>                  <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">))])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># fit to an order-3 polynomial data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([ 3., -2.,  1., -1.])</span>
</pre></div>
</div>
<p>利用多项式特征训练的线性模型能够准确地恢复输入多项式系数。</p>
<p>在某些情况下，没有必要包含任何单个特征的更高的幂，只需要相乘最多 <span class="math">d</span> 个不同的特征即可，所谓 <em>interaction features（交互特征）</em> 。这些可通过设定 <a class="reference internal" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code></a> 的 <code class="docutils literal notranslate"><span class="pre">interaction_only=True</span></code> 得到。</p>
<p>例如，当处理布尔属性，对于所有 <span class="math">n</span>  <span class="math">x_i^n = x_i</span> ，因此是无用的；但 <span class="math">x_i x_j</span> 代表两布尔结合。这样我们就可以用线性分类器解决异或问题:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Perceptron</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">^</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">array([0, 1, 1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [1, 0, 1, 0],</span>
<span class="go">       [1, 1, 0, 0],</span>
<span class="go">       [1, 1, 1, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>分类器的 “predictions” 是完美的:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 1, 1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    
    <!-- modules/linear_model.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@专业吹牛逼的小明</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Gladiator</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@瓜牛</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@年纪大了反应慢了</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Hazekiah</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@BWM-蜜蜂</a><br/>  
    </div>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/linear_model.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="../supervised_learning.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="lda_qda.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>