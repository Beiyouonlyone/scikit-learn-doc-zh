

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.6. 最近邻 &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.7. 高斯过程" href="gaussian_process.html" />
    <link rel="prev" title="1.5. 随机梯度下降" href="sgd.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/neighbors.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="sgd.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      1.5. 随机梯度下降
      </span>
          <span class="hiddenrellink">
          1.5. 随机梯度下降
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="gaussian_process.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      1.7. 高斯过程
      </span>
          <span class="hiddenrellink">
          1.7. 高斯过程
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../supervised_learning.html">
      Up
      <br/>
      <span class="smallrellink">
      1. 监督学习
      </span>
          <span class="hiddenrellink">
          1. 监督学习
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">1.6. 最近邻</a><ul>
<li><a class="reference internal" href="#unsupervised-neighbors">1.6.1. 无监督最近邻</a><ul>
<li><a class="reference internal" href="#id3">1.6.1.1. 找到最近邻</a></li>
<li><a class="reference internal" href="#kdtree-balltree">1.6.1.2. KDTree 和 BallTree 类</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">1.6.2. 最近邻分类</a></li>
<li><a class="reference internal" href="#regression">1.6.3. 最近邻回归</a></li>
<li><a class="reference internal" href="#id6">1.6.4. 最近邻算法</a><ul>
<li><a class="reference internal" href="#brute-force">1.6.4.1. 暴力计算</a></li>
<li><a class="reference internal" href="#k-d">1.6.4.2. K-D 树</a></li>
<li><a class="reference internal" href="#ball">1.6.4.3. Ball 树</a></li>
<li><a class="reference internal" href="#id8">1.6.4.4. 最近邻算法的选择</a></li>
<li><a class="reference internal" href="#leaf-size">1.6.4.5. <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 的影响</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearest-centroid-classifier">1.6.5. 最近质心分类</a><ul>
<li><a class="reference internal" href="#id10">1.6.5.1. 最近缩小质心</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="neighbors">
<span id="id1"></span><h1>1.6. 最近邻<a class="headerlink" href="#neighbors" title="Permalink to this headline">¶</a></h1>
<p><a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能。
无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。
neighbors-based (基于邻居的) 监督学习分为两种： <a class="reference internal" href="#classification">classification</a> （分类）针对的是具有离散标签的数据，<a class="reference internal" href="#regression">regression</a> （回归）针对的是具有连续标签的数据。</p>
<p>最近邻方法背后的原理是从训练样本中找到与新点在距离上最近的预定数量的几个点，然后从这些点中预测标签。
这些点的数量可以是用户自定义的常量（K-最近邻学习），
也可以根据不同的点的局部密度（基于半径的最近邻学习）。距离通常可以通过任何度量来衡量：
standard Euclidean distance（标准欧式距离）是最常见的选择。Neighbors-based（基于邻居的）方法被称为 <em>非泛化</em> 机器学习方法，
因为它们只是简单地”记住”了其所有的训练数据（可能转换为一个快速索引结构，如 <a class="reference internal" href="#ball-tree"><span class="std std-ref">Ball Tree</span></a> 或 <a class="reference internal" href="#kd-tree"><span class="std std-ref">KD Tree</span></a>）。</p>
<p>尽管它简单，但最近邻算法已经成功地适用于很多的分类和回归问题，例如手写数字或卫星图像的场景。
作为一个 non-parametric（非参数化）方法，它经常成功地应用于决策边界非常不规则的分类情景下。</p>
<p><a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 可以处理 Numpy 数组或 <cite>scipy.sparse</cite> 矩阵作为其输入。
对于密集矩阵，大多数可能的距离度量都是支持的。对于稀疏矩阵，支持搜索任意的 Minkowski 度量。</p>
<p>许多学习路径/方法都是依赖最近邻作为核心。
一个例子是 <a class="reference internal" href="density.html#kernel-density"><span class="std std-ref">核密度估计</span></a> ,
在 <a class="reference internal" href="density.html#density-estimation"><span class="std std-ref">密度估计</span></a> 章节中有讨论。</p>
<div class="section" id="unsupervised-neighbors">
<span id="id2"></span><h2>1.6.1. 无监督最近邻<a class="headerlink" href="#unsupervised-neighbors" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> （最近邻）实现了 unsupervised nearest neighbors learning（无监督的最近邻学习）。
它为三种不同的最近邻算法提供统一的接口：<a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a>, <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a>, 还有基于 <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a>
的 brute-force 算法。算法的选择可通过关键字 <code class="docutils literal notranslate"><span class="pre">'algorithm'</span></code> 来控制，
并必须是 <code class="docutils literal notranslate"><span class="pre">['auto',</span> <span class="pre">'ball_tree',</span> <span class="pre">'kd_tree',</span> <span class="pre">'brute']</span></code> 其中的一个。当默认值设置为 <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>
时，算法会尝试从训练数据中确定最佳方法。有关上述每个选项的优缺点，参见 <a href="#id11"><span class="problematic" id="id12">`Nearest Neighbor Algorithms`_</span></a> 。</p>
<blockquote>
<div><div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">关于最近邻算法，如果邻居 <span class="math">k+1</span> 和邻居 <span class="math">k</span> 具有相同的距离，但具有不同的标签，
结果将取决于训练数据的顺序。</p>
</div>
</div></blockquote>
<div class="section" id="id3">
<h3>1.6.1.1. 找到最近邻<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>为了完成找到两组数据集中最近邻点的简单任务, 可以使用 <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 中的无监督算法:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">NearestNeighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>                                           <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span>
<span class="go">array([[ 0.        ,  1.        ],</span>
<span class="go">       [ 0.        ,  1.        ],</span>
<span class="go">       [ 0.        ,  1.41421356],</span>
<span class="go">       [ 0.        ,  1.        ],</span>
<span class="go">       [ 0.        ,  1.        ],</span>
<span class="go">       [ 0.        ,  1.41421356]])</span>
</pre></div>
</div>
<p>因为查询集匹配训练集，每个点的最近邻点是其自身，距离为0。</p>
<p>还可以有效地生成一个稀疏图来标识相连点之间的连接情况：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors_graph</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  1.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 1.,  1.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  1.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  1.,  1.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  1.,  1.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  1.,  1.]])</span>
</pre></div>
</div>
<p>我们的数据集是结构化的，因此按索引顺序的相邻点就在参数空间相邻，从而生成了近似 K-nearest neighbors（K-近邻）的块对角矩阵。
这种稀疏图在各种的利用点之间的空间关系进行无监督学习的情况下都很有用：特别地可参见 <a class="reference internal" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.Isomap</span></code></a>,
<a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.LocallyLinearEmbedding</span></code></a>, 和 <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.cluster.SpectralClustering</span></code></a>。</p>
</div>
<div class="section" id="kdtree-balltree">
<h3>1.6.1.2. KDTree 和 BallTree 类<a class="headerlink" href="#kdtree-balltree" title="Permalink to this headline">¶</a></h3>
<p>另外，我们可以使用 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 或 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 来找最近邻。
这是上文使用过的 <a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> 类所包含的功能。
<a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 和 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 具有相同的接口；
我们将在这里展示使用 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 的例子：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KDTree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>          <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
</pre></div>
</div>
<p>对于近邻搜索中选项的更多信息，包括各种距离度量的说明和策略的说明等，请参阅 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 和 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 类文档。
关于可用度量距离的列表，请参阅 <a class="reference internal" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> 类。</p>
</div>
</div>
<div class="section" id="classification">
<span id="id4"></span><h2>1.6.2. 最近邻分类<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>最近邻分类属于 <em>基于实例的学习</em> 或 <em>非泛化学习</em> ：它不会去构造一个泛化的内部模型，而是简单地存储训练数据的实例。
分类是由每个点的最近邻的简单多数投票中计算得到的：一个查询点的数据类型是由它最近邻点中最具代表性的数据类型来决定的。</p>
<dl class="docutils">
<dt>scikit-learn 实现了两种不同的最近邻分类器：<a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> 基于每个查询点的 <span class="math">k</span> 个最近邻实现，</dt>
<dd>其中 <span class="math">k</span> 是用户指定的整数值。<a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> 基于每个查询点的固定半径 <span class="math">r</span> 内的邻居数量实现，
其中 <span class="math">r</span> 是用户指定的浮点数值。</dd>
<dt><span class="math">k</span> -邻居分类是 <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> 下的两种技术中比较常用的一种。<span class="math">k</span> 值的最佳选择是高度依赖数据的：</dt>
<dd><p class="first">通常较大的 <span class="math">k</span> 是会抑制噪声的影响，但是使得分类界限不明显。</p>
<p class="last">如果数据是不均匀采样的，那么 <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> 中的基于半径的近邻分类可能是更好的选择。</p>
</dd>
<dt>用户指定一个固定半径 <span class="math">r</span>，使得稀疏邻居中的点使用较少的最近邻来分类。</dt>
<dd><p class="first">对于高维参数空间，这个方法会由于所谓的 “维度灾难” 而变得不那么有效。</p>
<p class="last">基本的最近邻分类使用统一的权重：分配给查询点的值是从最近邻的简单多数投票中计算出来的。
在某些环境下，最好对邻居进行加权，使得更近邻更有利于拟合。可以通过 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 关键字来实现。</p>
</dd>
</dl>
<p>默认值 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code> 为每个近邻分配统一的权重。而 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance'</span></code> 分配权重与查询点的距离成反比。
或者，用户可以自定义一个距离函数用来计算权重。</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body">../auto_examples/neighbors/plot_classification.html</td>
</tr>
<tr class="field-even field"><th class="field-name">scale:</th><td class="field-body">50</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body">../auto_examples/neighbors/plot_classification.html</td>
</tr>
<tr class="field-even field"><th class="field-name">scale:</th><td class="field-body">50</td>
</tr>
</tbody>
</table>
<p class="centered">
<strong><img alt="classification_1" src="modules/../auto_examples/neighbors/images/sphx_glr_plot_classification_001.png" /> <img alt="classification_2" src="modules/../auto_examples/neighbors/images/sphx_glr_plot_classification_002.png" /></strong></p><div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">最近邻分类</span></a>: 使用最近邻进行分类的示例。</li>
</ul>
</div>
</div>
<div class="section" id="regression">
<span id="id5"></span><h2>1.6.3. 最近邻回归<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<p>最近邻回归是用在数据标签为连续变量，而不是离散变量的情况下。分配给查询点的标签是由它的最近邻标签的均值计算而来的。</p>
<p>scikit-learn 实现了两种不同的最近邻回归：<a class="reference internal" href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a> 基于每个查询点的 <span class="math">k</span> 个最近邻实现，
其中 <span class="math">k</span> 是用户指定的整数值。<a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a> 基于每个查询点的固定半径 <span class="math">r</span> 内的邻点数量实现，
其中 <span class="math">r</span> 是用户指定的浮点数值。</p>
<p>基本的最近邻回归使用统一的权重：即，本地邻域内的每个邻点对查询点的分类贡献一致。
在某些环境下，对邻点加权可能是有利的，使得附近点对于回归所作出的贡献多于远处点。
这可以通过 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 关键字来实现。默认值 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code> 为所有点分配同等权重。
而 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance'</span></code> 分配的权重与查询点距离呈反比。
或者，用户可以自定义一个距离函数用来计算权重。</p>
<div class="figure">
<img alt="modules/../auto_examples/neighbors/images/sphx_glr_plot_regression_001.png" src="modules/../auto_examples/neighbors/images/sphx_glr_plot_regression_001.png" />
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body"><p class="first">../auto_examples/neighbors/plot_regression.html
:align: center</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">scale:</th><td class="field-body">75</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p class="last">使用多输出的最近邻进行回归分析 <a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">用多输出估算器进行面部修复</span></a>。</p>
</td>
</tr>
</tbody>
</table>
<p>在这个示例中，输入 X 是脸上半部分像素，输出 Y 是脸下半部分像素。</p>
<div class="figure">
<img alt="modules/../auto_examples/images/sphx_glr_plot_multioutput_face_completion_001.png" src="modules/../auto_examples/images/sphx_glr_plot_multioutput_face_completion_001.png" />
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body"><p class="first">../auto_examples/plot_multioutput_face_completion.html
:scale: 75</p>
<blockquote class="last">
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">align:</th><td class="field-body">center</td>
</tr>
</tbody>
</table>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"><span class="std std-ref">最近邻回归</span></a>: 使用最近邻进行回归的示例。</li>
<li><a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">用多输出估算器进行面部修复</span></a>: 使用最近邻进行多输出回归的示例。</li>
</ul>
</div>
</div>
<div class="section" id="id6">
<h2>1.6.4. 最近邻算法<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="section" id="brute-force">
<span id="id7"></span><h3>1.6.4.1. 暴力计算<a class="headerlink" href="#brute-force" title="Permalink to this headline">¶</a></h3>
<p>最近邻的快速计算是机器学习中一个活跃的研究领域。最简单的近邻搜索的实现涉及数据集中所有成对点之间距离的暴力计算：
对于 <span class="math">D</span> 维度中的 <span class="math">N</span> 个样本来说, 这个方法的复杂度是 <span class="math">O[D N^2]</span>。
对于小数据样本，高效的暴力近邻搜索是非常有竞争力的。
然而，随着样本数 <span class="math">N</span> 的增长，暴力方法很快变得不切实际了。在 <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 类中，
暴力近邻搜索通过关键字 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'brute'</span></code> 来指定，并通过 <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> 中的例程来进行计算。</p>
</div>
<div class="section" id="k-d">
<span id="kd-tree"></span><h3>1.6.4.2. K-D 树<a class="headerlink" href="#k-d" title="Permalink to this headline">¶</a></h3>
<p>为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说，
这些结构试图通过有效地编码样本的 aggregate distance (聚合距离) 信息来减少所需的距离计算量。
基本思想是，若 <span class="math">A</span> 点距离 <span class="math">B</span> 点非常远，<span class="math">B</span> 点距离 <span class="math">C</span> 点非常近，
可知 <span class="math">A</span> 点与 <span class="math">C</span> 点很遥远，<em>不需要明确计算它们的距离</em>。
通过这样的方式，近邻搜索的计算成本可以降低为 <span class="math">O[D N \log(N)]</span> 或更低。
这是对于暴力搜索在大样本数 <span class="math">N</span> 中表现的显著改善。</p>
<p>利用这种聚合信息的早期方法是 <em>KD tree</em> 数据结构（* K-dimensional tree* 的简写）,
它将二维 <em>Quad-trees</em> 和三维 <em>Oct-trees</em> 推广到任意数量的维度.
KD 树是一个二叉树结构，它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。
KD 树的构造非常快：因为只需沿数据轴执行分区, 无需计算 <span class="math">D</span>-dimensional 距离。
一旦构建完成, 查询点的最近邻距离计算复杂度仅为 <span class="math">O[\log(N)]</span> 。
虽然 KD 树的方法对于低维度 (<span class="math">D &lt; 20</span>) 近邻搜索非常快, 当 <span class="math">D</span> 增长到很大时,
效率变低: 这就是所谓的 “维度灾难” 的一种体现。
在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'kd_tree'</span></code> 来指定,
并且使用类 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 来计算。</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://dl.acm.org/citation.cfm?doid=361002.361007">“Multidimensional binary search trees used for associative searching”</a>,
Bentley, J.L., Communications of the ACM (1975)</li>
</ul>
</div>
</div>
<div class="section" id="ball">
<span id="ball-tree"></span><h3>1.6.4.3. Ball 树<a class="headerlink" href="#ball" title="Permalink to this headline">¶</a></h3>
<p>为了解决 KD 树在高维上效率低下的问题, <em>ball 树</em> 数据结构就被研发出来了.
其中 KD 树沿卡迪尔轴（即坐标轴）分割数据, ball 树在沿着一系列的 hyper-spheres 来分割数据.
通过这种方法构建的树要比 KD 树消耗更多的时间,
但是这种数据结构对于高结构化的数据是非常有效的, 即使在高维度上也是一样.</p>
<dl class="docutils">
<dt>ball 树将数据递归地划分为由质心 <span class="math">C</span> 和半径 <span class="math">r</span> 定义的节点,</dt>
<dd>使得节点中的每个点位于由 <span class="math">r</span> 和 <span class="math">C</span> 定义的 hyper-sphere 内.
通过使用 <em>triangle inequality（三角不等式）</em> 减少近邻搜索的候选点数:</dd>
</dl>
<div class="math">
<p><span class="math">|x+y| \leq |x| + |y|</span></p>
</div><p>通过这种设置, 测试点和质心之间的单一距离计算足以确定距节点内所有点的距离的下限和上限.
由于 ball 树节点的球形几何, 它在高维度上的性能超出 <em>KD-tree</em>, 尽管实际的性能高度依赖于训练数据的结构.
在 scikit-learn 中, 基于 ball 树的近邻搜索可以使用关键字 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'ball_tree'</span></code> 来指定,
并且使用类 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.BallTree</span></code></a> 来计算.
或者, 用户可以直接使用 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 类.</p>
<div class="topic">
<p class="topic-title first">参考:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209">“Five balltree construction algorithms”</a>,
Omohundro, S.M., International Computer Science Institute
Technical Report (1989)</li>
</ul>
</div>
</div>
<div class="section" id="id8">
<h3>1.6.4.4. 最近邻算法的选择<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>对于给定数据集的最优算法是一个复杂的选择, 并且取决于多个因素:</p>
<ul>
<li><p class="first">样本数量 <span class="math">N</span> (i.e. <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>) 和维度 <span class="math">D</span> (例如. <code class="docutils literal notranslate"><span class="pre">n_features</span></code>).</p>
<ul>
<li><p class="first"><em>Brute force</em> 查询时间以 <span class="math">O[D N]</span> 增长</p>
</li>
<li><p class="first"><em>Ball tree</em> 查询时间大约以 <span class="math">O[D \log(N)]</span> 增长</p>
</li>
<li><dl class="first docutils">
<dt><em>KD tree</em> 的查询时间 <span class="math">D</span> 的变化是很难精确描述的.</dt>
<dd><p class="first last">对于较小的 <span class="math">D</span> (小于20) 的成本大约是 <span class="math">O[D\log(N)]</span>, 并且 KD 树更加有效.</p>
</dd>
</dl>
<p>对于较大的 <span class="math">D</span> 成本的增加接近 <span class="math">O[DN]</span>,
由于树结构引起的开销会导致查询效率比暴力还要低.</p>
</li>
</ul>
<dl class="docutils">
<dt>对于小数据集 (<span class="math">N</span> 小于30), <span class="math">\log(N)</span> 相当于 <span class="math">N</span>, 暴力算法比基于树的算法更加有效.</dt>
<dd><p class="first last"><a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 和 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 通过提供一个 <em>leaf size</em> 参数来解决这个问题:</p>
</dd>
</dl>
<p>这控制了查询切换到暴力计算样本数量. 使得两种算法的效率都能接近于对较小的 <span class="math">N</span> 的暴力计算的效率.</p>
</li>
<li><p class="first">数据结构: 数据的 <em>intrinsic dimensionality</em> (本征维数) 和/或数据的 <em>sparsity</em> (稀疏度).
本征维数是指数据所在的流形的维数 <span class="math">d \le D</span>, 在参数空间可以是线性或非线性的.
稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同,
数据矩阵可能没有零项, 但是从这个意义上来讲,它的 <strong>structure</strong> 仍然是 “稀疏” 的)。</p>
<ul class="simple">
<li><em>Brute force</em> (暴力查询)时间不受数据结构的影响。</li>
<li><em>Ball tree</em> 和 <em>KD tree</em> 的数据结构对查询时间影响很大.
一般地, 小维度的 sparser (稀疏) 数据会使查询更快.
因为 KD 树的内部表现形式是与参数轴对齐的,
对于任意的结构化数据它通常不会表现的像 ball tree 那样好.</li>
</ul>
<p>在机器学习中往往使用的数据集是非常结构化的, 而且非常适合基于树结构的查询。</p>
</li>
<li><p class="first">query point（查询点）所需的近邻数 <span class="math">k</span> 。</p>
<blockquote>
<div><ul class="simple">
<li><em>Brute force</em> 查询时间几乎不受 <span class="math">k</span> 值的影响.</li>
<li><em>Ball tree</em> 和 <em>KD tree</em> 的查询时间会随着 <span class="math">k</span> 的增加而变慢.
这是由于两个影响: 首先, <span class="math">k</span> 的值越大在参数空间中搜索的部分就越大.
其次, 使用 <span class="math">k &gt; 1</span> 进行树的遍历时, 需要对内部结果进行排序.</li>
</ul>
</div></blockquote>
<p>当 <span class="math">k</span> 相比  <span class="math">N</span> 变大时, 在基于树的查询中修剪树枝的能力是减弱的. 在这种情况下, 暴力查询会更加有效.</p>
</li>
<li><p class="first">query points（查询点）数.  ball tree 和 KD Tree 都需要一个构建阶段.
在许多查询中分摊时，这种结构的成本可以忽略不计。
如果只执行少量的查询, 可是构建成本却占总成本的很大一部分.
如果仅需查询很少的点, 暴力方法会比基于树的方法更好.</p>
</li>
</ul>
<p>一般地, <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'auto'</span></code> 选择 <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> 如果 <span class="math">k &lt; N/2</span>
并且 <code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> 在 <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> 的列表 <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code> 如果 <span class="math">k &lt; N/2</span> 并且
<code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> 在 <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code> 的列表 <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils literal notranslate"><span class="pre">'brute'</span></code> 如果 <span class="math">k &lt; N/2</span> 并且
<code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> 不在 <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> 或 <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code> 的列表 <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils literal notranslate"><span class="pre">'brute'</span></code> 如果 <span class="math">k &gt;= N/2</span>.</p>
<p>这种选择基于以下假设: 查询点的数量与训练点的数量至少在相同的数量级, 并且 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 接近其默认值 <code class="docutils literal notranslate"><span class="pre">30</span></code>.</p>
</div>
<div class="section" id="leaf-size">
<h3>1.6.4.5. <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 的影响<a class="headerlink" href="#leaf-size" title="Permalink to this headline">¶</a></h3>
<p>如上所述, 对于小样本暴力搜索是比基于数的搜索更有效的方法.
这一事实在 ball 树和 KD 树中被解释为在叶节点内部切换到蛮力搜索.
该开关的级别可以使用参数 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 来指定.
这个参数选择有很多的效果:</p>
<dl class="docutils">
<dt><strong>构造时间</strong></dt>
<dd>更大的 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 会导致更快的树构建时间, 因为需要创建更少的节点.</dd>
<dt><strong>查询时间</strong></dt>
<dd>一个大或小的 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 可能会导致次优查询成本.
当 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 接近 1 时, 遍历节点所涉及的开销大大减慢了查询时间.
当 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>, 接近训练集的大小，查询变得本质上是暴力的.
这些之间的一个很好的妥协是 <code class="docutils literal notranslate"><span class="pre">leaf_size</span> <span class="pre">=</span> <span class="pre">30</span></code>, 这是该参数的默认值.</dd>
</dl>
<p><strong>内存</strong>
随着leaf_size的增加，存储树结构所需的内存减少。
对于存储每个节点的D维质心的ball tree，这点至关重要。
针对 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 所需的存储空间近似于 <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">leaf_size</span></code> 乘以训练集的大小.</p>
<p><code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 不被 brute force queries（暴力查询）所引用.</p>
</div>
</div>
<div class="section" id="nearest-centroid-classifier">
<span id="id9"></span><h2>1.6.5. 最近质心分类<a class="headerlink" href="#nearest-centroid-classifier" title="Permalink to this headline">¶</a></h2>
<p>该 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 分类器是一个简单的算法, 通过其成员的质心来表示每个类。
实际上, 这使得它类似于 <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.KMeans</span></code> 算法的标签更新阶段.
它也没有参数选择, 使其成为良好的基准分类器.
然而，它确实受到非凸类的影响，即当类有显著不同的方差时。所以这个分类器假设所有维度的方差都是相等的。
对于没有做出这个假设的更复杂的方法, 请参阅线性判别分析 (<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span></code></a>)
和二次判别分析 (<a class="reference internal" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</span></code></a>).
默认的 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 用法示例如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="k">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="section" id="id10">
<h3>1.6.5.1. 最近缩小质心<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 分类器有一个 <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code> 参数,
它实现了 nearest shrunken centroid 分类器.
实际上, 每个质心的每个特征的值除以该特征的类中的方差.
然后通过 <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code> 来减小特征值.
最值得注意的是, 如果特定特征值过0, 则将其设置为0.
实际上，这个方法移除了影响分类器的特征。
这很有用, 例如, 去除噪声特征.</p>
<p>在以下例子中, 使用一个较小的 shrink 阀值将模型的准确度从 0.81 提高到 0.82.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body">../auto_examples/neighbors/plot_nearest_centroid.html</td>
</tr>
<tr class="field-even field"><th class="field-name">scale:</th><td class="field-body">50</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">target:</th><td class="field-body">../auto_examples/neighbors/plot_nearest_centroid.html</td>
</tr>
<tr class="field-even field"><th class="field-name">scale:</th><td class="field-body">50</td>
</tr>
</tbody>
</table>
<p class="centered">
<strong><img alt="nearest_centroid_1" src="modules/../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_001.png" /> <img alt="nearest_centroid_2" src="modules/../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_002.png" /></strong></p><div class="topic">
<p class="topic-title first">例子:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="std std-ref">Nearest Centroid Classification</span></a>: 一个分类的例子, 它使用了不同 shrink 阀值的最近质心.</li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    

    

    

    

    

    

    
    <!-- modules/neighbors.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/DataMonk2017">@DataMonk2017</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/caopeirui">@Veyron C</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/pan8664716">@舞空</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/wangyangting">@那伊抹微笑</a><br/> 
    </div>
    

    

    

    

    

    

    

    

    

    

    

    
    
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/neighbors.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="sgd.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="gaussian_process.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>