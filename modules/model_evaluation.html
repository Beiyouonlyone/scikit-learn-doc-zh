

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>3.3. 模型评估: 量化预测的质量 &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.4. 模型持久化" href="model_persistence.html" />
    <link rel="prev" title="3.2.4.3.6. sklearn.ensemble.GradientBoostingRegressor" href="generated/sklearn.ensemble.GradientBoostingRegressor.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/model_evaluation.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="generated/sklearn.ensemble.GradientBoostingRegressor.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      3.2.4.3.6. sk...
      </span>
          <span class="hiddenrellink">
          3.2.4.3.6. sklearn.ensemble.GradientBoostingRegressor
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="model_persistence.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      3.4. 模型持久化
      </span>
          <span class="hiddenrellink">
          3.4. 模型持久化
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../model_selection.html">
      Up
      <br/>
      <span class="smallrellink">
      3. 模型选择和评估
      </span>
          <span class="hiddenrellink">
          3. 模型选择和评估
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">3.3. 模型评估: 量化预测的质量</a><ul>
<li><a class="reference internal" href="#scoring">3.3.1. <code class="docutils literal notranslate"><span class="pre">scoring</span></code> 参数: 定义模型评估规则</a><ul>
<li><a class="reference internal" href="#id2">3.3.1.1. 常见场景: 预定义值</a></li>
<li><a class="reference internal" href="#metric">3.3.1.2. 根据 metric 函数定义您的评分策略</a></li>
<li><a class="reference internal" href="#diy-scoring">3.3.1.3. 实现自己的记分对象</a></li>
<li><a class="reference internal" href="#multimetric-scoring">3.3.1.4. 使用多个指数评估</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification-metrics">3.3.2. 分类指标</a><ul>
<li><a class="reference internal" href="#multilabel">3.3.2.1. 从二分到多分类和 multilabel</a></li>
<li><a class="reference internal" href="#accuracy-score">3.3.2.2. 精确度得分</a></li>
<li><a class="reference internal" href="#cohen-s-kappa">3.3.2.3. Cohen’s kappa</a></li>
<li><a class="reference internal" href="#confusion-matrix">3.3.2.4. 混淆矩阵</a></li>
<li><a class="reference internal" href="#classification-report">3.3.2.5. 分类报告</a></li>
<li><a class="reference internal" href="#hamming-loss">3.3.2.6. 汉明损失</a></li>
<li><a class="reference internal" href="#jaccard-score">3.3.2.7. Jaccard 相似系数 score</a></li>
<li><a class="reference internal" href="#f-measures">3.3.2.8. 精准，召回和 F-measures</a><ul>
<li><a class="reference internal" href="#id14">3.3.2.8.1. 二分类</a></li>
<li><a class="reference internal" href="#id15">3.3.2.8.2. 多类和多标签分类</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hinge-loss">3.3.2.9. Hinge loss</a></li>
<li><a class="reference internal" href="#log">3.3.2.10. Log 损失</a></li>
<li><a class="reference internal" href="#matthews-corrcoef">3.3.2.11. 马修斯相关系数</a></li>
<li><a class="reference internal" href="#receiver-operating-characteristic-roc">3.3.2.12. Receiver operating characteristic (ROC)</a></li>
<li><a class="reference internal" href="#zero-one-loss">3.3.2.13. 零一损失</a></li>
<li><a class="reference internal" href="#brier">3.3.2.14. Brier 分数损失</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multilabel-ranking-metrics">3.3.3. 多标签排名指标</a><ul>
<li><a class="reference internal" href="#coverage-error">3.3.3.1. 覆盖误差</a></li>
<li><a class="reference internal" href="#label-ranking-average-precision">3.3.3.2. 标签排名平均精度</a></li>
<li><a class="reference internal" href="#label-ranking-loss">3.3.3.3. 排序损失</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression-metrics">3.3.4. 回归指标</a><ul>
<li><a class="reference internal" href="#explained-variance-score">3.3.4.1. 解释方差得分</a></li>
<li><a class="reference internal" href="#mean-absolute-error">3.3.4.2. 平均绝对误差</a></li>
<li><a class="reference internal" href="#mean-squared-error">3.3.4.3. 均方误差</a></li>
<li><a class="reference internal" href="#mean-squared-log-error">3.3.4.4. 均方误差对数</a></li>
<li><a class="reference internal" href="#median-absolute-error">3.3.4.5. 中位绝对误差</a></li>
<li><a class="reference internal" href="#r2-score">3.3.4.6. R² score, 可决系数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#clustering-metrics">3.3.5. 聚类指标</a></li>
<li><a class="reference internal" href="#dummy-estimators">3.3.6. 虚拟估计</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="model-evaluation">
<span id="id1"></span><h1>3.3. 模型评估: 量化预测的质量<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h1>
<p>有 3 种不同的 API 用于评估模型预测的质量:</p>
<ul class="simple">
<li><strong>Estimator score method（估计器得分的方法）</strong>: Estimators（估计器）有一个 <code class="docutils literal notranslate"><span class="pre">score（得分）</span></code> 方法，为其解决的问题提供了默认的 evaluation criterion （评估标准）。
在这个页面上没有相关讨论，但是在每个 estimator （估计器）的文档中会有相关的讨论。</li>
<li><strong>Scoring parameter（评分参数）</strong>: Model-evaluation tools （模型评估工具）使用 <a class="reference internal" href="cross_validation.html#cross-validation"><span class="std std-ref">cross-validation</span></a> (如 <a class="reference internal" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">model_selection.cross_val_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">model_selection.GridSearchCV</span></code></a>) 依靠 internal <em>scoring</em> strategy （内部 <em>scoring（得分）</em> 策略）。这在 <a class="reference internal" href="#scoring-parameter"><span class="std std-ref">scoring 参数: 定义模型评估规则</span></a> 部分讨论。</li>
<li><strong>Metric functions（指标函数）</strong>: <code class="xref py py-mod docutils literal notranslate"><span class="pre">metrics</span></code> 模块实现了针对特定目的评估预测误差的函数。这些指标在以下部分部分详细介绍 <a class="reference internal" href="#classification-metrics"><span class="std std-ref">分类指标</span></a>, <a class="reference internal" href="#multilabel-ranking-metrics"><span class="std std-ref">多标签排名指标</span></a>, <a class="reference internal" href="#regression-metrics"><span class="std std-ref">回归指标</span></a> 和 <a class="reference internal" href="#clustering-metrics"><span class="std std-ref">聚类指标</span></a> 。</li>
</ul>
<p>最后， <a class="reference internal" href="#dummy-estimators"><span class="std std-ref">虚拟估计</span></a> 用于获取随机预测的这些指标的基准值。</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">对于 “pairwise（成对）” metrics（指标），<em>samples（样本）</em> 之间而不是 estimators （估计量）或者 predictions（预测值），请参阅 <a class="reference internal" href="metrics.html#metrics"><span class="std std-ref">成对的矩阵, 类别和核函数</span></a> 部分。</p>
</div>
<div class="section" id="scoring">
<span id="scoring-parameter"></span><h2>3.3.1. <code class="docutils literal notranslate"><span class="pre">scoring</span></code> 参数: 定义模型评估规则<a class="headerlink" href="#scoring" title="Permalink to this headline">¶</a></h2>
<p>Model selection （模型选择）和 evaluation （评估）使用工具，例如 <a class="reference internal" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">model_selection.GridSearchCV</span></code></a> 和 <a class="reference internal" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">model_selection.cross_val_score</span></code></a> ，采用 <code class="docutils literal notranslate"><span class="pre">scoring</span></code> 参数来控制它们对 estimators evaluated （评估的估计量）应用的指标。</p>
<div class="section" id="id2">
<h3>3.3.1.1. 常见场景: 预定义值<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>对于最常见的用例, 您可以使用 <code class="docutils literal notranslate"><span class="pre">scoring</span></code> 参数指定一个 scorer object （记分对象）; 下表显示了所有可能的值。
所有 scorer objects （记分对象）遵循惯例  <strong>higher return values are better than lower return values（较高的返回值优于较低的返回值）</strong> 。因此，测量模型和数据之间距离的 metrics （度量），如 <a class="reference internal" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.mean_squared_error</span></code></a> 可用作返回 metric （指数）的 negated value （否定值）的 neg_mean_squared_error 。</p>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="32%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Scoring（得分）</th>
<th class="head">Function（函数）</th>
<th class="head">Comment（注解）</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>Classification（分类）</strong></td>
<td>&#160;</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘accuracy’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.accuracy_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘average_precision’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.average_precision_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘f1’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.f1_score</span></code></a></td>
<td>for binary targets（用于二进制目标）</td>
</tr>
<tr class="row-even"><td>‘f1_micro’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.f1_score</span></code></a></td>
<td>micro-averaged（微平均）</td>
</tr>
<tr class="row-odd"><td>‘f1_macro’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.f1_score</span></code></a></td>
<td>macro-averaged（微平均）</td>
</tr>
<tr class="row-even"><td>‘f1_weighted’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.f1_score</span></code></a></td>
<td>weighted average（加权平均）</td>
</tr>
<tr class="row-odd"><td>‘f1_samples’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.f1_score</span></code></a></td>
<td>by multilabel sample（通过 multilabel 样本）</td>
</tr>
<tr class="row-even"><td>‘neg_log_loss’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.log_loss</span></code></a></td>
<td>requires <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> support（需要 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 支持）</td>
</tr>
<tr class="row-odd"><td>‘precision’ etc.</td>
<td><a class="reference internal" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.precision_score</span></code></a></td>
<td>suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr class="row-even"><td>‘recall’ etc.</td>
<td><a class="reference internal" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.recall_score</span></code></a></td>
<td>suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr class="row-odd"><td>‘roc_auc’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.roc_auc_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td><strong>Clustering（聚类）</strong></td>
<td>&#160;</td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘adjusted_mutual_info_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.adjusted_mutual_info_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘adjusted_rand_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.adjusted_rand_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘completeness_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.completeness_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘fowlkes_mallows_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.fowlkes_mallows_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘homogeneity_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.homogeneity_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘mutual_info_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.mutual_info_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘normalized_mutual_info_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.normalized_mutual_info_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘v_measure_score’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.v_measure_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td><strong>Regression（回归）</strong></td>
<td>&#160;</td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘explained_variance’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.explained_variance_score</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘neg_mean_absolute_error’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.mean_absolute_error</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘neg_mean_squared_error’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.mean_squared_error</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘neg_mean_squared_log_error’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.mean_squared_log_error</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-even"><td>‘neg_median_absolute_error’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.median_absolute_error</span></code></a></td>
<td>&#160;</td>
</tr>
<tr class="row-odd"><td>‘r2’</td>
<td><a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.r2_score</span></code></a></td>
<td>&#160;</td>
</tr>
</tbody>
</table>
<p>使用案例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_log_loss&#39;</span><span class="p">)</span> <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">array([-0.07..., -0.16..., -0.06...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;wrong_choice&#39;</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="gr">ValueError</span>: <span class="n">&#39;wrong_choice&#39; is not a valid scoring value. Valid options are [&#39;accuracy&#39;, &#39;adjusted_mutual_info_score&#39;, &#39;adjusted_rand_score&#39;, &#39;average_precision&#39;, &#39;completeness_score&#39;, &#39;explained_variance&#39;, &#39;f1&#39;, &#39;f1_macro&#39;, &#39;f1_micro&#39;, &#39;f1_samples&#39;, &#39;f1_weighted&#39;, &#39;fowlkes_mallows_score&#39;, &#39;homogeneity_score&#39;, &#39;mutual_info_score&#39;, &#39;neg_log_loss&#39;, &#39;neg_mean_absolute_error&#39;, &#39;neg_mean_squared_error&#39;, &#39;neg_mean_squared_log_error&#39;, &#39;neg_median_absolute_error&#39;, &#39;normalized_mutual_info_score&#39;, &#39;precision&#39;, &#39;precision_macro&#39;, &#39;precision_micro&#39;, &#39;precision_samples&#39;, &#39;precision_weighted&#39;, &#39;r2&#39;, &#39;recall&#39;, &#39;recall_macro&#39;, &#39;recall_micro&#39;, &#39;recall_samples&#39;, &#39;recall_weighted&#39;, &#39;roc_auc&#39;, &#39;v_measure_score&#39;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">ValueError exception 列出的值对应于以下部分描述的 functions measuring prediction accuracy （测量预测精度的函数）。
这些函数的 scorer objects （记分对象）存储在 dictionary <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.SCORERS</span></code> 中。</p>
</div>
</div>
<div class="section" id="metric">
<span id="id3"></span><h3>3.3.1.2. 根据 metric 函数定义您的评分策略<a class="headerlink" href="#metric" title="Permalink to this headline">¶</a></h3>
<p>模块 <a class="reference internal" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> 还公开了一组 measuring a prediction error （测量预测误差）的简单函数，给出了基础真实的数据和预测:</p>
<ul class="simple">
<li>函数以 <code class="docutils literal notranslate"><span class="pre">_score</span></code> 结尾返回一个值来最大化，越高越好。</li>
<li>函数 <code class="docutils literal notranslate"><span class="pre">_error</span></code> 或 <code class="docutils literal notranslate"><span class="pre">_loss</span></code> 结尾返回一个值来 minimize （最小化），越低越好。当使用 <a class="reference internal" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="xref py py-func docutils literal notranslate"><span class="pre">make_scorer</span></code></a> 转换成 scorer object （记分对象）时，将 <code class="docutils literal notranslate"><span class="pre">greater_is_better</span></code> 参数设置为 False（默认为 True; 请参阅下面的参数说明）。</li>
</ul>
<p>可用于各种机器学习任务的 Metrics （指标）在下面详细介绍。</p>
<p>许多 metrics （指标）没有被用作 <code class="docutils literal notranslate"><span class="pre">scoring（得分）</span></code> 值的名称，有时是因为它们需要额外的参数，例如 <a class="reference internal" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">fbeta_score</span></code></a> 。在这种情况下，您需要生成一个适当的 scoring object （评分对象）。生成 callable object for scoring （可评估对象进行评分）的最简单方法是使用 <a class="reference internal" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="xref py py-func docutils literal notranslate"><span class="pre">make_scorer</span></code></a> 。该函数将 metrics （指数）转换为可用于可调用的 model evaluation （模型评估）。</p>
<p>一个典型的用例是从库中包含一个非默认值参数的 existing metric function （现有指数函数），例如 <a class="reference internal" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">fbeta_score</span></code></a> 函数的 <code class="docutils literal notranslate"><span class="pre">beta</span></code> 参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">fbeta_score</span><span class="p">,</span> <span class="n">make_scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ftwo_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(),</span> <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span> <span class="n">scoring</span><span class="o">=</span><span class="n">ftwo_scorer</span><span class="p">)</span>
</pre></div>
</div>
<p>第二个用例是使用 <a class="reference internal" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="xref py py-func docutils literal notranslate"><span class="pre">make_scorer</span></code></a> 从简单的 python 函数构建一个完全 custom scorer object （自定义的记分对象），可以使用几个参数 :</p>
<ul class="simple">
<li>你要使用的 python 函数（在下面的例子中是 <code class="docutils literal notranslate"><span class="pre">my_custom_loss_func</span></code>）</li>
<li>python 函数是否返回一个分数 (<code class="docutils literal notranslate"><span class="pre">greater_is_better=True</span></code>, 默认值) 或者一个 loss （损失） (<code class="docutils literal notranslate"><span class="pre">greater_is_better=False</span></code>)。 如果是一个 loss （损失），scorer object （记分对象）的 python 函数的输出被 negated （否定），符合 cross validation convention （交叉验证约定），scorers 为更好的模型返回更高的值。</li>
<li>仅用于 classification metrics （分类指数）: 您提供的 python 函数是否需要连续的 continuous decision certainties （判断确定性）（<code class="docutils literal notranslate"><span class="pre">needs_threshold=True</span></code>）。默认值为 False 。</li>
<li>任何其他参数，如 <code class="docutils literal notranslate"><span class="pre">beta</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">labels</span></code> 在 函数 <a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">f1_score</span></code></a> 。</li>
</ul>
<p>以下是建立 custom scorers （自定义记分对象）的示例，并使用 <code class="docutils literal notranslate"><span class="pre">greater_is_better</span></code> 参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_custom_loss_func</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ground_truth</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">diff</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># loss_func will negate the return value of my_custom_loss_func,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#  which will be np.log(2), 0.693, given the values for ground_truth</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#  and predictions defined below.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span>  <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">my_custom_loss_func</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">my_custom_loss_func</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ground_truth</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="k">import</span> <span class="n">DummyClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;most_frequent&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span> 
<span class="go">-0.69...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="n">ground_truth</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span> 
<span class="go">0.69...</span>
</pre></div>
</div>
</div>
<div class="section" id="diy-scoring">
<span id="id4"></span><h3>3.3.1.3. 实现自己的记分对象<a class="headerlink" href="#diy-scoring" title="Permalink to this headline">¶</a></h3>
<p>您可以通过从头开始构建自己的 scoring object （记分对象），而不使用 <a class="reference internal" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="xref py py-func docutils literal notranslate"><span class="pre">make_scorer</span></code></a> factory 来生成更加灵活的 model scorers （模型记分对象）。
对于被叫做 scorer 来说，它需要符合以下两个规则所指定的协议:</p>
<ul class="simple">
<li>可以使用参数 <code class="docutils literal notranslate"><span class="pre">(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code> 来调用它，其中 <code class="docutils literal notranslate"><span class="pre">estimator</span></code> 是要被评估的模型，<code class="docutils literal notranslate"><span class="pre">X</span></code> 是验证数据， <code class="docutils literal notranslate"><span class="pre">y</span></code> 是 <code class="docutils literal notranslate"><span class="pre">X</span></code> (在有监督情况下) 或 <code class="docutils literal notranslate"><span class="pre">None</span></code> (在无监督情况下) 已经被标注的真实数据目标。</li>
<li>它返回一个浮点数，用于对 <code class="docutils literal notranslate"><span class="pre">X</span></code> 进行量化 <code class="docutils literal notranslate"><span class="pre">estimator</span></code> 的预测质量，参考 <code class="docutils literal notranslate"><span class="pre">y</span></code> 。
再次，按照惯例，更高的数字更好，所以如果你的 scorer 返回 loss ，那么这个值应该被 negated 。</li>
</ul>
</div>
<div class="section" id="multimetric-scoring">
<span id="id5"></span><h3>3.3.1.4. 使用多个指数评估<a class="headerlink" href="#multimetric-scoring" title="Permalink to this headline">¶</a></h3>
<p>Scikit-learn 还允许在 <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>, <code class="docutils literal notranslate"><span class="pre">RandomizedSearchCV</span></code> 和 <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> 中评估 multiple metric （多个指数）。</p>
<p>为 <code class="docutils literal notranslate"><span class="pre">scoring</span></code> 参数指定多个评分指标有两种方法:</p>
<ul>
<li><dl class="first docutils">
<dt>As an iterable of string metrics（作为 string metrics 的迭代）::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>As a <code class="docutils literal notranslate"><span class="pre">dict</span></code> mapping the scorer name to the scoring function（作为 <code class="docutils literal notranslate"><span class="pre">dict</span></code> ，将 scorer 名称映射到 scoring 函数）::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">make_scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">),</span>
<span class="gp">... </span>           <span class="s1">&#39;prec&#39;</span><span class="p">:</span> <span class="s1">&#39;precision&#39;</span><span class="p">}</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
<p>请注意， dict 值可以是 scorer functions （记分函数）或者 predefined metric strings （预定义 metric 字符串）之一。</p>
<p>目前，只有那些返回 single score （单一分数）的 scorer functions （记分函数）才能在 dict 内传递。不允许返回多个值的 Scorer functions （Scorer 函数），并且需要一个 wrapper 才能返回 single metric（单个指标）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_validate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># A sample toy binary classification dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">tp</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">tn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">fp</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span> <span class="k">return</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tp&#39;</span> <span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">tp</span><span class="p">),</span> <span class="s1">&#39;tn&#39;</span> <span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">tn</span><span class="p">),</span>
<span class="gp">... </span>           <span class="s1">&#39;fp&#39;</span> <span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fp</span><span class="p">),</span> <span class="s1">&#39;fn&#39;</span> <span class="p">:</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fn</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Getting the test set true positive scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_tp&#39;</span><span class="p">])</span>          
<span class="go">[12 13 15]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Getting the test set false negative scores</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_fn&#39;</span><span class="p">])</span>          
<span class="go">[5 4 1]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="classification-metrics">
<span id="id6"></span><h2>3.3.2. 分类指标<a class="headerlink" href="#classification-metrics" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> 模块实现了几个 loss, score, 和 utility 函数来衡量 classification （分类）性能。
某些 metrics （指标）可能需要 positive class （正类），confidence values（置信度值）或 binary decisions values （二进制决策值）的概率估计。
大多数的实现允许每个样本通过 <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> 参数为 overall score （总分）提供 weighted contribution （加权贡献）。</p>
<p>其中一些仅限于二分类案例:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a>(y_true,&nbsp;probas_pred)</td>
<td>Compute precision-recall pairs for different probability thresholds</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a>(y_true,&nbsp;y_score[,&nbsp;pos_label,&nbsp;…])</td>
<td>Compute Receiver operating characteristic (ROC)</td>
</tr>
</tbody>
</table>
<p>其他也可以在多分类案例中运行:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cohen_kappa_score</span></code></a>(y1,&nbsp;y2[,&nbsp;labels,&nbsp;weights,&nbsp;…])</td>
<td>Cohen’s kappa: a statistic that measures inter-annotator agreement.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute confusion matrix to evaluate the accuracy of a classification</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hinge_loss</span></code></a>(y_true,&nbsp;pred_decision[,&nbsp;labels,&nbsp;…])</td>
<td>Average hinge loss (non-regularized)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matthews_corrcoef</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td>Compute the Matthews correlation coefficient (MCC)</td>
</tr>
</tbody>
</table>
<p>有些还可以在 multilabel case （多重案例）中工作:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…])</td>
<td>Accuracy classification score.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code class="xref py py-obj docutils literal notranslate"><span class="pre">classification_report</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td>Build a text report showing the main classification metrics</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">f1_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the F1 score, also known as balanced F-score or F-measure</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fbeta_score</span></code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the F-beta score</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the average Hamming loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_similarity_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td>Jaccard similarity coefficient score</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;eps,&nbsp;normalize,&nbsp;…])</td>
<td>Log loss, aka logistic loss or cross-entropy loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>(y_true,&nbsp;y_pred)</td>
<td>Compute precision, recall, F-measure and support for each class</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the precision</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">recall_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the recall</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_one_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…])</td>
<td>Zero-one classification loss.</td>
</tr>
</tbody>
</table>
<p>一些通常用于 ranking:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score" title="sklearn.metrics.dcg_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dcg_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;k])</td>
<td>Discounted cumulative gain (DCG) at rank K.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score" title="sklearn.metrics.ndcg_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ndcg_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;k])</td>
<td>Normalized discounted cumulative gain (NDCG) at rank K.</td>
</tr>
</tbody>
</table>
<p>有些工作与 binary 和 multilabel （但不是多类）的问题:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">average_precision_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;…])</td>
<td>Compute average precision (AP) from prediction scores</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;average,&nbsp;…])</td>
<td>Compute Area Under the Curve (AUC) from prediction scores</td>
</tr>
</tbody>
</table>
<p>在以下小节中，我们将介绍每个这些功能，前面是一些关于通用 API 和 metric 定义的注释。</p>
<div class="section" id="multilabel">
<h3>3.3.2.1. 从二分到多分类和 multilabel<a class="headerlink" href="#multilabel" title="Permalink to this headline">¶</a></h3>
<p>一些 metrics 基本上是为 binary classification tasks （二分类任务）定义的 (例如 <a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">f1_score</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a>) 。在这些情况下，默认情况下仅评估 positive label （正标签），假设默认情况下，positive label （正类）标记为 <code class="docutils literal notranslate"><span class="pre">1</span></code> （尽管可以通过 <code class="docutils literal notranslate"><span class="pre">pos_label</span></code> 参数进行配置）。</p>
<p id="average">将 binary metric （二分指标）扩展为 multiclass （多类）或 multilabel （多标签）问题时，数据将被视为二分问题的集合，每个类都有一个。
然后可以使用多种方法在整个类中 average binary metric calculations （平均二分指标计算），每种类在某些情况下可能会有用。
如果可用，您应该使用 <code class="docutils literal notranslate"><span class="pre">average</span></code> 参数来选择它们。</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">&quot;macro（宏）&quot;</span></code> 简单地计算 binary metrics （二分指标）的平均值，赋予每个类别相同的权重。在不常见的类别重要的问题上，macro-averaging （宏观平均）可能是突出表现的一种手段。另一方面，所有类别同样重要的假设通常是不真实的，因此 macro-averaging （宏观平均）将过度强调不频繁类的典型的低性能。</li>
<li><code class="docutils literal notranslate"><span class="pre">&quot;weighted（加权）&quot;</span></code> 通过计算其在真实数据样本中的存在来对每个类的 score 进行加权的 binary metrics （二分指标）的平均值来计算类不平衡。</li>
<li><code class="docutils literal notranslate"><span class="pre">&quot;micro（微）&quot;</span></code> 给每个 sample-class pair （样本类对）对 overall metric （总体指数）（sample-class 权重的结果除外） 等同的贡献。除了对每个类别的 metric 进行求和之外，这个总和构成每个类别度量的 dividends （除数）和 divisors （除数）计算一个整体商。
在 multilabel settings （多标签设置）中，Micro-averaging 可能是优先选择的，包括要忽略 majority class （多数类）的 multiclass classification （多类分类）。</li>
<li><code class="docutils literal notranslate"><span class="pre">&quot;samples（样本）&quot;</span></code> 仅适用于 multilabel problems （多标签问题）。它 does not calculate a per-class measure （不计算每个类别的 measure），而是计算 evaluation data （评估数据）中的每个样本的 true and predicted classes （真实和预测类别）的 metric （指标），并返回 (<code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>-weighted) 加权平均。</li>
<li>选择 <code class="docutils literal notranslate"><span class="pre">average=None</span></code> 将返回一个 array 与每个类的 score 。</li>
</ul>
<p>虽然将 multiclass data （多类数据）提供给 metric ，如 binary targets （二分类目标），作为 array of class labels （类标签的数组），multilabel data （多标签数据）被指定为 indicator matrix（指示符矩阵），其中 cell <code class="docutils literal notranslate"><span class="pre">[i,</span> <span class="pre">j]</span></code> 具有值 1，如果样本 <code class="docutils literal notranslate"><span class="pre">i</span></code> 具有标号 <code class="docutils literal notranslate"><span class="pre">j</span></code> ，否则为值 0 。</p>
</div>
<div class="section" id="accuracy-score">
<span id="id7"></span><h3>3.3.2.2. 精确度得分<a class="headerlink" href="#accuracy-score" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score</span></code></a> 函数计算 <a class="reference external" href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, 正确预测的分数（默认）或计数 (normalize=False)。</p>
<p>在 multilabel classification （多标签分类）中，函数返回 subset accuracy（子集精度）。如果样本的 entire set of predicted labels （整套预测标签）与真正的标签组合匹配，则子集精度为 1.0; 否则为 0.0 。</p>
<p>如果 <span class="math">\hat{y}_i</span> 是第 <span class="math">i</span> 个样本的预测值，<span class="math">y_i</span> 是相应的真实值，则 <span class="math">n_\text{samples}</span> 上的正确预测的分数被定义为</p>
<div class="math">
<p><span class="math">\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)</span></p>
</div><p>其中 <span class="math">1(x)</span> 是 <a class="reference external" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指示函数）</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<p>In the multilabel case with binary label indicators（在具有二分标签指示符的多标签情况下）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.5</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/feature_selection/plot_permutation_test_for_classification.html#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py"><span class="std std-ref">Test with permutations the significance of a classification score</span></a>
例如使用数据集排列的 accuracy score （精度分数）。</li>
</ul>
</div>
</div>
<div class="section" id="cohen-s-kappa">
<span id="cohen-kappa"></span><h3>3.3.2.3. Cohen’s kappa<a class="headerlink" href="#cohen-s-kappa" title="Permalink to this headline">¶</a></h3>
<p>函数 <a class="reference internal" href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">cohen_kappa_score</span></code></a> 计算 <a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s kappa</a> statistic（统计）。
这个 measure （措施）旨在比较不同人工标注者的标签，而不是 classifier （分类器）与 ground truth （真实数据）。</p>
<p>kappa score （参阅 docstring ）是 -1 和 1 之间的数字。
.8 以上的 scores 通常被认为是很好的 agreement （协议）;
0 或者 更低表示没有 agreement （实际上是 random labels （随机标签））。</p>
<p>Kappa scores 可以计算 binary or multiclass （二分或者多分类）问题，但不能用于 multilabel problems （多标签问题）（除了手动计算 per-label score （每个标签分数）），而不是两个以上的 annotators （注释器）。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">cohen_kappa_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cohen_kappa_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.4285714285714286</span>
</pre></div>
</div>
</div>
<div class="section" id="confusion-matrix">
<span id="id9"></span><h3>3.3.2.4. 混淆矩阵<a class="headerlink" href="#confusion-matrix" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a> 函数通过计算 <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix（混淆矩阵）</a> 来 evaluates classification accuracy （评估分类的准确性）。</p>
<p>根据定义，confusion matrix （混淆矩阵）中的 entry（条目） <span class="math">i, j</span>，是实际上在 group <span class="math">i</span> 中的 observations （观察数），但预测在 group <span class="math">j</span> 中。这里是一个示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([[2, 0, 0],</span>
<span class="go">       [0, 0, 1],</span>
<span class="go">       [1, 0, 2]])</span>
</pre></div>
</div>
<p>这是一个这样的 confusion matrix （混淆矩阵）的可视化表示 （这个数字来自于 <a class="reference internal" href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="std std-ref">Confusion matrix</span></a>）:</p>
<a class="reference external image-reference" href="../auto_examples/model_selection/plot_confusion_matrix.html"><img alt="modules/../auto_examples/model_selection/images/sphx_glr_plot_confusion_matrix_001.png" class="align-center" src="modules/../auto_examples/model_selection/images/sphx_glr_plot_confusion_matrix_001.png" /></a>
<p>对于 binary problems （二分类问题），我们可以得到 true negatives（真 negatives）, false positives（假 positives）, false negatives（假 negatives） 和 true positives（真 positives） 的数量如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span>
<span class="go">(2, 1, 2, 3)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="std std-ref">Confusion matrix</span></a>
例如使用 confusion matrix （混淆矩阵）来评估 classifier （分类器）的输出质量。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="std std-ref">Recognizing hand-written digits</span></a>
例如使用 confusion matrix （混淆矩阵）来分类手写数字。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>
例如使用 confusion matrix （混淆矩阵）对文本文档进行分类。</li>
</ul>
</div>
</div>
<div class="section" id="classification-report">
<span id="id11"></span><h3>3.3.2.5. 分类报告<a class="headerlink" href="#classification-report" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code class="xref py py-func docutils literal notranslate"><span class="pre">classification_report</span></code></a> 函数构建一个显示 main classification metrics （主分类指标）的文本报告。这是一个小例子，其中包含自定义的 <code class="docutils literal notranslate"><span class="pre">target_names</span></code> 和 inferred labels （推断标签）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">classification_report</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;class 2&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>
<span class="go">             precision    recall  f1-score   support</span>

<span class="go">    class 0       0.67      1.00      0.80         2</span>
<span class="go">    class 1       0.00      0.00      0.00         1</span>
<span class="go">    class 2       1.00      0.50      0.67         2</span>

<span class="go">avg / total       0.67      0.60      0.59         5</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="std std-ref">Recognizing hand-written digits</span></a>
作为手写数字的分类报告的使用示例。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>
作为文本文档的分类报告使用的示例。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="std std-ref">Parameter estimation using grid search with cross-validation</span></a>
例如使用 grid search with nested cross-validation （嵌套交叉验证进行网格搜索）的分类报告。</li>
</ul>
</div>
</div>
<div class="section" id="hamming-loss">
<span id="id12"></span><h3>3.3.2.6. 汉明损失<a class="headerlink" href="#hamming-loss" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">hamming_loss</span></code></a> 计算两组样本之间的 average Hamming loss （平均汉明损失）或者 <a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance（汉明距离）</a> 。</p>
<p>如果 <span class="math">\hat{y}_j</span> 是给定样本的第 <span class="math">j</span> 个标签的预测值，则 <span class="math">y_j</span> 是相应的真实值，而 <span class="math">n_\text{labels}</span> 是 classes or labels （类或者标签）的数量，则两个样本之间的 Hamming loss （汉明损失） <span class="math">L_{Hamming}</span> 定义为:</p>
<div class="math">
<p><span class="math">L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{labels}} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_j \not= y_j)</span></p>
</div><p>其中 <span class="math">1(x)</span> 是 <a class="reference external" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指标函数）</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">hamming_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.25</span>
</pre></div>
</div>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">在 multiclass classification （多类分类）中， Hamming loss （汉明损失）对应于 <code class="docutils literal notranslate"><span class="pre">y_true</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> 之间的 Hamming distance（汉明距离），它类似于 <a class="reference internal" href="#zero-one-loss"><span class="std std-ref">零一损失</span></a> 函数。然而， zero-one loss penalizes （0-1损失惩罚）不严格匹配真实集合的预测集，Hamming loss （汉明损失）惩罚 individual labels （独立标签）。因此，Hamming loss（汉明损失）高于 zero-one loss（0-1 损失），总是在 0 和 1 之间，包括 0 和 1;预测真正的标签的正确的 subset or superset （子集或超集）将给出 0 和 1 之间的 Hamming loss（汉明损失）。</p>
</div>
</div>
<div class="section" id="jaccard-score">
<span id="jaccard-similarity-score"></span><h3>3.3.2.7. Jaccard 相似系数 score<a class="headerlink" href="#jaccard-score" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">jaccard_similarity_score</span></code></a> 函数计算 pairs of label sets （标签组对）之间的 <a class="reference external" href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity coefficients</a> 也称作 Jaccard index 的平均值（默认）或总和。</p>
<p>将第 <span class="math">i</span> 个样本的 Jaccard similarity coefficient 与 被标注过的真实数据的标签集 <span class="math">y_i</span> 和 predicted label set （预测标签集）:math:<cite>hat{y}_i</cite> 定义为</p>
<div class="math">
<p><span class="math">J(y_i, \hat{y}_i) = \frac{|y_i \cap \hat{y}_i|}{|y_i \cup \hat{y}_i|}.</span></p>
</div><p>在 binary and multiclass classification （二分和多类分类）中，Jaccard similarity coefficient score 等于 classification accuracy（分类精度）。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">jaccard_similarity_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_similarity_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_similarity_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_similarity_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.75</span>
</pre></div>
</div>
</div>
<div class="section" id="f-measures">
<span id="precision-recall-f-measure-metrics"></span><h3>3.3.2.8. 精准，召回和 F-measures<a class="headerlink" href="#f-measures" title="Permalink to this headline">¶</a></h3>
<p>直观地来理解，<a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall#Precision">precision</a> 是 the ability of the classifier not to label as positive a sample that is negative （classifier （分类器）的标签不能被标记为正的样本为负的能力），并且 <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall">recall</a> 是 classifier （分类器）查找所有 positive samples （正样本）的能力。</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">F-measure</a> (<span class="math">F_\beta</span> 和 <span class="math">F_1</span> measures) 可以解释为 precision （精度）和 recall （召回）的 weighted harmonic mean （加权调和平均值）。 <span class="math">F_\beta</span> measure 值达到其最佳值 1 ，其最差分数为 0 。与 <span class="math">\beta = 1</span>, <span class="math">F_\beta</span> 和 <span class="math">F_1</span> 是等价的， recall （召回）和 precision （精度）同样重要。</p>
<p><a class="reference internal" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a> 通过改变 decision threshold （决策阈值）从 ground truth label （被标记的真实数据标签） 和 score given by the classifier （分类器给出的分数）计算 precision-recall curve （精确召回曲线）。</p>
<p><a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision_score</span></code></a> 函数根据 prediction scores （预测分数）计算出 average precision (AP)（平均精度）。该分数对应于 precision-recall curve （精确召回曲线）下的面积。该值在 0 和 1 之间，并且越高越好。通过 random predictions （随机预测）， AP 是 fraction of positive samples （正样本的分数）。</p>
<p>几个函数可以让您 analyze the precision （分析精度），recall（召回） 和 F-measures 得分:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">average_precision_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;…])</td>
<td>Compute average precision (AP) from prediction scores</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">f1_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the F1 score, also known as balanced F-score or F-measure</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fbeta_score</span></code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the F-beta score</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a>(y_true,&nbsp;probas_pred)</td>
<td>Compute precision-recall pairs for different probability thresholds</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>(y_true,&nbsp;y_pred)</td>
<td>Compute precision, recall, F-measure and support for each class</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the precision</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">recall_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td>Compute the recall</td>
</tr>
</tbody>
</table>
<p>请注意，<a class="reference internal" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a> 函数仅限于 binary case （二分情况）。 <a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision_score</span></code></a> 函数只适用于 binary classification and multilabel indicator format （二分类和多标签指示器格式）。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>
例如 <a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">f1_score</span></code></a> 用于分类文本文档的用法。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="std std-ref">Parameter estimation using grid search with cross-validation</span></a>
例如 <a class="reference internal" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">recall_score</span></code></a> 用于 using grid search with nested cross-validation （使用嵌套交叉验证的网格搜索）来估计参数。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"><span class="std std-ref">Precision-Recall</span></a>
例如 <a class="reference internal" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a> 用于 evaluate classifier output quality（评估分类器输出质量）。</li>
</ul>
</div>
<div class="section" id="id14">
<h4>3.3.2.8.1. 二分类<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>在二分类任务中，术语 ‘’positive（正）’’ 和 ‘’negative（负）’’ 是指 classifier’s prediction （分类器的预测），术语 ‘’true（真）’’ 和 ‘’false（假）’’ 是指该预测是否对应于 external judgment （外部判断）（有时被称为 ‘’observation（观测值）’‘）。给出这些定义，我们可以指定下表:</p>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="32%" />
<col width="39%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&#160;</td>
<td colspan="2">Actual class (observation)</td>
</tr>
<tr class="row-even"><td rowspan="2">Predicted class
(expectation)</td>
<td>tp (true positive)
Correct result</td>
<td>fp (false positive)
Unexpected result</td>
</tr>
<tr class="row-odd"><td>fn (false negative)
Missing result</td>
<td>tn (true negative)
Correct absence of result</td>
</tr>
</tbody>
</table>
<p>在这种情况下，我们可以定义 precision（精度）, recall（召回） 和 F-measure 的概念:</p>
<div class="math">
<p><span class="math">\text{precision} = \frac{tp}{tp + fp},</span></p>
</div><div class="math">
<p><span class="math">\text{recall} = \frac{tp}{tp + fn},</span></p>
</div><div class="math">
<p><span class="math">F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}.</span></p>
</div><p>以下是 binary classification （二分类）中的一些小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  
<span class="go">0.83...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
<span class="go">0.66...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 
<span class="go">0.55...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  
<span class="go">(array([ 0.66...,  1.        ]), array([ 1. ,  0.5]), array([ 0.71...,  0.83...]), array([2, 2]...))</span>


<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">precision_recall_curve</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">average_precision_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span>  
<span class="go">array([ 0.66...,  0.5       ,  1.        ,  1.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall</span>
<span class="go">array([ 1. ,  0.5,  0.5,  0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">threshold</span>
<span class="go">array([ 0.35,  0.4 ,  0.8 ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>  
<span class="go">0.83...</span>
</pre></div>
</div>
</div>
<div class="section" id="id15">
<h4>3.3.2.8.2. 多类和多标签分类<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h4>
<p>在 multiclass and multilabel classification task（多类和多标签分类任务）中，precision（精度）, recall（召回）, and F-measures 的概念可以独立地应用于每个标签。
有以下几种方法 combine results across labels （将结果跨越标签组合），由 <code class="docutils literal notranslate"><span class="pre">average</span></code> 参数指定为 <a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision_score</span></code></a> （仅用于 multilabel）， <a class="reference internal" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">f1_score</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">fbeta_score</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">recall_score</span></code></a> 函数，如上 <a class="reference internal" href="#average"><span class="std std-ref">above</span></a> 所述。请注意，对于在包含所有标签的多类设置中进行 “micro”-averaging （”微”平均），将产生相等的 precision（精度）， recall（召回）和 <span class="math">F</span> ，而 “weighted（加权）” averaging（平均）可能会产生 precision（精度）和 recall（召回）之间的 F-score 。</p>
<p>为了使这一点更加明确，请考虑以下 notation （符号）:</p>
<ul class="simple">
<li><span class="math">y</span> <em>predicted（预测）</em> <span class="math">(sample, label)</span> 对</li>
<li><span class="math">\hat{y}</span> <em>true（真）</em> <span class="math">(sample, label)</span> 对</li>
<li><span class="math">L</span> labels 集合</li>
<li><span class="math">S</span> samples 集合</li>
<li><span class="math">y_s</span> <span class="math">y</span> 的子集与样本 <span class="math">s</span>, 即 <span class="math">y_s := \left\{(s', l) \in y | s' = s\right\}</span></li>
<li><span class="math">y_l</span> <span class="math">y</span> 的子集与 label <span class="math">l</span></li>
<li>类似的, <span class="math">\hat{y}_s</span> 和 <span class="math">\hat{y}_l</span> 是 <span class="math">\hat{y}</span> 的子集</li>
<li><span class="math">P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}</span></li>
<li><span class="math">R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}</span>
(Conventions （公约）在处理 <span class="math">B = \emptyset</span> 有所不同; 这个实现使用 <span class="math">R(A, B):=0</span>, 与 <span class="math">P</span> 类似.)</li>
<li><span class="math">F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}</span></li>
</ul>
<p>然后将 metrics （指标）定义为:</p>
<table border="1" class="docutils">
<colgroup>
<col width="4%" />
<col width="32%" />
<col width="32%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><code class="docutils literal notranslate"><span class="pre">average</span></code></th>
<th class="head">Precision</th>
<th class="head">Recall</th>
<th class="head">F_beta</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;micro&quot;</span></code></td>
<td><span class="math">P(y, \hat{y})</span></td>
<td><span class="math">R(y, \hat{y})</span></td>
<td><span class="math">F_\beta(y, \hat{y})</span></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">&quot;samples&quot;</span></code></td>
<td><span class="math">\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)</span></td>
<td><span class="math">\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)</span></td>
<td><span class="math">\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)</span></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;macro&quot;</span></code></td>
<td><span class="math">\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)</span></td>
<td><span class="math">\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)</span></td>
<td><span class="math">\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)</span></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">&quot;weighted&quot;</span></code></td>
<td><span class="math">\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| P(y_l, \hat{y}_l)</span></td>
<td><span class="math">\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| R(y_l, \hat{y}_l)</span></td>
<td><span class="math">\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| F_\beta(y_l, \hat{y}_l)</span></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
<td><span class="math">\langle P(y_l, \hat{y}_l) | l \in L \rangle</span></td>
<td><span class="math">\langle R(y_l, \hat{y}_l) | l \in L \rangle</span></td>
<td><span class="math">\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle</span></td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.22...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="gp">... </span><span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.23...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">... </span><span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">(array([ 0.66...,  0.        ,  0.        ]), array([ 1.,  0.,  0.]), array([ 0.71...,  0.        ,  0.        ]), array([2, 2, 2]...))</span>
</pre></div>
</div>
<p>For multiclass classification with a “negative class”, it is possible to exclude some labels:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="gp">... </span><span class="c1"># excluding 0, no labels were correctly recalled</span>
<span class="go">0.0</span>
</pre></div>
</div>
<p>Similarly, labels not present in the data sample may be accounted for in macro-averaging.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="gp">... </span><span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.166...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="hinge-loss">
<span id="id16"></span><h3>3.3.2.9. Hinge loss<a class="headerlink" href="#hinge-loss" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">hinge_loss</span></code></a> 函数使用 <a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> 计算模型和数据之间的 average distance （平均距离），这是一种只考虑 prediction errors （预测误差）的 one-sided metric （单向指标）。（Hinge loss 用于最大边界分类器，如支持向量机）</p>
<p>如果标签用 +1 和 -1 编码，则 <span class="math">y</span>: 是真实值，并且 <span class="math">w</span> 是由 <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> 输出的 predicted decisions （预测决策），则 hinge loss 定义为:</p>
<div class="math">
<p><span class="math">L_\text{Hinge}(y, w) = \max\left\{1 - wy, 0\right\} = \left|1 - wy\right|_+</span></p>
</div><p>如果有两个以上的标签， <a class="reference internal" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">hinge_loss</span></code></a> 由于 Crammer &amp; Singer 而使用了 multiclass variant （多类型变体）。
<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf">Here</a> 是描述它的论文。</p>
<p>如果 <span class="math">y_w</span> 是真实标签的 predicted decision （预测决策），并且 <span class="math">y_t</span> 是所有其他标签的预测决策的最大值，其中预测决策由 decision function （决策函数）输出，则 multiclass hinge loss 定义如下:</p>
<div class="math">
<p><span class="math">L_\text{Hinge}(y_w, y_t) = \max\left\{1 + y_t - y_w, 0\right\}</span></p>
</div><p>这里是一个小例子，演示了在 binary class （二类）问题中使用了具有 svm classifier （svm 的分类器）的 <a class="reference internal" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">hinge_loss</span></code></a> 函数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">hinge_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="go">     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=0, tol=0.0001,</span>
<span class="go">     verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span>  
<span class="go">array([-2.18...,  2.36...,  0.09...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hinge_loss</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">pred_decision</span><span class="p">)</span>  
<span class="go">0.3...</span>
</pre></div>
</div>
<p>这里是一个示例，演示了在 multiclass problem （多类问题）中使用了具有 svm 分类器的 <a class="reference internal" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">hinge_loss</span></code></a> 函数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="go">     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,</span>
<span class="go">     verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_decision</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  
<span class="go">0.56...</span>
</pre></div>
</div>
</div>
<div class="section" id="log">
<span id="log-loss"></span><h3>3.3.2.10. Log 损失<a class="headerlink" href="#log" title="Permalink to this headline">¶</a></h3>
<p>Log loss，又被称为 logistic regression loss（logistic 回归损失）或者 cross-entropy loss（交叉熵损失） 定义在 probability estimates （概率估计）。它通常用于 (multinomial) logistic regression （（多项式）logistic 回归）和 neural networks （神经网络）以及 expectation-maximization （期望最大化）的一些变体中，并且可用于评估分类器的 probability outputs （概率输出）（<code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>）而不是其 discrete predictions （离散预测）。</p>
<p>对于具有真实标签 <span class="math">y \in \{0,1\}</span> 的 binary classification （二分类）和 probability estimate （概率估计） <span class="math">p = \operatorname{Pr}(y = 1)</span>, 每个样本的 log loss 是给定的分类器的 negative log-likelihood 真正的标签:</p>
<div class="math">
<p><span class="math">L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))</span></p>
</div><p>这扩展到 multiclass case （多类案例）如下。
让一组样本的真实标签被编码为 1-of-K binary indicator matrix <span class="math">Y</span>, 即 如果样本 <span class="math">i</span> 具有取自一组 <span class="math">K</span> 个标签的标签 <span class="math">k</span> ，则 <span class="math">y_{i,k} = 1</span> 。令 <span class="math">P</span> 为 matrix of probability estimates （概率估计矩阵）， <span class="math">p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)</span> 。那么整套的 log loss 就是</p>
<div class="math">
<p><span class="math">L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}^{N-1} \sum_{k=0}^{K-1} y_{i,k} \log p_{i,k}</span></p>
</div><p>为了看这这里如何 generalizes （推广）上面给出的 binary log loss （二分 log loss），请注意，在 binary case （二分情况下），<span class="math">p_{i,0} = 1 - p_{i,1}</span> 和 <span class="math">y_{i,0} = 1 - y_{i,1}</span> ，因此扩展 <span class="math">y_{i,k} \in \{0,1\}</span> 的 inner sum （内部和），给出 binary log loss （二分 log loss）。</p>
<p><a class="reference internal" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">log_loss</span></code></a> 函数计算出一个 a list of ground-truth labels （已标注的真实数据的标签的列表）和一个 probability matrix （概率矩阵） 的 log loss，由 estimator （估计器）的 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 方法返回。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">log_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>    <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.1738...</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">y_pred</span></code> 中的第一个 <code class="docutils literal notranslate"><span class="pre">[.9,</span> <span class="pre">.1]</span></code> 表示第一个样本具有标签 0 的 90% 概率。log loss 是非负数。</p>
</div>
<div class="section" id="matthews-corrcoef">
<span id="id18"></span><h3>3.3.2.11. 马修斯相关系数<a class="headerlink" href="#matthews-corrcoef" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="xref py py-func docutils literal notranslate"><span class="pre">matthews_corrcoef</span></code></a> 函数用于计算 binary classes （二分类）的 <a class="reference external" href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Matthew’s correlation coefficient (MCC)</a> 引用自 Wikipedia:</p>
<blockquote>
<div>“Matthews correlation coefficient（马修斯相关系数）用于机器学习，作为 binary (two-class) classifications （二分类）分类质量的度量。它考虑到 true and false positives and negatives （真和假的 positives 和 negatives），通常被认为是可以使用的 balanced measure（平衡措施），即使 classes are of very different sizes （类别大小不同）。MCC 本质上是 -1 和 +1 之间的相关系数值。系数 +1 表示完美预测，0 表示平均随机预测， -1 表示反向预测。statistic （统计量）也称为 phi coefficient （phi）系数。”</div></blockquote>
<p>在 binary (two-class) （二分类）情况下，<span class="math">tp</span>, <span class="math">tn</span>, <span class="math">fp</span> 和 <span class="math">fn</span> 分别是 true positives, true negatives, false positives 和 false negatives 的数量，MCC 定义为</p>
<div class="math">
<p><span class="math">MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.</span></p>
</div><p>在 multiclass case （多类的情况）下， Matthews correlation coefficient（马修斯相关系数） 可以根据 <span class="math">K</span> classes （类）的 <a class="reference internal" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a> <span class="math">C</span> 定义 <a class="reference external" href="http://rk.kvl.dk/introduction/index.html">defined</a> 。为了简化定义，考虑以下中间变量:</p>
<ul class="simple">
<li><span class="math">t_k=\sum_{i}^{K} C_{ik}</span> 真正发生了 <span class="math">k</span> 类的次数,</li>
<li><span class="math">p_k=\sum_{i}^{K} C_{ki}</span> <span class="math">k</span> 类被预测的次数,</li>
<li><span class="math">c=\sum_{k}^{K} C_{kk}</span> 正确预测的样本总数,</li>
<li><span class="math">s=\sum_{i}^{K} \sum_{j}^{K} C_{ij}</span> 样本总数.</li>
</ul>
<p>然后 multiclass MCC 定义为:</p>
<div class="math">
<p><span class="math">MCC = \frac{
    c \times s - \sum_{k}^{K} p_k \times t_k
}{\sqrt{
    (s^2 - \sum_{k}^{K} p_k^2) \times
    (s^2 - \sum_{k}^{K} t_k^2)
}}</span></p>
</div><p>当有两个以上的标签时， MCC 的值将不再在 -1 和 +1 之间。相反，根据已经标注的真实数据的数量和分布情况，最小值将介于 -1 和 0 之间。最大值始终为 +1 。</p>
<p>这是一个小例子，说明了使用 <a class="reference internal" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="xref py py-func docutils literal notranslate"><span class="pre">matthews_corrcoef</span></code></a> 函数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">matthews_corrcoef</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">-0.33...</span>
</pre></div>
</div>
</div>
<div class="section" id="receiver-operating-characteristic-roc">
<span id="roc-metrics"></span><h3>3.3.2.12. Receiver operating characteristic (ROC)<a class="headerlink" href="#receiver-operating-characteristic-roc" title="Permalink to this headline">¶</a></h3>
<p>函数 <a class="reference internal" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_curve</span></code></a> 计算 <a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic curve, or ROC curve</a>.
引用 Wikipedia :</p>
<blockquote>
<div>“A receiver operating characteristic (ROC), 或者简单的 ROC 曲线，是一个图形图，说明了 binary classifier （二分分类器）系统的性能，因为 discrimination threshold （鉴别阈值）是变化的。它是通过在不同的阈值设置下，从 true positives out of the positives (TPR = true positive 比例) 与 false positives out of the negatives (FPR = false positive 比例) 绘制 true positive 的比例来创建的。 TPR 也称为 sensitivity（灵敏度），FPR 是减去 specificity（特异性） 或 true negative 比例。”</div></blockquote>
<p>该函数需要真正的 binar value （二分值）和 target scores（目标分数），这可以是 positive class 的 probability estimates （概率估计），confidence values（置信度值）或 binary decisions（二分决策）。
这是一个如何使用 <a class="reference internal" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_curve</span></code></a> 函数的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_curve</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span>
<span class="go">array([ 0. ,  0.5,  0.5,  1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tpr</span>
<span class="go">array([ 0.5,  0.5,  1. ,  1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">thresholds</span>
<span class="go">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span>
</pre></div>
</div>
<p>该图显示了这样的 ROC 曲线的示例:</p>
<a class="reference external image-reference" href="../auto_examples/model_selection/plot_roc.html"><img alt="modules/../auto_examples/model_selection/images/sphx_glr_plot_roc_001.png" class="align-center" src="modules/../auto_examples/model_selection/images/sphx_glr_plot_roc_001.png" /></a>
<p><a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a> 函数计算 receiver operating characteristic (ROC) 曲线下的面积，也由 AUC 和 AUROC 表示。通过计算 roc 曲线下的面积，曲线信息总结为一个数字。
有关更多的信息，请参阅 <a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">Wikipedia article on AUC</a> .</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_auc_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="go">0.75</span>
</pre></div>
</div>
<p>在 multi-label classification （多标签分类）中， <a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a> 函数通过在标签上进行平均来扩展 <a class="reference internal" href="#average"><span class="std std-ref">above</span></a> .</p>
<p>与诸如 subset accuracy （子集精确度），Hamming loss（汉明损失）或 F1 score 的 metrics（指标）相比， ROC 不需要优化每个标签的阈值。<a class="reference internal" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a> 函数也可以用于 multi-class classification （多类分类），如果预测的输出被 binarized （二分化）。</p>
<a class="reference external image-reference" href="../auto_examples/model_selection/plot_roc.html"><img alt="modules/../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png" class="align-center" src="modules/../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png" /></a>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py"><span class="std std-ref">Receiver Operating Characteristic (ROC)</span></a>
例如使用 ROC 来评估分类器输出的质量。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py"><span class="std std-ref">Receiver Operating Characteristic (ROC) with cross validation</span></a>
例如使用 ROC 来评估分类器输出质量，使用 cross-validation （交叉验证）。</li>
<li>参阅 <a class="reference internal" href="../auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py"><span class="std std-ref">Species distribution modeling</span></a>
例如使用 ROC 来 model species distribution 模拟物种分布。</li>
</ul>
</div>
</div>
<div class="section" id="zero-one-loss">
<span id="id19"></span><h3>3.3.2.13. 零一损失<a class="headerlink" href="#zero-one-loss" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">zero_one_loss</span></code></a> 函数通过 <span class="math">n_{\text{samples}}</span> 计算 0-1 classification loss (<span class="math">L_{0-1}</span>) 的 sum （和）或 average （平均值）。默认情况下，函数在样本上 normalizes （标准化）。要获得 <span class="math">L_{0-1}</span> 的总和，将 <code class="docutils literal notranslate"><span class="pre">normalize</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p>在 multilabel classification （多标签分类）中，如果零标签与标签严格匹配，则 <a class="reference internal" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">zero_one_loss</span></code></a> 将一个子集作为一个子集，如果有任何错误，则为零。默认情况下，函数返回不完全预测子集的百分比。为了得到这样的子集的计数，将 <code class="docutils literal notranslate"><span class="pre">normalize</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p>
<p>如果 <span class="math">\hat{y}_i</span> 是第 <span class="math">i</span> 个样本的预测值，<span class="math">y_i</span> 是相应的真实值，则 0-1 loss <span class="math">L_{0-1}</span> 定义为:</p>
<div class="math">
<p><span class="math">L_{0-1}(y_i, \hat{y}_i) = 1(\hat{y}_i \not= y_i)</span></p>
</div><p>其中 <span class="math">1(x)</span> 是 <a class="reference external" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">zero_one_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.25</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下，第一个标签集 [0,1] 有错误:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.5</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>  <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>参阅 <a class="reference internal" href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"><span class="std std-ref">Recursive feature elimination with cross-validation</span></a>
例如 zero one loss 使用以通过 cross-validation （交叉验证）执行递归特征消除。</li>
</ul>
</div>
</div>
<div class="section" id="brier">
<span id="brier-score-loss"></span><h3>3.3.2.14. Brier 分数损失<a class="headerlink" href="#brier" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss" title="sklearn.metrics.brier_score_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">brier_score_loss</span></code></a> 函数计算二进制类的 <a class="reference external" href="https://en.wikipedia.org/wiki/Brier_score">Brier 分数</a> 。引用维基百科：</p>
<blockquote>
<div>“Brier 分数是一个特有的分数函数，用于衡量概率预测的准确性。它适用于预测必须将概率分配给一组相互排斥的离散结果的任务。”</div></blockquote>
<p>该函数返回的是 实际结果与可能结果 的预测概率之间均方差的得分。 实际结果必须为1或0（真或假），而实际结果的预测概率可以是0到1之间的值。</p>
<p>Brier 分数损失也在0到1之间，分数越低（均方差越小），预测越准确。它可以被认为是对一组概率预测的 “校准” 的度量。</p>
<div class="math">
<p><span class="math">BS = \frac{1}{N} \sum_{t=1}^{N}(f_t - o_t)^2</span></p>
</div><p>其中: <span class="math">N</span> 是预测的总数， <span class="math">f_t</span> 是实际结果 <span class="math">o_t</span> 的预测概率。</p>
<p>这是一个使用这个函数的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">brier_score_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true_categorical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
<span class="go">0.055</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_prob</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">0.055</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true_categorical</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;ham&quot;</span><span class="p">)</span>
<span class="go">0.055</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>请参阅分类器的概率校准 <a class="reference internal" href="../auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py"><span class="std std-ref">Probability calibration of classifiers</span></a> ，通过 Brier 分数损失使用示例 来执行分类器的概率校准。</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><ol class="first upperalpha" start="7">
<li>Brier, <a class="reference external" href="http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf">以概率表示的预测验证</a> , 月度天气评估78.1（1950）</li>
</ol>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="multilabel-ranking-metrics">
<span id="id23"></span><h2>3.3.3. 多标签排名指标<a class="headerlink" href="#multilabel-ranking-metrics" title="Permalink to this headline">¶</a></h2>
<p>在多分类学习中，每个样本可以具有与其相关联的任何数量的真实标签。目标是给予高分，更好地评价真实标签。</p>
<div class="section" id="coverage-error">
<span id="id24"></span><h3>3.3.3.1. 覆盖误差<a class="headerlink" href="#coverage-error" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error" title="sklearn.metrics.coverage_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">coverage_error</span></code></a> 函数计算必须包含在最终预测中的标签的平均数，以便预测所有真正的标签。
如果您想知道有多少 top 评分标签，您必须通过平均来预测，而不会丢失任何真正的标签，这很有用。
因此，此指标的最佳价值是真正标签的平均数量。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">我们的实现的分数比 Tsoumakas 等人在2010年的分数大1。
这扩展了它来处理一个具有0个真实标签实例的退化情况。</p>
</div>
<p>正式地，给定真实标签 <span class="math">y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}</span> 的二进制指示矩阵和与每个标签 <span class="math">\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}</span> 相关联的分数，覆盖范围被定义为</p>
<div class="math">
<p><span class="math">coverage(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \max_{j:y_{ij} = 1} \text{rank}_{ij}</span></p>
</div><p>与 <span class="math">\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|</span> 。给定等级定义，通过给出将被分配给所有绑定值的最大等级， <code class="docutils literal notranslate"><span class="pre">y_scores</span></code> 中的关系会被破坏。</p>
<p>这是一个使用这个函数的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">coverage_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coverage_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>
<span class="go">2.5</span>
</pre></div>
</div>
</div>
<div class="section" id="label-ranking-average-precision">
<span id="id25"></span><h3>3.3.3.2. 标签排名平均精度<a class="headerlink" href="#label-ranking-average-precision" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score" title="sklearn.metrics.label_ranking_average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">label_ranking_average_precision_score</span></code></a> 函数实现标签排名平均精度（LRAP）。
该度量值与 <a class="reference internal" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision_score</span></code></a> 函数相关联，但是基于标签排名的概念，而不是精确度和召回。</p>
<p>标签排名平均精度（LRAP）是分配给每个样本的每个真实标签的平均值，真实对总标签与较低分数的比率。
如果能够为每个样本相关标签提供更好的排名，这个指标就会产生更好的分数。
获得的得分总是严格大于0，最佳值为1。如果每个样本只有一个相关标签，则标签排名平均精度等于 <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">平均倒数等级</a> 。</p>
<p>正式地，给定真实标签 <span class="math">y \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}</span> 的二进制指示矩阵和与每个标签 <span class="math">\hat{f} \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}</span> 相关联的得分，平均精度被定义为</p>
<div class="math">
<p><span class="math">LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|}
  \sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}</span></p>
</div><p>与 <span class="math">\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}</span>，
<span class="math">\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|</span> 和 <span class="math">|\cdot|</span> 是集合的 l0 范数或基数。</p>
<p>这是一个使用这个函数的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">label_ranking_average_precision_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_ranking_average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span> 
<span class="go">0.416...</span>
</pre></div>
</div>
</div>
<div class="section" id="label-ranking-loss">
<span id="id27"></span><h3>3.3.3.3. 排序损失<a class="headerlink" href="#label-ranking-loss" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss" title="sklearn.metrics.label_ranking_loss"><code class="xref py py-func docutils literal notranslate"><span class="pre">label_ranking_loss</span></code></a> 函数计算在样本上平均排序错误的标签对数量的排序损失，即真实标签的分数低于假标签，由虚假和真实标签的倒数加权。最低可实现的排名损失为零。</p>
<p>正式地，给定真相标签 <span class="math">y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}</span> 的二进制指示矩阵和与每个标签 <span class="math">\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}</span> 相关联的得分，排序损失被定义为</p>
<div class="math">
<p><span class="math">\text{ranking\_loss}(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|(n_\text{labels} - |y_i|)}
  \left|\left\{(k, l): \hat{f}_{ik} &lt; \hat{f}_{il}, y_{ik} = 1, y_{il} = 0&nbsp;\right\}\right|</span></p>
</div><p>其中 <span class="math">|\cdot|</span> 是 <span class="math">\ell_0</span> 范数或集合的基数。</p>
<p>这是一个使用这个函数的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">label_ranking_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_ranking_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span> 
<span class="go">0.75...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With the following prediction, we have perfect and minimal loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_ranking_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010). 挖掘多标签数据。在数据挖掘和知识发现手册（第667-685页）。美国 Springer.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="regression-metrics">
<span id="id28"></span><h2>3.3.4. 回归指标<a class="headerlink" href="#regression-metrics" title="Permalink to this headline">¶</a></h2>
<p>该 <a class="reference internal" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> 模块实现了一些 loss, score 以及 utility 函数以测量 regression（回归）的性能.
其中一些已经被加强以处理多个输出的场景: <a class="reference internal" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_error</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_absolute_error</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">explained_variance_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a>.</p>
<p>这些函数有 <code class="docutils literal notranslate"><span class="pre">multioutput</span></code> 这样一个 keyword（关键的）参数, 它指定每一个目标的 score（得分）或 loss（损失）的平均值的方式.
默认是 <code class="docutils literal notranslate"><span class="pre">'uniform_average'</span></code>, 其指定了输出时一致的权重均值.
如果一个 <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> 的 shape <code class="docutils literal notranslate"><span class="pre">(n_outputs,)</span></code> 被传递, 则其中的 entries（条目）将被解释为权重，并返回相应的加权平均值.
如果 <code class="docutils literal notranslate"><span class="pre">multioutput</span></code> 指定了 <code class="docutils literal notranslate"><span class="pre">'raw_values'</span></code> , 则所有未改变的部分 score（得分）或 loss（损失）将以 <code class="docutils literal notranslate"><span class="pre">(n_outputs,)</span></code> 形式的数组返回.</p>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">explained_variance_score</span></code></a> 函数接受一个额外的值 <code class="docutils literal notranslate"><span class="pre">'variance_weighted'</span></code> 用于 <code class="docutils literal notranslate"><span class="pre">multioutput</span></code> 参数.
该选项通过相应目标变量的方差使得每个单独的 score 进行加权.
该设置量化了全局捕获的未缩放方差.
如果目标变量的大小不一样, 则该 score 更好地解释了较高的方差变量.
<code class="docutils literal notranslate"><span class="pre">multioutput='variance_weighted'</span></code> 是 <a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a> 的默认值以向后兼容.
以后该值会被改成 <code class="docutils literal notranslate"><span class="pre">uniform_average</span></code>.</p>
<div class="section" id="explained-variance-score">
<span id="id29"></span><h3>3.3.4.1. 解释方差得分<a class="headerlink" href="#explained-variance-score" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">explained_variance_score</span></code></a> 函数计算了 <a class="reference external" href="https://en.wikipedia.org/wiki/Explained_variation">explained variance
regression score（解释的方差回归得分）</a>.</p>
<p>如果 <span class="math">\hat{y}</span> 是预估的目标输出, <span class="math">y</span> 是相应（正确的）目标输出, 并且 <span class="math">Var</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Variance">方差</a>, 标准差的平方, 那么解释的方差预估如下:</p>
<div class="math">
<p><span class="math">\texttt{explained\_{}variance}(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}</span></p>
</div><p>最好的得分是 1.0, 值越低越差.</p>
<p>下面是一下有关 <a class="reference internal" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">explained_variance_score</span></code></a> 函数使用的一些例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">explained_variance_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.957...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">array([ 0.967...,  1.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">0.990...</span>
</pre></div>
</div>
</div>
<div class="section" id="mean-absolute-error">
<span id="id31"></span><h3>3.3.4.2. 平均绝对误差<a class="headerlink" href="#mean-absolute-error" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_absolute_error</span></code></a> 函数计算了 <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_absolute_error">平均绝对误差</a>, 一个对应绝对误差损失预期值或者 <span class="math">l1</span>-norm 损失的风险度量.</p>
<p>如果 <span class="math">\hat{y}_i</span> 是 <span class="math">i</span>-th 样本的预测值,
并且 <span class="math">y_i</span> 是对应的真实值, 则平均绝对误差 (MAE) 预估的 <span class="math">n_{\text{samples}}</span> 定义如下</p>
<div class="math">
<p><span class="math">\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|.</span></p>
</div><p>下面是一个有关 <a class="reference internal" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_absolute_error</span></code></a> 函数用法的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_absolute_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="go">array([ 0.5,  1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">0.849...</span>
</pre></div>
</div>
</div>
<div class="section" id="mean-squared-error">
<span id="id33"></span><h3>3.3.4.3. 均方误差<a class="headerlink" href="#mean-squared-error" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_error</span></code></a> 函数计算了 <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error">均方误差</a>, 一个对应于平方（二次）误差或损失的预期值的风险度量.</p>
<p>如果 <span class="math">\hat{y}_i</span> 是 <span class="math">i</span>-th 样本的预测值,
并且 <span class="math">y_i</span> 是对应的真实值, 则均方误差（MSE）预估的 <span class="math">n_{\text{samples}}</span> 定义如下</p>
<div class="math">
<p><span class="math">\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (y_i - \hat{y}_i)^2.</span></p>
</div><p>下面是一个有关 <a class="reference internal" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_error</span></code></a> 函数用法的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.375</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.7083...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li>点击 <a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a>
查看均方误差用于梯度上升（gradient boosting）回归的使用例子。</li>
</ul>
</div>
</div>
<div class="section" id="mean-squared-log-error">
<span id="id35"></span><h3>3.3.4.4. 均方误差对数<a class="headerlink" href="#mean-squared-log-error" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_log_error</span></code></a> 函数计算了一个对应平方对数（二次）误差或损失的预估值风险度量.</p>
<p>如果 <span class="math">\hat{y}_i</span> 是 <span class="math">i</span>-th 样本的预测值,
并且 <span class="math">y_i</span> 是对应的真实值, 则均方误差对数（MSLE）预估的 <span class="math">n_{\text{samples}}</span> 定义如下</p>
<div class="math">
<p><span class="math">\text{MSLE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )^2.</span></p>
</div><p>其中 <span class="math">\log_e (x)</span> 表示 <span class="math">x</span> 的自然对数.
当目标具有指数增长的趋势时, 该指标最适合使用, 例如人口数量, 跨年度商品的平均销售额等.
请注意, 该指标会对低于预测的估计值进行估计.</p>
<p>下面是一个有关 <a class="reference internal" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_log_error</span></code></a> 函数用法的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_log_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.039...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.044...</span>
</pre></div>
</div>
</div>
<div class="section" id="median-absolute-error">
<span id="id36"></span><h3>3.3.4.5. 中位绝对误差<a class="headerlink" href="#median-absolute-error" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">median_absolute_error</span></code></a> 函数尤其有趣, 因为它的离群值很强.
通过取目标和预测之间的所有绝对差值的中值来计算损失.</p>
<p>如果 <span class="math">\hat{y}_i</span> 是 <span class="math">i</span>-th 样本的预测值,
并且 <span class="math">y_i</span> 是对应的真实值, 则中位绝对误差（MedAE）预估的 <span class="math">n_{\text{samples}}</span> 定义如下</p>
<div class="math">
<p><span class="math">\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid).</span></p>
</div><p>该 <a class="reference internal" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">median_absolute_error</span></code></a> 函数不支持多输出.</p>
<p>下面是一个有关 <a class="reference internal" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">median_absolute_error</span></code></a> 函数用法的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">median_absolute_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</div>
<div class="section" id="r2-score">
<span id="id37"></span><h3>3.3.4.6. R² score, 可决系数<a class="headerlink" href="#r2-score" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a> 函数计算了 computes R², 即 <a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">可决系数</a>.
它提供了将来样本如何可能被模型预测的估量.
最佳分数为 1.0, 可以为负数（因为模型可能会更糟）.
总是预测 y 的预期值，不考虑输入特征的常数模型将得到 R^2 得分为 0.0.</p>
<p>如果 <span class="math">\hat{y}_i</span> 是 <span class="math">i</span>-th 样本的预测值,
并且 <span class="math">y_i</span> 是对应的真实值, 则 R² 得分预估的 <span class="math">n_{\text{samples}}</span> 定义如下</p>
<div class="math">
<p><span class="math">R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^{n_{\text{samples}} - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n_\text{samples} - 1} (y_i - \bar{y})^2}</span></p>
</div><p>其中 <span class="math">\bar{y} =  \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1} y_i</span>.</p>
<p>下面是一个有关 <a class="reference internal" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">r2_score</span></code></a> 函数用法的小例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.948...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">0.938...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;uniform_average&#39;</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">0.936...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">array([ 0.965...,  0.908...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">0.925...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li>点击 <a class="reference internal" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">Lasso and Elastic Net for Sparse Signals</span></a>
查看关于R²用于评估在Lasso and Elastic Net on sparse signals上的使用.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="clustering-metrics">
<span id="id39"></span><h2>3.3.5. 聚类指标<a class="headerlink" href="#clustering-metrics" title="Permalink to this headline">¶</a></h2>
<p>该 <a class="reference internal" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> 模块实现了一些 loss, score 和 utility 函数.
更多信息请参阅 <a class="reference internal" href="clustering.html#clustering-evaluation"><span class="std std-ref">聚类性能度量</span></a> 部分, 例如聚类, 以及用于二分聚类的 <a class="reference internal" href="biclustering.html#biclustering-evaluation"><span class="std std-ref">Biclustering 评测</span></a>.</p>
</div>
<div class="section" id="dummy-estimators">
<span id="id40"></span><h2>3.3.6. 虚拟估计<a class="headerlink" href="#dummy-estimators" title="Permalink to this headline">¶</a></h2>
<p>在进行监督学习的过程中，简单的 sanity check（理性检查）包括将人的估计与简单的经验法则进行比较.
<a class="reference internal" href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DummyClassifier</span></code></a> 实现了几种简单的分类策略:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">stratified</span></code> 通过在训练集类分布方面来生成随机预测.</li>
<li><code class="docutils literal notranslate"><span class="pre">most_frequent</span></code> 总是预测训练集中最常见的标签.</li>
<li><code class="docutils literal notranslate"><span class="pre">prior</span></code> always predicts the class that maximizes the class prior
(like <code class="docutils literal notranslate"><span class="pre">most_frequent`)</span> <span class="pre">and</span> <span class="pre">``predict_proba</span></code> returns the class prior.</li>
<li><code class="docutils literal notranslate"><span class="pre">uniform</span></code> 随机产生预测.</li>
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">constant</span></code> 总是预测用户提供的常量标签.</dt>
<dd>A major motivation of this method is F1-scoring, when the positive class
is in the minority.
这种方法的主要动机是 F1-scoring, 当 positive class（正类）较少时.</dd>
</dl>
</li>
</ul>
<p>请注意, 这些所有的策略, <code class="docutils literal notranslate"><span class="pre">predict</span></code> 方法彻底的忽略了输入数据!</p>
<p>为了说明 <a class="reference internal" href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DummyClassifier</span></code></a>, 首先让我们创建一个 imbalanced dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[</span><span class="n">y</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>接下来, 让我们比较一下 <code class="docutils literal notranslate"><span class="pre">SVC</span></code> 和  <code class="docutils literal notranslate"><span class="pre">most_frequent</span></code> 的准确性.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="k">import</span> <span class="n">DummyClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.63...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;most_frequent&#39;</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">DummyClassifier(constant=None, random_state=0, strategy=&#39;most_frequent&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.57...</span>
</pre></div>
</div>
<p>我们看到 <code class="docutils literal notranslate"><span class="pre">SVC</span></code> 没有比一个 dummy classifier（虚拟分类器）好很多.
现在, 让我们来更改一下 kernel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  
<span class="go">0.97...</span>
</pre></div>
</div>
<p>我们看到准确率提升到将近 100%.
建议采用交叉验证策略, 以更好地估计精度, 如果不是太耗 CPU 的话.
更多信息请参阅 <a class="reference internal" href="cross_validation.html#cross-validation"><span class="std std-ref">交叉验证：评估估算器的表现</span></a> 部分.
此外，如果要优化参数空间，强烈建议您使用适当的方法;
更多详情请参阅 <a class="reference internal" href="grid_search.html#grid-search"><span class="std std-ref">调整估计器的超参数</span></a> 部分.</p>
<p>通常来说，当分类器的准确度太接近随机情况时，这可能意味着出现了一些问题: 特征没有帮助, 超参数没有正确调整, class 不平衡造成分类器有问题等…</p>
<p><a class="reference internal" href="generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor" title="sklearn.dummy.DummyRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DummyRegressor</span></code></a> 还实现了四个简单的经验法则来进行回归:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">mean</span></code> 总是预测训练目标的平均值.</li>
<li><code class="docutils literal notranslate"><span class="pre">median</span></code> 总是预测训练目标的中位数.</li>
<li><code class="docutils literal notranslate"><span class="pre">quantile</span></code> 总是预测用户提供的训练目标的 quantile（分位数）.</li>
<li><code class="docutils literal notranslate"><span class="pre">constant</span></code> 总是预测由用户提供的常数值.</li>
</ul>
<p>在以上所有的策略中, <code class="docutils literal notranslate"><span class="pre">predict</span></code> 方法完全忽略了输入数据.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    
    

    

    

    

    

    

    

    

    

    

    

    
    <!-- modules/model_evaluation.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@飓风</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@FAME</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@v</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@片刻</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@那伊抹微笑</a><br/>     
    </div>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/model_evaluation.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="generated/sklearn.ensemble.GradientBoostingRegressor.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="model_persistence.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>