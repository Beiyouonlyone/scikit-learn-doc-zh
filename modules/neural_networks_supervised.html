

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.17. 神经网络模型（有监督） &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. 无监督学习" href="../unsupervised_learning.html" />
    <link rel="prev" title="1.16. 概率校准" href="calibration.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/neural_networks_supervised.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="calibration.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      1.16. 概率校准
      </span>
          <span class="hiddenrellink">
          1.16. 概率校准
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="../unsupervised_learning.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      2. 无监督学习
      </span>
          <span class="hiddenrellink">
          2. 无监督学习
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../supervised_learning.html">
      Up
      <br/>
      <span class="smallrellink">
      1. 监督学习
      </span>
          <span class="hiddenrellink">
          1. 监督学习
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">1.17. 神经网络模型（有监督）</a><ul>
<li><a class="reference internal" href="#multilayer-perceptron">1.17.1. 多层感知器</a></li>
<li><a class="reference internal" href="#id3">1.17.2. 分类</a></li>
<li><a class="reference internal" href="#id4">1.17.3. 回归</a></li>
<li><a class="reference internal" href="#id5">1.17.4. 正则化</a></li>
<li><a class="reference internal" href="#id6">1.17.5. 算法</a></li>
<li><a class="reference internal" href="#id7">1.17.6. 复杂度</a></li>
<li><a class="reference internal" href="#id8">1.17.7. 数学公式</a></li>
<li><a class="reference internal" href="#mlp-tips">1.17.8. 实用技巧</a></li>
<li><a class="reference internal" href="#warm-start">1.17.9. 使用 warm_start 的更多控制</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="neural-networks-supervised">
<span id="id1"></span><h1>1.17. 神经网络模型（有监督）<a class="headerlink" href="#neural-networks-supervised" title="Permalink to this headline">¶</a></h1>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">此实现不适用于大规模数据应用。 特别是 scikit-learn 不支持 GPU。如果想要提高运行速度并使用基于 GPU 的实现以及为构建深度学习架构提供更多灵活性的框架，请参阅 <a class="reference internal" href="../related_projects.html#related-projects"><span class="std std-ref">Related Projects</span></a> 。</p>
</div>
<div class="section" id="multilayer-perceptron">
<span id="id2"></span><h2>1.17.1. 多层感知器<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">¶</a></h2>
<p><strong>多层感知器(MLP)</strong> 是一种监督学习算法，通过在数据集上训练来学习函数 <span class="math">f(\cdot): R^m \rightarrow R^o</span>，其中 <span class="math">m</span> 是输入的维数，<span class="math">o</span> 是输出的维数。 给定一组特征 <span class="math">X = {x_1, x_2, ..., x_m}</span> 和标签 <span class="math">y</span> ，它可以学习用于分类或回归的非线性函数。 与逻辑回归不同的是，在输入层和输出层之间，可以有一个或多个非线性层，称为隐藏层。 图1 展示了一个具有标量输出的单隐藏层 MLP。</p>
<div class="figure align-center" id="id11">
<a class="reference internal image-reference" href="../_images/multilayerperceptron_network.png"><img alt="../_images/multilayerperceptron_network.png" src="../_images/multilayerperceptron_network.png" style="width: 469.79999999999995px; height: 510.59999999999997px;" /></a>
<p class="caption"><span class="caption-text"><strong>图1：单隐藏层MLP.</strong></span></p>
</div>
<p>最左层的输入层由一组代表输入特征的神经元 <span class="math">\{x_i | x_1, x_2, ..., x_m\}</span> 组成。 每个隐藏层中的神经元将前一层的值进行加权线性求和转换 <span class="math">w_1x_1 + w_2x_2 + ... + w_mx_m</span> ，再通过非线性激活函数 <span class="math">g(\cdot):R \rightarrow R</span> - 比如双曲正切函数 tanh 。 输出层接收到的值是最后一个隐藏层的输出经过变换而来的。</p>
<p>该模块包含公共属性 <code class="docutils literal notranslate"><span class="pre">coefs_</span></code> 和 <code class="docutils literal notranslate"><span class="pre">intercepts_</span></code> 。 <code class="docutils literal notranslate"><span class="pre">coefs_</span></code> 是一系列权重矩阵，其中下标为 <span class="math">i</span> 的权重矩阵表示第 <span class="math">i</span> 层和第 <span class="math">i+1</span> 层之间的权重。 <code class="docutils literal notranslate"><span class="pre">intercepts_</span></code> 是一系列偏置向量，其中的下标为 <span class="math">i</span> 的向量表示添加到第 <span class="math">i+1</span> 层的偏置值。</p>
<p>多层感知器的优点:</p>
<blockquote>
<div><ul class="simple">
<li>可以学习得到非线性模型。</li>
<li>使用``partial_fit`` 可以学习得到实时模型(在线学习)。</li>
</ul>
</div></blockquote>
<p>多层感知器(MLP)的缺点:</p>
<blockquote>
<div><ul class="simple">
<li>具有隐藏层的 MLP 具有非凸的损失函数，它有不止一个的局部最小值。 因此不同的随机权重初始化会导致不同的验证集准确率。</li>
<li>MLP 需要调试一些超参数，例如隐藏层神经元的数量、层数和迭代轮数。</li>
<li>MLP 对特征归一化很敏感.</li>
</ul>
</div></blockquote>
<p>解决这些缺点的方法请参阅 <a class="reference internal" href="#mlp-tips"><span class="std std-ref">实用使用技巧</span></a> 部分。</p>
</div>
<div class="section" id="id3">
<h2>1.17.2. 分类<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 类实现了通过 <a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">Backpropagation</a> 进行训练的多层感知器（MLP）算法。</div></blockquote>
<p>MLP 在两个 array 上进行训练:大小为 (n_samples, n_features) 的 array X 储存表示训练样本的浮点型特征向量; 大小为 (n_samples,) 的 array y 储存训练样本的目标值（类别标签）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="k">import</span> <span class="n">MLPClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                         
<span class="go">MLPClassifier(activation=&#39;relu&#39;, alpha=1e-05, batch_size=&#39;auto&#39;,</span>
<span class="go">       beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="go">       epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate=&#39;constant&#39;,</span>
<span class="go">       learning_rate_init=0.001, max_iter=200, momentum=0.9,</span>
<span class="go">       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,</span>
<span class="go">       solver=&#39;lbfgs&#39;, tol=0.0001, validation_fraction=0.1, verbose=False,</span>
<span class="go">       warm_start=False)</span>
</pre></div>
</div>
<p>拟合（训练）后，该模型可以预测新样本的标签:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1, 0])</span>
</pre></div>
</div>
<p>MLP 可以为训练数据拟合一个非线性模型。 <code class="docutils literal notranslate"><span class="pre">clf.coefs_</span></code> 包含了构建模型的权值矩阵:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">coef</span> <span class="ow">in</span> <span class="n">clf</span><span class="o">.</span><span class="n">coefs_</span><span class="p">]</span>
<span class="go">[(2, 5), (5, 2), (2, 1)]</span>
</pre></div>
</div>
<p>目前， <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 只支持交叉熵损失函数，通过运行 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 方法进行概率估计。</p>
<p>MLP 算法使用的是反向传播的方式。 更准确地说，它使用了通过反向传播计算得到的梯度和某种形式的梯度下降来进行训练。 对于分类来说，它最小化交叉熵损失函数，为每个样本 <span class="math">x</span> 给出一个向量形式的概率估计 <span class="math">P(y|x)</span></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>  
<span class="go">array([[  1.967...e-04,   9.998...-01],</span>
<span class="go">       [  1.967...e-04,   9.998...-01]])</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 通过应用 <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_activation_function">Softmax</a> 作为输出函数来支持多分类。</p>
<p>此外，该模型支持 <a class="reference internal" href="multiclass.html#multiclass"><span class="std std-ref">多标签分类</span></a> ，一个样本可能属于多个类别。 对于每个类，原始输出经过 logistic 函数变换后，大于或等于 0.5 的值将进为 1，否则为 0。 对于样本的预测输出，值为 1 的索引位置表示该样本的分类类别:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                         
<span class="go">MLPClassifier(activation=&#39;relu&#39;, alpha=1e-05, batch_size=&#39;auto&#39;,</span>
<span class="go">       beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="go">       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate=&#39;constant&#39;,</span>
<span class="go">       learning_rate_init=0.001, max_iter=200, momentum=0.9,</span>
<span class="go">       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,</span>
<span class="go">       solver=&#39;lbfgs&#39;, tol=0.0001, validation_fraction=0.1, verbose=False,</span>
<span class="go">       warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([[1, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="go">array([[0, 1]])</span>
</pre></div>
</div>
<p>更多内容请参阅下面的示例和文档 <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit" title="sklearn.neural_network.MLPClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">MLPClassifier.fit</span></code></a> 。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py"><span class="std std-ref">Compare Stochastic learning strategies for MLPClassifier</span></a></li>
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py"><span class="std std-ref">Visualization of MLP weights on MNIST</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id4">
<h2>1.17.3. 回归<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 类多层感知器（MLP）的实现，在使用反向传播进行训练时的输出层没有使用激活函数，也可以看作是使用恒等函数（identity function）作为激活函数。 因此，它使用平方误差作为损失函数，输出是一组连续值。</p>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 还支持多输出回归，其中一个样本可以有多个目标值。</p>
</div>
<div class="section" id="id5">
<h2>1.17.4. 正则化<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 类和 <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 类都使用参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 作为正则化( L2 正则化)系数，正则化通过惩罚大数量级的权重值以避免过拟合问题。 下面的图表展示了不同的 alpha 值下的决策函数的变化。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/neural_networks/plot_mlp_alpha.html"><img alt="modules/../auto_examples/neural_networks/images/sphx_glr_plot_mlp_alpha_001.png" src="modules/../auto_examples/neural_networks/images/sphx_glr_plot_mlp_alpha_001.png" /></a>
</div>
<p>详细信息，请参阅下面的示例。</p>
<div class="topic">
<p class="topic-title first">示例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py"><span class="std std-ref">Varying regularization in Multi-layer Perceptron</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id6">
<h2>1.17.5. 算法<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>MLP 使用 <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent（随机梯度下降）(SGD)</a>,
<a class="reference external" href="http://arxiv.org/abs/1412.6980">Adam</a>, 或者 <a class="reference external" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> 进行训练。
随机梯度下降（SGD） 使用关于需要适应的一个参数的损失函数的梯度来更新参数，即</p>
<div class="math">
<p><span class="math">w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial Loss}{\partial w})</span></p>
</div><p>其中 <span class="math">\eta</span> 是控制训练过程参数更新步长的学习率（learning rate）。 <span class="math">Loss</span> 是损失函数（loss function）。</p>
<p>更多细节可以在这个文档中找到 <a class="reference external" href="http://scikit-learn.org/stable/modules/sgd.html">SGD</a> 。</p>
<p>Adam 类似于 SGD，因为它是 stochastic optimizer （随机优化器），但它可以根据低阶矩的自适应估计自动调整参数更新的量。</p>
<p>使用 SGD 或 Adam ，训练过程支持在线模式和小批量学习模式。</p>
<p>L-BFGS 是利用 Hessian 矩阵来近似函数的二阶偏导数的求解器，它使用 Hessian 的逆矩阵来近似进行参数更新。 该实现使用 Scipy 版本的 <a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html">L-BFGS</a>。</p>
<p>如果所选择的方法是 ‘L-BFGS’，训练过程不支持在线学习模式和小批量学习模式。</p>
</div>
<div class="section" id="id7">
<h2>1.17.6. 复杂度<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>假设有 <span class="math">n</span> 个训练样本， <span class="math">m</span> 个特征， <span class="math">k</span> 个隐藏层，每个包含 <span class="math">h</span> 个神经元 - 为简单起见， <span class="math">o</span> 个输出神经元。 反向传播的时间复杂度是 <span class="math">O(n\cdot m \cdot h^k \cdot o \cdot i)</span> ，其中 <span class="math">i</span> 是迭代次数。 由于反向传播具有高时间复杂性，最好以较少数量的隐藏层神经元和较少的隐藏层个数开始训练。</p>
</div>
<div class="section" id="id8">
<h2>1.17.7. 数学公式<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>给出一组训练样本 <span class="math">(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)</span> 其中 <span class="math">x_i \in \mathbf{R}^n</span> ， <span class="math">y_i \in \{0, 1\}</span> ，一个单隐藏层单神经元 MLP 学习到的函数是 <span class="math">f(x) = W_2 g(W_1^T x + b_1) + b_2</span> ，其中 <span class="math">W_1 \in \mathbf{R}^m</span> 和 <span class="math">W_2, b_1, b_2 \in \mathbf{R}</span> 是模型参数.
<span class="math">W_1, W_2</span> 分别是输入层与隐藏层之间和隐藏层与输出层之间的权重， <span class="math">b_1, b_2</span> 分别是隐藏层和输出层的偏置值.
<span class="math">g(\cdot) : R \rightarrow R</span> 是激活函数，默认为双曲正切函数。 具体形式如下，</p>
<div class="math">
<p><span class="math">g(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}</span></p>
</div><p>对于二分类， <span class="math">f(x)</span> 经过 logistic 函数 <span class="math">g(z)=1/(1+e^{-z})</span> 得到 0 到 1 之间的输出值。 阈值设置为 0.5 ，输出大于等于 0.5 的样本分到 positive class （正类），其他的分为 negative class （负类）。</p>
<p>如果多于两类，则 <span class="math">f(x)</span> 本身将是一个大小为 (n_classes,) 的向量。 它需要经过 softmax 函数而不是 logistic 函数进行变换，具体形式如下，</p>
<div class="math">
<p><span class="math">\text{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{l=1}^k\exp(z_l)}</span></p>
</div><p>其中 <span class="math">z_i</span> 表示 softmax 函数的第 <span class="math">i</span> 个输入的元素，它对应于第 <span class="math">i</span> 类， <span class="math">K</span> 是类别的数量。 计算结果是样本 <span class="math">x</span> 属于每个类别的概率的向量。 最终输出的分类结果是具有最高概率的类别。</p>
<p>在回归问题中，输出依然是 <span class="math">f(x)</span> ;因此，输出激活函数就是恒等函数。</p>
<p>MLP 根据特定问题使用不同的损失函数。 二分类问题的损失函数的是交叉熵，具体形式如下，</p>
<div class="math">
<p><span class="math">Loss(\hat{y},y,W) = -y \ln {\hat{y}} - (1-y) \ln{(1-\hat{y})} + \alpha ||W||_2^2</span></p>
</div><p>其中 <span class="math">\alpha ||W||_2^2</span> 是 L2 正则化的模型复杂度惩罚项;  <span class="math">\alpha &gt; 0</span> 这个非负的超参数控制惩罚的程度。</p>
<p>对于回归问题，MLP 使用平方误差损失函数，具体形式如下，</p>
<div class="math">
<p><span class="math">Loss(\hat{y},y,W) = \frac{1}{2}||\hat{y} - y ||_2^2 + \frac{\alpha}{2} ||W||_2^2</span></p>
</div><p>从随机初始化权重开始，多层感知器（MLP）不断更新这些权重值来最小化损失函数。计算完损失之后，从输出层到前面各层进行反向传播，更新权重参数的值，旨在减小损失函数。</p>
<p>在梯度下降中，计算得到损失函数关于每个权重的梯度 <span class="math">\nabla Loss_{W}</span> 并从权重 <span class="math">W</span> 中减掉。用公式表示为，</p>
<div class="math">
<p><span class="math">W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}</span></p>
</div><p>其中 <span class="math">i</span> 是当前迭代步数， <span class="math">\epsilon</span> 是大于 0 学习率。</p>
<p>算法停止的条件或者是达到预设的最大迭代次数，或者是损失函数低于某个特定值。</p>
</div>
<div class="section" id="mlp-tips">
<span id="id9"></span><h2>1.17.8. 实用技巧<a class="headerlink" href="#mlp-tips" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li>多层感知器对特征的缩放是敏感的，所以它强烈建议您归一化你的数据。 例如，将输入向量 X 的每个属性放缩到到 [0, 1] 或 [-1，+1] ，或者将其标准化使它具有 0 均值和方差 1。</li>
</ul>
<p>注意，为了得到有意义的结果，您必须对测试集也应用 <em>相同的</em> 尺度缩放。 您可以使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code> 进行标准化。</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Don&#39;t cheat - fit only on training data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># apply same transformation to test data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
</pre></div>
</div>
<p>另一个推荐的方法是在 <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code> 中使用的 <code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code> 。</p>
</div></blockquote>
<ul class="simple">
<li>最好使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code> 找到一个合理的正则化参数 <span class="math">\alpha</span> ，通常范围是在 <code class="docutils literal notranslate"><span class="pre">10.0</span> <span class="pre">**</span> <span class="pre">-np.arange(1,</span> <span class="pre">7)</span></code> 。</li>
<li>据经验可知，我们观察到 <cite>L-BFGS</cite> 收敛速度是更快的并且是小数据集上更好的解决方案。对于规模相对比较大的数据集，<cite>Adam</cite> 是非常鲁棒的。 它通常会迅速收敛，并得到相当不错的表现。</li>
</ul>
<p>另一方面，如果学习率调整得正确， 使用 momentum 或 nesterov’s momentum 的 <cite>SGD</cite> 可以比这两种算法更好。</p>
</div></blockquote>
</div>
<div class="section" id="warm-start">
<h2>1.17.9. 使用 warm_start 的更多控制<a class="headerlink" href="#warm-start" title="Permalink to this headline">¶</a></h2>
<p>如果您希望更多地控制 SGD 中的停止标准或学习率，或者想要进行额外的监视，使用 <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code> 和 <code class="docutils literal notranslate"><span class="pre">max_iter=1</span></code> 并且自身迭代可能会有所帮助:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># additional monitoring / inspection </span>
<span class="go">MLPClassifier(...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf">“Learning representations by back-propagating errors.”</a>
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.</li>
<li><a class="reference external" href="http://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li><a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">“Backpropagation”</a>
Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.</li>
<li><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</li>
<li><a class="reference external" href="http://arxiv.org/pdf/1412.6980v8.pdf">“Adam: A method for stochastic optimization.”</a>
Kingma, Diederik, and Jimmy Ba. arXiv preprint arXiv:1412.6980 (2014).</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <!-- modules/neural_networks_supervised.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/tiantian1412">@tiantian1412</a><br/>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@火星</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@A</a><br/>
    </div>
    
    
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/neural_networks_supervised.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="calibration.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="../unsupervised_learning.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>