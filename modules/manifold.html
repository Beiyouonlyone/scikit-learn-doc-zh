

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>2.2. 流形学习 &#8212; scikit-learn 0.19.0 中文文档 - ApacheCN</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.3. 聚类" href="clustering.html" />
    <link rel="prev" title="2.1. 高斯混合模型" href="mixture.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/manifold.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
     function () {
         $(this).css('background-color', '#FF9C34');
     },
     function () {
         $(this).css('background-color', '#A7D6E2');
     }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
        topNav.className += " responsive";
    } else {
        topNav.className = "navbar";
    }
  };
</script>

  </head><body>

<div class="header-wrapper">
  <div class="header">
      <p class="logo"><a href="../index.html">
          <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
      </a>
      </p><div class="navbar" id="scikit-navbar">
          <ul>
              <li><a href="../index.html">首页</a></li>
              <li><a href="../install.html">安装</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="../documentation.html">文档</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li class="link-title">Scikit-learn 0.19</li>
                      <li><a href="../tutorial/index.html">教程</a></li>
                      <li><a href="../user_guide.html">用户指南</a></li>
                      <li><a href="classes.html">API</a></li>
                      <li><a href="../faq.html">FAQ</a></li>
                      <li><a href="../developers/contributing.html">贡献</a></li>
                      <li class="divider"></li>
                      <li><a href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 0.19 (stable)</a></li>
                      <li><a href="http://scikit-learn.org/0.18/documentation.html">Scikit-learn 0.18</a></li>
                      <li><a href="http://scikit-learn.org/0.17/documentation.html">Scikit-learn 0.17</a></li>
                      <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="../auto_examples/index.html">示例</a></li>
              <li><a href="../project-timeline.html">时光轴</a></li>
              <li class="btn-li">
                <div class="btn-group">
                    <a href="javascript:void(0)">项目相关</a>
                    <a class="btn dropdown-toggle" data-toggle="dropdown">
                      <span class="caret"></span>
                    </a>
                    <ul class="dropdown-menu">
                      <li><a href="../project-role.html">项目角色</a></li>
                      <li><a href="../project-check-progress.html">校验进度</a></li>
                      <li><a href="../project-translation-progress.html">翻译进度</a></li>
                      <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-timeline.html">时光轴</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-reward.html">项目奖励</a></li>
                      <li class="divider"></li>
                      <li><a href="http://www.apachecn.org/organization/244.html" target="_blank">积分物品</a></li>
                      <li><a href="http://www.apachecn.org/organization/269.html" target="_blank">兑换记录</a></li>
                      <li class="divider"></li>
                      <li><a href="../project-feedback.html">建议反馈</a></li>
                      <li><a href="../project-communication-group.html">技术交流</a></li>
                    </ul>
                </div>
              </li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh#%E8%B4%A1%E7%8C%AE%E8%80%85" target="_blank">贡献者</a></li>
              <li><a href="//github.com/apachecn/scikit-learn-doc-zh" target="_blank">GitHub</a></li>
          </ul>
          <a href="javascript:void(0);" onclick="showMenu()">
              <div class="nav-icon">
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
                  <div class="hamburger-line"></div>
              </div>
          </a>
          <div class="search_form">
              <div class="gcse-search" id="cse" style="width: 100%;"></div>
          </div>
      </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/apachecn/scikit-learn-doc-zh">
<img class="fork-me"
     style="position: absolute; top: 0; right: 0; border: 0;"
     src="../_static/img/starme.png"
     alt="Star me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
  <div class="sphinxsidebarwrapper">
      <div class="rel">
  
      <div class="rellink">
      <a href="mixture.html"
      accesskey="P">Previous
      <br/>
      <span class="smallrellink">
      2.1. 高斯混合模型
      </span>
          <span class="hiddenrellink">
          2.1. 高斯混合模型
          </span>
      </a>
      </div>
          <div class="spacer">
          &nbsp;
          </div>
      <div class="rellink">
      <a href="clustering.html"
      accesskey="N">Next
      <br/>
      <span class="smallrellink">
      2.3. 聚类
      </span>
          <span class="hiddenrellink">
          2.3. 聚类
          </span>
      </a>
      </div>

  <!-- Ad a link to the 'up' page -->
      <div class="spacer">
      &nbsp;
      </div>
      <div class="rellink">
      <a href="../unsupervised_learning.html">
      Up
      <br/>
      <span class="smallrellink">
      2. 无监督学习
      </span>
          <span class="hiddenrellink">
          2. 无监督学习
          </span>
          
      </a>
      </div>
  </div>
  
    <p class="doc-version"><b>scikit-learn v0.19.0</b><br/>
    <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
  <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
  <ul>
<li><a class="reference internal" href="#">2.2. 流形学习</a><ul>
<li><a class="reference internal" href="#id2">2.2.1. 介绍</a></li>
<li><a class="reference internal" href="#isomap">2.2.2. Isomap</a><ul>
<li><a class="reference internal" href="#id4">2.2.2.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#locally-linear-embedding">2.2.3. 局部线性嵌入</a><ul>
<li><a class="reference internal" href="#id6">2.2.3.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlle">2.2.4. 改进型局部线性嵌入（MLLE）</a><ul>
<li><a class="reference internal" href="#id7">2.2.4.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#he">2.2.5. 黑塞特征映射（HE）</a><ul>
<li><a class="reference internal" href="#id8">2.2.5.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#spectral-embedding">2.2.6. 谱嵌入</a><ul>
<li><a class="reference internal" href="#id10">2.2.6.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ltsa">2.2.7. 局部切空间对齐（LTSA）</a><ul>
<li><a class="reference internal" href="#id11">2.2.7.1. 复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mds">2.2.8. 多维尺度分析（MDS）</a><ul>
<li><a class="reference internal" href="#id13">2.2.8.1. 度量 MDS</a></li>
<li><a class="reference internal" href="#id14">2.2.8.2. 非度量 MDS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#t-t-sne">2.2.9. t 分布随机邻域嵌入（t-SNE）</a><ul>
<li><a class="reference internal" href="#id15">2.2.9.1. 优化 t-SNE</a></li>
<li><a class="reference internal" href="#barnes-hut-t-sne">2.2.9.2. Barnes-Hut t-SNE</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">2.2.10. 实用技巧</a></li>
</ul>
</li>
</ul>

  </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




    <div class="content">
          
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="manifold">
<span id="id1"></span><h1>2.2. 流形学习<a class="headerlink" href="#manifold" title="Permalink to this headline">¶</a></h1>
<div class="quote line-block">
<div class="line">Look for the bare necessities</div>
<div class="line">The simple bare necessities</div>
<div class="line">Forget about your worries and your strife</div>
<div class="line">I mean the bare necessities</div>
<div class="line">Old Mother Nature’s recipes</div>
<div class="line">That bring the bare necessities of life</div>
<div class="line"><br /></div>
<div class="line-block">
<div class="line">– Baloo的歌 [奇幻森林]</div>
</div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_compare_methods.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_compare_methods_001.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_compare_methods_001.png" /></a>
</div>
<p>流形学习是一种非线性降维方法。其算法基于的思想是：许多数据集的维度过高只是由人为导致的。</p>
<div class="section" id="id2">
<h2>2.2.1. 介绍<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>高维数据集会非常难以可视化。 虽然可以绘制两维或三维的数据来显示数据的固有结构，但与之等效的高维图不太直观。 为了帮助数据集结构的可视化，必须以某种方式降低维度。</p>
<p>通过对数据的随机投影来实现降维是最简单的方法。 虽然这样做能实现数据结构一定程度的可视化，但随机选择投影仍有许多有待改进之处。 在随机投影中，数据中更有趣的结构很可能会丢失。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="digits_img" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_001.png" /></a> <a class="reference external" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="projected_img" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_002.png" /></a></strong></p><p>为了解决这一问题，一些监督和无监督的线性降维框架被设计出来，如主成分分析（PCA），独立成分分析，线性判别分析等。 这些算法定义了明确的规定来选择数据的“有趣的”线性投影。 它们虽然强大，但是会经常错失数据中重要的非线性结构。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="PCA_img" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_003.png" /></a> <a class="reference external" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="LDA_img" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_004.png" /></a></strong></p><p>流形学习可以被认为是一种将线性框架（如 PCA ）推广为对数据中非线性结构敏感的尝试。 虽然存在监督变量，但是典型的流形学习问题是无监督的：它从数据本身学习数据的高维结构，而不使用预定的分类。</p>
<div class="topic">
<p class="topic-title first">例子:</p>
<ul class="simple">
<li>参见 <a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a> ,手写数字降维的例子。</li>
<li>参见 <a class="reference internal" href="../auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"><span class="std std-ref">Comparison of Manifold Learning methods</span></a> ,玩具 “S曲线” 数据集降维的例子。</li>
</ul>
</div>
<p>以下概述了 scikit-learn 中可用的流形学习实现</p>
</div>
<div class="section" id="isomap">
<span id="id3"></span><h2>2.2.2. Isomap<a class="headerlink" href="#isomap" title="Permalink to this headline">¶</a></h2>
<p>流形学习的最早方法之一是 Isomap 算法，等距映射（Isometric Mapping）的缩写。 Isomap 可以被视为多维缩放（Multi-dimensional Scaling：MDS）或核主成分分析（Kernel PCA）的扩展。 Isomap 寻求一个较低维度的嵌入，它保持所有点之间的测量距离。 Isomap 可以通过 <a class="reference internal" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="xref py py-class docutils literal notranslate"><span class="pre">Isomap</span></code></a> 对象执行。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_005.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_005.png" /></a>
</div>
<div class="section" id="id4">
<h3>2.2.2.1. 复杂度<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Isomap 算法包括三个阶段:</p>
<ol class="arabic simple">
<li><strong>最近邻搜索.</strong>  Isomap 使用
<a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.BallTree</span></code></a> 进行有效的近邻搜索。
对于 <span class="math">D</span> 维中 <span class="math">N</span> 个点的 <span class="math">k</span> 个最近邻，代价约为 <span class="math">O[D \log(k) N \log(N)]</span></li>
<li><strong>最短路径图搜索.</strong>  最有效的已知算法是 Dijkstra 算法或 Floyd-Warshall 算法，其复杂度分别是约
<span class="math">O[N^2(k + \log(N))]</span> 和 <span class="math">O[N^3]</span> 。 用户可通过使用 isomap 的 path_method 关键字选择该算法。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
<li><strong>部分特征值分解.</strong>  对应于 <span class="math">N \times N</span> isomap核中 <span class="math">d</span> 个最大特征值的特征向量，进行嵌入编码。 对于密集求解器，代价约为 <span class="math">O[d N^2]</span> 。 通常可以使用 ARPACK 求解器来减少代价。 用户可通过使用 isomap 的 path_method 关键字指定特征求解器。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
</ol>
<p>Isomap 的整体复杂度是
<span class="math">O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]</span>.</p>
<ul class="simple">
<li><span class="math">N</span> : 训练数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻的个数</li>
<li><span class="math">d</span> : 输出维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://science.sciencemag.org/content/290/5500/2319.full">“A global geometric framework for nonlinear dimensionality reduction”</a>
Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C.  Science 290 (5500)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="locally-linear-embedding">
<span id="id5"></span><h2>2.2.3. 局部线性嵌入<a class="headerlink" href="#locally-linear-embedding" title="Permalink to this headline">¶</a></h2>
<p>局部线性嵌入（LLE）通过保留局部邻域内的距离来寻求数据的低维投影。 它可以被认为是一系列的局部主成分分析，与全局相比，找到最优的局部非线性嵌入。</p>
<p>局部线性嵌入可以使用
<a class="reference internal" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">locally_linear_embedding</span></code></a> 函数或其面向对象的等效方法
<a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocallyLinearEmbedding</span></code></a> 来实现。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_006.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_006.png" /></a>
</div>
<div class="section" id="id6">
<h3>2.2.3.1. 复杂度<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>标准的 LLE 算法包括三个阶段:</p>
<ol class="arabic simple">
<li><strong>最邻近搜索</strong>.  参见上述 Isomap 讨论。</li>
<li><strong>构造权重矩阵</strong>. <span class="math">O[D N k^3]</span>.
LLE 权重矩阵的构造包括每个 <span class="math">N</span> 局部邻域的 <span class="math">k \times k</span> 线性方程的解</li>
<li><strong>部分特征值分解</strong>. 参见上述 Isomap 讨论。</li>
</ol>
<p>标准 LLE 的整体复杂度是
<span class="math">O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]</span>.</p>
<ul class="simple">
<li><span class="math">N</span> : 训练数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻的个数</li>
<li><span class="math">d</span> : 输出维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.sciencemag.org/content/290/5500/2323.full">“Nonlinear dimensionality reduction by locally linear embedding”</a>
Roweis, S. &amp; Saul, L.  Science 290:2323 (2000)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="mlle">
<h2>2.2.4. 改进型局部线性嵌入（MLLE）<a class="headerlink" href="#mlle" title="Permalink to this headline">¶</a></h2>
<p>关于局部线性嵌入（LLE）的一个众所周知的问题是正则化问题。当 neighbors（邻域）的数量多于输入的维度数量时，定义每个局部邻域的矩阵是不满秩的。为解决这个问题，标准的局部线性嵌入算法使用一个任意正则化参数 <span class="math">r</span> ，它的取值受局部权重矩阵的迹的影响。虽然可以认为 <span class="math">r \to 0</span> ，即解收敛于嵌入情况，但是不保证最优解情况下 <span class="math">r &gt; 0</span> 。此问题说明，在嵌入时此问题会扭曲流形的内部几何形状，使其失真。</p>
<p>解决正则化问题的一种方法是对邻域使用多个权重向量。这就是改进型局部线性嵌入（MLLE）算法的精髓。MLLE 可被执行于函数 <a class="reference internal" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">locally_linear_embedding</span></code></a> ，或者面向对象的副本 <a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils literal notranslate"><span class="pre">method</span> <span class="pre">=</span> <span class="pre">'modified'</span></code> 。它需要满足 <code class="docutils literal notranslate"><span class="pre">n_neighbors</span> <span class="pre">&gt;</span> <span class="pre">n_components</span></code> 。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_007.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_007.png" /></a>
</div>
<div class="section" id="id7">
<h3>2.2.4.1. 复杂度<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>MLLE 算法分为三部分：</p>
<ol class="arabic simple">
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>权重矩阵构造</strong>。大约是
<span class="math">O[D N k^3] + O[N (k-D) k^2]</span> 。该式第一项恰好等于标准 LLE 算法的复杂度。该式第二项与由多个权重来构造权重矩阵相关。在实践中，（在第二步中）构造 MLLE 权重矩阵（对复杂度）增加的影响，比第一步和第三步的小。</li>
<li><strong>部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p>综上，MLLE 的复杂度为
<span class="math">O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]</span> 。</p>
<ul class="simple">
<li><span class="math">N</span> : 训练集数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻域的个数</li>
<li><span class="math">d</span> : 输出的维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">“MLLE: Modified Locally Linear Embedding Using Multiple Weights”</a>
Zhang, Z. &amp; Wang, J.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="he">
<h2>2.2.5. 黑塞特征映射（HE）<a class="headerlink" href="#he" title="Permalink to this headline">¶</a></h2>
<p>黑塞特征映射 (也称作基于黑塞的 LLE: HLLE ）是解决 LLE 正则化问题的另一种方法。在每个用于恢复局部线性结构的邻域内，它会围绕一个基于黑塞的二次型展开。虽然其他实现表明它对数据大小缩放较差，但是 sklearn 实现了一些算法改进，使得在输出低维度时它的损耗可与其他 LLE 变体相媲美。HLLE 可实现为函数 <a class="reference internal" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">locally_linear_embedding</span></code></a> 或其面向对象的形式 <a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils literal notranslate"><span class="pre">method</span> <span class="pre">=</span> <span class="pre">'hessian'</span></code> 。它需满足 <code class="docutils literal notranslate"><span class="pre">n_neighbors</span> <span class="pre">&gt;</span> <span class="pre">n_components</span> <span class="pre">*</span> <span class="pre">(n_components</span> <span class="pre">+</span> <span class="pre">3)</span> <span class="pre">/</span> <span class="pre">2</span></code> 。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_008.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_008.png" /></a>
</div>
<div class="section" id="id8">
<h3>2.2.5.1. 复杂度<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>HLLE 算法分为三部分:</p>
<ol class="arabic simple">
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>权重矩阵构造</strong>. 大约是 <span class="math">O[D N k^3] + O[N d^6]</span> 。其中第一项与标准 LLE 相似。第二项来自于局部黑塞估计量的一个 QR 分解。</li>
<li><strong>部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p>综上，HLLE 的复杂度为 <span class="math">O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]</span> 。</p>
<ul class="simple">
<li><span class="math">N</span> : 训练集数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻域的个数</li>
<li><span class="math">d</span> : 输出的维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.pnas.org/content/100/10/5591">“Hessian Eigenmaps: Locally linear embedding techniques for
high-dimensional data”</a>
Donoho, D. &amp; Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="spectral-embedding">
<span id="id9"></span><h2>2.2.6. 谱嵌入<a class="headerlink" href="#spectral-embedding" title="Permalink to this headline">¶</a></h2>
<p>谱嵌入是计算非线性嵌入的一种方法。scikit-learn 执行拉普拉斯特征映射，该映射是用图拉普拉斯的谱分解的方法把数据进行低维表达。这个生成的图可认为是低维流形在高维空间里的离散近似值。基于图的代价函数的最小化确保流形上彼此临近的点被映射后在低维空间也彼此临近，低维空间保持了局部距离。谱嵌入可执行为函数 <a class="reference internal" href="generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding" title="sklearn.manifold.spectral_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">spectral_embedding</span></code></a> 或它的面向对象的对应形式 <a class="reference internal" href="generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding" title="sklearn.manifold.SpectralEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralEmbedding</span></code></a> 。</p>
<div class="section" id="id10">
<h3>2.2.6.1. 复杂度<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>谱嵌入（拉普拉斯特征映射）算法含三部分：</p>
<ol class="arabic simple">
<li><strong>加权图结构</strong>。把原输入数据转换为用相似（邻接）矩阵表达的图表达。</li>
<li><strong>图拉普拉斯结构</strong>。非规格化的图拉普拉斯是按 <span class="math">L = D - A</span> 构造，并按 <span class="math">L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}</span> 规格化的。</li>
<li><strong>部分特征值分解</strong>。在图拉普拉斯上进行特征值分解。</li>
</ol>
<p>综上，谱嵌入的复杂度是 <span class="math">O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]</span> 。</p>
<ul class="simple">
<li><span class="math">N</span> : 训练集数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻域的个数</li>
<li><span class="math">d</span> : 输出的维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf">“Laplacian Eigenmaps for Dimensionality Reduction
and Data Representation”</a>
M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396</li>
</ul>
</div>
</div>
</div>
<div class="section" id="ltsa">
<h2>2.2.7. 局部切空间对齐（LTSA）<a class="headerlink" href="#ltsa" title="Permalink to this headline">¶</a></h2>
<p>尽管局部切空间对齐（LTSA）在技术上并不是 LLE 的变体，但它与 LLE 足够相近，可以放入这个目录。与 LLE 算法关注于保持临点距离不同，LTSA 寻求通过切空间来描述局部几何形状，并（通过）实现全局最优化来对其这些局部切空间，从而学会嵌入。
LTSA 可执行为函数
<a class="reference internal" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="xref py py-func docutils literal notranslate"><span class="pre">locally_linear_embedding</span></code></a> 或它的面向对象的对应形式
<a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils literal notranslate"><span class="pre">method</span> <span class="pre">=</span> <span class="pre">'ltsa'</span></code> 。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_009.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_009.png" /></a>
</div>
<div class="section" id="id11">
<h3>2.2.7.1. 复杂度<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>LTSA 算法含三部分:</p>
<ol class="arabic simple">
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>加权矩阵构造</strong>。大约是 <span class="math">O[D N k^3] + O[k^2 d]</span> 。其中第一项与标准 LLE 相似。</li>
<li><strong>部分特征值分解</strong>。同于标准 LLE 。</li>
</ol>
<p>综上，复杂度是 <span class="math">O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]</span> 。</p>
<ul class="simple">
<li><span class="math">N</span> : 训练集数据点的个数</li>
<li><span class="math">D</span> : 输入维度</li>
<li><span class="math">k</span> : 最近邻域的个数</li>
<li><span class="math">d</span> : 输出的维度</li>
</ul>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693">“Principal manifolds and nonlinear dimensionality reduction via
tangent space alignment”</a>
Zhang, Z. &amp; Zha, H. Journal of Shanghai Univ. 8:406 (2004)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="mds">
<span id="multidimensional-scaling"></span><h2>2.2.8. 多维尺度分析（MDS）<a class="headerlink" href="#mds" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Multidimensional_scaling">多维尺度分析 Multidimensional scaling</a> （ <a class="reference internal" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="xref py py-class docutils literal notranslate"><span class="pre">MDS</span></code></a> ） 寻求数据的低维表示，（低维下的）它的距离保持了在初始高维空间中的距离。</p>
<p>一般来说，（MDS）是一种用来分析在几何空间距离相似或相异数据的技术。MDS 尝试将相似或相异的数据建模为几何空间距离。这些数据可以是物体间的相似等级，也可是分子的作用频率，还可以是国家简单贸易指数。</p>
<p>MDS算法有2类：度量和非度量。在 scikit-learn 中， <a class="reference internal" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="xref py py-class docutils literal notranslate"><span class="pre">MDS</span></code></a> 类中二者都有。在度量 MDS 中，输入相似度矩阵源自度量(并因此遵从三角形不等式)，输出两点之间的距离被设置为尽可能接近相似度或相异度的数据。在非度量版本中，算法尝试保持距离的控制，并因此寻找在所嵌入空间中的距离和相似/相异之间的单调关系。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_010.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_010.png" /></a>
</div>
<p>设 <span class="math">S</span> 是相似度矩阵，<span class="math">X</span> 是 <span class="math">n</span> 个输入点的坐标。差异 <span class="math">\hat{d}_{ij}</span> 是以某种最佳方式选择的相似度的转换。然后，通过 <span class="math">sum_{i &lt; j} d_{ij}(X) - \hat{d}_{ij}(X)</span> 定义称为 Stress （应力值）的对象。</p>
<div class="section" id="id13">
<h3>2.2.8.1. 度量 MDS<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>最简单的度量 <a class="reference internal" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="xref py py-class docutils literal notranslate"><span class="pre">MDS</span></code></a> 模型称为 <em>absolute MDS（绝对MDS）</em>，差异由 <span class="math">\hat{d}_{ij} = S_{ij}</span> 定义。对于绝对 MDS，值 <span class="math">S_{ij}</span> 应精确地对应于嵌入点的点 <span class="math">i</span> 和 <span class="math">j</span> 之间的距离。</p>
<p>大多数情况下，差异应设置为 <span class="math">\hat{d}_{ij} = b S_{ij}</span> 。</p>
</div>
<div class="section" id="id14">
<h3>2.2.8.2. 非度量 MDS<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>非度量 <a class="reference internal" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="xref py py-class docutils literal notranslate"><span class="pre">MDS</span></code></a> 关注数据的排序。如果 <span class="math">S_{ij} &lt; S_{kl}</span> ，则嵌入应执行 <span class="math">d_{ij} &lt; d_{jk}</span> 。这样执行的一个简单算法是在 <span class="math">S_{ij}</span> 上使用 <span class="math">d_{ij}</span> 的单调回归，产生与 <span class="math">S_{ij}</span> 相同顺序的差异 <span class="math">\hat{d}_{ij}</span> 。</p>
<p>此问题的 a trivial solution（一个平凡解）是把所有点设置到原点上。为了避免这种情况，将差异 <span class="math">S_{ij}</span> 标准化。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_mds.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_mds_001.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_mds_001.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.springer.com/fr/book/9780387251509">“Modern Multidimensional Scaling - Theory and Applications”</a>
Borg, I.; Groenen P. Springer Series in Statistics (1997)</li>
<li><a class="reference external" href="http://link.springer.com/article/10.1007%2FBF02289694">“Nonmetric multidimensional scaling: a numerical method”</a>
Kruskal, J. Psychometrika, 29 (1964)</li>
<li><a class="reference external" href="http://link.springer.com/article/10.1007%2FBF02289565">“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”</a>
Kruskal, J. Psychometrika, 29, (1964)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="t-t-sne">
<span id="t-sne"></span><h2>2.2.9. t 分布随机邻域嵌入（t-SNE）<a class="headerlink" href="#t-t-sne" title="Permalink to this headline">¶</a></h2>
<p>t-SNE（ <a class="reference internal" href="generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="sklearn.manifold.TSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a> ）将数据点的相似性转换为概率。原始空间中的相似性表示为高斯联合概率，嵌入空间中的相似性表示为 “学生” 的 t 分布。这允许 t-SNE 对局部结构特别敏感，并且有超过现有技术的一些其它优点:</p>
<ul class="simple">
<li>在一个单一映射上以多种比例显示结构</li>
<li>显示位于多个、不同的流形或聚类中的数据</li>
<li>减轻在中心聚集的趋势</li>
</ul>
<p>Isomap、LLE 和其它变体最适合展开单个连续低维流形，而 t-SNE 将侧重于数据的局部结构，并倾向于提取聚类的局部样本组，就像S曲线示例中突出显示的那样。这种基于局部结构对样本进行分组的能力可能有助于在视觉上同时解开包括多个流形的数据集，如数字数据集中的情况。</p>
<p>原始空间和嵌入空间中的联合概率的 Kullback-Leibler（KL） 散度将通过梯度下降而最小化。注意，KL 发散不是凸的，即具有不同初始化的多次重新开始将以KL发散的局部最小值结束。因此，尝试不同的开始值并选择具有最低KL散度的嵌入有时是有用的。</p>
<p>使用 t - SNE 的缺点大致如下:</p>
<ul class="simple">
<li>t-SNE 的计算成本很高，在百万样本数据集上可能需要几个小时，而PCA将在几秒或几分钟内完成同样工作。</li>
<li>Barnes-Hut t-SNE 方法仅限于二维或三维嵌入。</li>
<li>该算法是随机的，不同种子的多次重新开始可以产生不同的嵌入。然而，以最小的误差选择嵌入是完全合理的。</li>
<li>未明确保留全局结构。用PCA初始化点(使用 <cite>init=’pca’</cite> )，可以减轻此问题。</li>
</ul>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_013.png" src="modules/../auto_examples/manifold/images/sphx_glr_plot_lle_digits_013.png" /></a>
</div>
<div class="section" id="id15">
<h3>2.2.9.1. 优化 t-SNE<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>t-SNE 的主要目的是实现高维数据的可视化。因此，当数据将嵌入到二维或三维时，它效果最好。</p>
<p>优化KL发散有时可能有点棘手。有五个参数控制 t-SNE 的优化，因此可能也控制最终嵌入的质量:</p>
<ul class="simple">
<li>复杂度</li>
<li>早期增长因子</li>
<li>学习率</li>
<li>最大迭代次数</li>
<li>角度（不在精确方法中使用）</li>
</ul>
<p>复杂度（perplexity）定义为 <span class="math">k=2^(S)</span> ，其中 <span class="math">S</span> 是条件概率分布的香农熵。k 面色子的复杂度是 k ，因此 k 实际上是生成条件概率时 t-SNE 考虑的最近邻域的个数。复杂度越大导致有越多的近邻域，则对小结构越不敏感。相反地，越低的复杂度考虑越少的邻域，并因此忽略越多的全局信息而越关注局部邻域。当数据集的大小变大时，需要更多的点来获得局部邻域的合理样本，因此可能需要更大的复杂度。类似地，噪声越大的数据集需要越大的复杂度来包含足够的局部邻域，以超出背景噪声。</p>
<p>最大迭代次数通常足够高，不需要任何调整。优化分为两个阶段:早期增长阶段和最终优化阶段。在早期增长中，原始空间中的联合概率将通过与给定因子相乘而被人为地增加。越大的因子导致数据中的自然聚类之间的差距越大。如果因子过高，KL 发散可能在此阶段增加。通常不需要对其进行调谐。学习率是一个关键参数。如果梯度太低，下降会陷入局部极小值。如果过高，KL发散将在优化阶段增加。可以在 Laurens van derMaaten 的常见问题解答中找到更多提示(见参考资料)。最后一个参数角度是性能和精度之间的折衷。角度越大意味着我们可以通过单个点来近似的区域越大，从而导致越快的速度，但结果越不准确。</p>
<p><a class="reference external" href="http://distill.pub/2016/misread-tsne/">“如何高效使用 t-SNE”</a> 提供了一个关于各种参数效果的很好的讨论，以及用来探索不同参数效果的交互式绘图。</p>
</div>
<div class="section" id="barnes-hut-t-sne">
<h3>2.2.9.2. Barnes-Hut t-SNE<a class="headerlink" href="#barnes-hut-t-sne" title="Permalink to this headline">¶</a></h3>
<p>在此实现的 Barnes-Hut t-SNE 通常比其他流形学习算法慢得多。优化是很困难的，梯度的计算是 <span class="math">O[d N log(N)]</span> ，其中 <span class="math">d</span> 是输出维数，<span class="math">N</span> 是样本个数。Barnes-Hut 方法在 t-SNE 复杂度为 <span class="math">O[d N^2]</span> 的精确方法上有所改进，但有其他几个显著区别:</p>
<ul class="simple">
<li>Barnes-Hut 实现仅在目标维度为3或更小时才有效。构建可视化时，2D 案例是典型的。</li>
<li>Barnes-Hut 仅适用于密集的输入数据。稀疏数据矩阵只能用精确方法嵌入，或者可以通过密集的低阶投影来近似，例如使用 <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.decomposition.TruncatedSVD</span></code></a></li>
<li>Barnes-Hut 是精确方法的近似。近似使用 angle 作为参数，因此当参数 method=”exact” 时，angle 参数无效。</li>
<li>Barnes-Hut 的拓展性很高。Barnes-Hut 可用于嵌入数十万个数据点，而精确方法只能处理数千个样本，再多就很困难了。</li>
</ul>
<p>出于可视化目的（ t-SNE 的主要使用情况），强烈建议使用 Barnes-Hut 方法。精确的 t-SNE 方法可用于检验高维空间中嵌入的理论性质，但由于计算约束而仅限于小数据集。</p>
<p>还要注意，数字 label 与 t-SNE 发现的自然聚类大致匹配，而 PCA 模型的线性 2D 投影产生标签区域在很大程度上重叠的表示。这是一个强有力的线索，表明该数据可以通过关注局部结构的非线性方法（例如，具有高斯 RBF 核的 SVM ）很好地分离。然而，如果不能在二维中用 t-SNE 来可视化分离良好的均匀标记组，并不一定意味着数据不能被监督模型正确地分类。可能是因为 2 维不够低，无法准确表示数据的内部结构。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://jmlr.org/papers/v9/vandermaaten08a.html">“Visualizing High-Dimensional Data Using t-SNE”</a>
van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research
(2008)</li>
<li><a class="reference external" href="http://lvdmaaten.github.io/tsne/">“t-Distributed Stochastic Neighbor Embedding”</a>
van der Maaten, L.J.P.</li>
<li><a class="reference external" href="https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf">“Accelerating t-SNE using Tree-Based Algorithms.”</a>
L.J.P. van der Maaten.  Journal of Machine Learning Research 15(Oct):3221-3245, 2014.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="id17">
<h2>2.2.10. 实用技巧<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>确保对所有特征使用相同的缩放。因为流形学习方法是基于最近邻搜索的，否则算法的性能可能很差。有关缩放异构数据的方便方法，请参阅 <a class="reference internal" href="preprocessing.html#preprocessing-scaler"><span class="std std-ref">StandardScaler</span></a> 。</li>
<li>由每个例程计算的重构误差可用于选择最佳输出维度。对于嵌入在 <span class="math">d</span> 维参数空间中的 <span class="math">d</span> 维流形，重构误差将随着 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 的增加而减小，直到 <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">==</span> <span class="pre">d</span></code> 。</li>
<li>注意，噪声数据可以对流形造成“短路”，其实质上充当了一个桥梁，用于连接流形的不同部分，否则（没有这样的“桥梁”）这些流形将被很好地划分开。噪声和/或不完全数据的流形学习是一个活跃的研究领域。</li>
<li>某些输入配置可能导致奇异加权矩阵，例如，当数据集中的两个以上点完全相同时，或者当数据被分割成不连续的组时。在这种情况下， <code class="docutils literal notranslate"><span class="pre">solver='arpack'</span></code> 将无法找到空空间。解决这一问题的最简单方法是使用 <code class="docutils literal notranslate"><span class="pre">solver='dense'</span></code> ，它将在一个奇异矩阵上进行，尽管它可能因为输入点的数量而非常缓慢。或者，人们可以尝试理解奇异的来源:如果它是由于不相交的集合，增加 <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> 可能有所帮助；如果是由于数据集中的相同点，则删除这些点可能有所帮助。</li>
</ul>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="ensemble.html#random-trees-embedding"><span class="std std-ref">完全随机树嵌入</span></a> 也可以用于得到特征空间的非线性表示，另外它不用降维。</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>

      <!-- 评论留言区代码 start -->
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDAwMi8xMDU0MA==">
        <script type="text/javascript">
        (function(d, s) {
            var j, e = d.getElementsByTagName(s)[0];

            if (typeof LivereTower === 'function') { return; }

            j = d.createElement(s);
            j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
            j.async = true;

            e.parentNode.insertBefore(j, e);
        })(document, 'script');
        </script>
      </div>
      <!-- 评论留言区代码 end -->

    </div>

    <!-- 提 PR 时按原来文档的字母排序 -->

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    
    

    
    <!-- modules/manifold.html -->
    <div class="apachecn_doc_right">
      校验者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi">@XuJianzhi</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/RyanZhiNie">@RyanZhiNie</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@羊三</a><br/>
      翻译者: <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi">@XuJianzhi</a><br/>
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@羊三</a><br/>
    </div>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </div>

  <div class="footer">
      &copy; 2007 - 2017, scikit-learn developers (BSD License).
    <a href="../_sources/modules/manifold.rst.txt" rel="nofollow">Show this page source</a>
  </div>
   <div class="rel">
  
  <div class="buttonPrevious">
    <a href="mixture.html">Previous
    </a>
  </div>
  <div class="buttonNext">
    <a href="clustering.html">Next
    </a>
  </div>
  
   </div>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
    ga('create', 'UA-102475051-5', 'auto');
    ga('send', 'pageview');
  
  </script>
  
  <!-- baidu tongji -->
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?9cbab13b4d28a9811ae1d2d2176dab66";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  <!-- baidu push -->
  <script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script>
  </body>
</html>